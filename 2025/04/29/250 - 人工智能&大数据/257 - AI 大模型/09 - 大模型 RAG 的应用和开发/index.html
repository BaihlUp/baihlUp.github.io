<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>大模型 RAG 的应用和开发 | 梦之痕</title><meta name="author" content="梦之痕"><meta name="copyright" content="梦之痕"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="1 向量数据库1.1 向量数据库简介向量数据库就是一种专门用于存储和处理向量数据的数据库系统，传统的关系型数据库通常不擅长处理向量数据，因为它们需要将数据映射为结构化的表格形式，而向量数据的维度较高、结构复杂，导致传统数据库存储和查询效率低下，所以向量数据库应运而生。 1.2 传统数据库与向量数据库的差异传统数据库采用基于行的存储方式，传统数据库将数据存储为行记录，每一行包含多个字段，并且每个字段">
<meta property="og:type" content="article">
<meta property="og:title" content="大模型 RAG 的应用和开发">
<meta property="og:url" content="https://baihlup.github.io/2025/04/29/250%20-%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD&%E5%A4%A7%E6%95%B0%E6%8D%AE/257%20-%20AI%20%E5%A4%A7%E6%A8%A1%E5%9E%8B/09%20-%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%20RAG%20%E7%9A%84%E5%BA%94%E7%94%A8%E5%92%8C%E5%BC%80%E5%8F%91/index.html">
<meta property="og:site_name" content="梦之痕">
<meta property="og:description" content="1 向量数据库1.1 向量数据库简介向量数据库就是一种专门用于存储和处理向量数据的数据库系统，传统的关系型数据库通常不擅长处理向量数据，因为它们需要将数据映射为结构化的表格形式，而向量数据的维度较高、结构复杂，导致传统数据库存储和查询效率低下，所以向量数据库应运而生。 1.2 传统数据库与向量数据库的差异传统数据库采用基于行的存储方式，传统数据库将数据存储为行记录，每一行包含多个字段，并且每个字段">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/41710043961_.pic.jpg">
<meta property="article:published_time" content="2025-04-29T00:00:00.000Z">
<meta property="article:modified_time" content="2025-05-06T10:12:56.674Z">
<meta property="article:author" content="梦之痕">
<meta property="article:tag" content="大模型">
<meta property="article:tag" content="RAG">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/41710043961_.pic.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://baihlup.github.io/2025/04/29/250%20-%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD&amp;%E5%A4%A7%E6%95%B0%E6%8D%AE/257%20-%20AI%20%E5%A4%A7%E6%A8%A1%E5%9E%8B/09%20-%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%20RAG%20%E7%9A%84%E5%BA%94%E7%94%A8%E5%92%8C%E5%BC%80%E5%8F%91/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Error',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '大模型 RAG 的应用和开发',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-05-06 10:12:56'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/41710043961_.pic.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">62</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">46</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">15</div></a></div><hr class="custom-hr"/></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="梦之痕"><span class="site-name">梦之痕</span></a></span><div id="menus"><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">大模型 RAG 的应用和开发</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2025-04-29T00:00:00.000Z" title="Created 2025-04-29 00:00:00">2025-04-29</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-05-06T10:12:56.674Z" title="Updated 2025-05-06 10:12:56">2025-05-06</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD-%E5%A4%A7%E6%95%B0%E6%8D%AE/">人工智能&amp;大数据</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="大模型 RAG 的应用和开发"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="1-向量数据库"><a href="#1-向量数据库" class="headerlink" title="1 向量数据库"></a>1 向量数据库</h1><h2 id="1-1-向量数据库简介"><a href="#1-1-向量数据库简介" class="headerlink" title="1.1 向量数据库简介"></a>1.1 向量数据库简介</h2><p>向量数据库就是一种专门用于存储和处理向量数据的数据库系统，传统的关系型数据库通常不擅长处理向量数据，因为它们需要将数据映射为结构化的表格形式，而<strong>向量数据的维度较高、结构复杂，导致传统数据库存储和查询效率低下</strong>，所以向量数据库应运而生。<br><img src="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20250429163216.png" alt="image.png"></p>
<h2 id="1-2-传统数据库与向量数据库的差异"><a href="#1-2-传统数据库与向量数据库的差异" class="headerlink" title="1.2 传统数据库与向量数据库的差异"></a>1.2 传统数据库与向量数据库的差异</h2><p>传统数据库采用基于行的存储方式，传统数据库将数据存储为行记录，每一行包含多个字段，并且每个字段都有固定的列。传统数据库通常使用索引来提高查询性能，例如下方就是一个典型的传统数据库表格<br><img src="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20250429163443.png" alt="image.png"></p>
<p>这种方式在处理结构化数据时非常高效，但在处理非结构化或半结构化数据时效率低下。</p>
<p>向量数据库将数据以列形式存储，即每个列都有一个独立的存储空间，这使得向量数据库可以更加灵活地处理复杂的数据结构。向量数据库还可以进行列压缩（<strong>稀疏矩阵</strong>），以减少存储空间和提高数据的访问速度。<br>并且在向量数据库中，将数据表示为<strong>高维向量</strong>，其中<strong>每个向量对应于数据点</strong>。<strong>这些向量之间的距离表示它们之间的相似性</strong>。这种方式使得非结构化或半结构化数据的存储和检索变得更加高效。</p>
<p>以电影数据库为例，我们可以将每部电影表示为一个特征向量。假设我们使用四个特征来描述每部电影：<strong>动作、冒险、爱情、科幻</strong>。每个特征都可以在0到1的范围内进行标准化，表示该电影在该特征上的强度。</p>
<p>例如，电影”阿凡达”的向量表示可以是 <code>[0.9, 0.8, 0.2, 0.9]</code>，其中数字分别表示动作、冒险、爱情、科幻的特征强度。其他电影也可以用类似的方式表示。这些向量可以存储在向量数据库中，如下所示：<br><img src="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20250429163534.png" alt="image.png"><br>现在，如果我们想要查找与电影”阿凡达”相似的电影，我们可以计算向量之间的距离，找到最接近的向量，从而实现相似性匹配，而无需复杂的SQL查询。这就像使用地图找到两个地点之间的最短路径一样简单。</p>
<h2 id="1-3-传统数据库与向量数据库优缺点"><a href="#1-3-传统数据库与向量数据库优缺点" class="headerlink" title="1.3 传统数据库与向量数据库优缺点"></a>1.3 传统数据库与向量数据库优缺点</h2><p><img src="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20250429163644.png" alt="image.png"></p>
<h2 id="1-4-相似度搜索算法"><a href="#1-4-相似度搜索算法" class="headerlink" title="1.4 相似度搜索算法"></a>1.4 相似度搜索算法</h2><h3 id="1-4-1-余弦相似度与欧氏距离"><a href="#1-4-1-余弦相似度与欧氏距离" class="headerlink" title="1.4.1 余弦相似度与欧氏距离"></a>1.4.1 余弦相似度与欧氏距离</h3><p>在向量数据库中，支持通过多种方式来计算两个向量的相似度，例如：<strong>余弦相似度、欧式距离、曼哈顿距离、闵可夫斯基距离、汉明距离、Jaccard相似度</strong>等多种。其中最常见的就是 余弦相似度 和 欧式距离。<br>例如下图，左侧就是 欧式距离，右侧就是 余弦相似度：<br><img src="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20250429163817.png" alt="image.png"></p>
<ol>
<li><p>余弦相似度主要用于衡量向量在方向上的相似性，特别适用于文本、图像和高维空间中的向量。它不受向量长度的影响，只考虑方向的相似程度，余弦相似度的计算公式如下（计算两个向量夹角的余弦值，取值范围为<code>[-1, 1]</code>）：<br><img src="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20250429163849.png" alt="image.png"></p>
</li>
<li><p>欧式距离衡量向量之间的直线距离，得到的值可能很大，最小为 0，通常用于低维空间或需要考虑向量各个维度之间差异的情况。欧氏距离较小的向量被认为更相似，欧式距离的计算公式如下：</p>
</li>
</ol>
<p><img src="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20250429163918.png" alt="image.png"></p>
<h3 id="1-4-2-相似性搜索加速算法"><a href="#1-4-2-相似性搜索加速算法" class="headerlink" title="1.4.2 相似性搜索加速算法"></a>1.4.2 相似性搜索加速算法</h3><p>在向量数据库中，数据按列进行存储，通常会将多个向量组织成一个 M×N 的矩阵，其中 M 是向量的维度（特征数），N 是向量的数量（数据库中的条目数），这个矩阵可以是稠密或者稀疏的，取决于向量的稀疏性和具体的存储优化策略。</p>
<p>这样计算相似性搜索时，本质上就变成了向量与 M×N 矩阵的每一行进行相似度计算，这里可以用到大量成熟的加速算法：</p>
<p>1. <strong>矩阵分解方法</strong>：</p>
<p>· <strong>SVD（奇异值分解）</strong>：可以通过奇异值分解将原始矩阵转换为更低秩的矩阵表示，从而减少计算量。<br>· <strong>PCA（主成分分析）</strong>：类似地，可以通过主成分分析将高维矩阵映射到低维空间，减少计算复杂度。</p>
<ol start="2">
<li><strong>索引结构和近似算法</strong>：</li>
</ol>
<p>· <strong>LSH（局部敏感哈希）</strong>：LSH 可以在近似相似度匹配中加速计算，特别适用于高维稀疏向量的情况。<br>· <strong>ANN（近似最近邻）算法</strong>：ANN 算法如KD-Tree、Ball-Tree等可以用来加速对最近邻搜索的计算，虽然主要用于向量空间，但也可以部分应用于相似度计算中。</p>
<ol start="3">
<li><strong>GPU 加速</strong>：使用图形处理单元（GPU）进行并行计算可以显著提高相似度计算的速度，尤其是对于大规模数据和高维度向量。</li>
<li><strong>分布式计算</strong>：由于行与行之间独立，所以可以很便捷地支持分布式计算每行与向量的相似度，从而加速整体计算过程。</li>
</ol>
<blockquote>
<p>向量数据库底层除了在算法层面上针对相似性搜索做了大量优化，在存储结构、索引机制等方面均做了大量的优化，这才使得向量数据库在处理高维数据和实现快速相似性搜索上展示出巨大的优势</p>
</blockquote>
<h2 id="1-5-向量数据库的配置和使用"><a href="#1-5-向量数据库的配置和使用" class="headerlink" title="1.5 向量数据库的配置和使用"></a>1.5 向量数据库的配置和使用</h2><p>按照部署方式和提供的服务类型进行划分，向量数据库可以划分成几种：<br>1. <strong>本地文件向量数据库</strong>：用户将向量数据存储到本地文件系统中，通过数据库查询的接口来检索向量数据，例如：<strong>Faiss</strong>。<br>2. <strong>本地部署 API 向量数据库</strong>：这类数据库不仅允许本地部署，而且提供了方便的 API 接口，使用户可以通过网络请求来访问和查询向量数据，这类数据库通常提供了更复杂的功能和管理选项，例如：<strong>Milvus、Annoy、Weaviate</strong> 等。<br>3. <strong>云端 API 向量数据库</strong>：将向量数据存储在云端，通过 API 提供向量数据的访问和管理功能，例如：<strong>TCVectorDB、Pinecone</strong> 等。</p>
<h3 id="1-5-1-Faiss-向量数据库"><a href="#1-5-1-Faiss-向量数据库" class="headerlink" title="1.5.1 Faiss 向量数据库"></a>1.5.1 Faiss 向量数据库</h3><h4 id="1-5-1-1-Faiss-基本使用"><a href="#1-5-1-1-Faiss-基本使用" class="headerlink" title="1.5.1.1 Faiss 基本使用"></a>1.5.1.1 Faiss 基本使用</h4><p>Faiss 是 Facebook 团队开源的向量检索工具，针对高维空间的海量数据，提供高效可靠的相似性检索方式，被广泛用于推荐系统、图片和视频搜索等业务。Faiss 支持 Linux、macOS 和 Windows 操作系统，在百万级向量的相似性检索表现中，Faiss 能实现 &lt; 10ms 的响应（需牺牲搜索准确度）。<br>CPU环境下使用</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install faiss-cpu</span><br></pre></td></tr></table></figure>
<p>GPU环境下使用并且已经安装了CUDA，则可以使用GPU版本</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install faiss-gpu</span><br></pre></td></tr></table></figure>


<p><strong>代码示例</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> dotenv  </span><br><span class="line"><span class="keyword">from</span> langchain_community.vectorstores <span class="keyword">import</span> FAISS  </span><br><span class="line"><span class="comment"># from langchain_openai import OpenAIEmbeddings  </span></span><br><span class="line"><span class="keyword">from</span> langchain_huggingface <span class="keyword">import</span> HuggingFaceEmbeddings  </span><br><span class="line">  </span><br><span class="line">dotenv.load_dotenv()  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># embedding = OpenAIEmbeddings(model=&quot;text-embedding-3-small&quot;)  </span></span><br><span class="line">  </span><br><span class="line">embeddings = HuggingFaceEmbeddings(  </span><br><span class="line">    model_name=<span class="string">&quot;sentence-transformers/all-MiniLM-L12-v2&quot;</span>,  </span><br><span class="line">    cache_folder=<span class="string">&quot;../22-其他Embedding嵌入模型的配置与使用/embeddings/&quot;</span>  </span><br><span class="line">)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># db = FAISS.from_texts([  </span></span><br><span class="line"><span class="comment">#     &quot;笨笨是一只很喜欢睡觉的猫咪&quot;,  </span></span><br><span class="line"><span class="comment">#     &quot;我喜欢在夜晚听音乐，这让我感到放松。&quot;,  </span></span><br><span class="line"><span class="comment">#     &quot;猫咪在窗台上打盹，看起来非常可爱。&quot;,  </span></span><br><span class="line"><span class="comment">#     &quot;学习新技能是每个人都应该追求的目标。&quot;,  </span></span><br><span class="line"><span class="comment">#     &quot;我最喜欢的食物是意大利面，尤其是番茄酱的那种。&quot;,  </span></span><br><span class="line"><span class="comment">#     &quot;昨晚我做了一个奇怪的梦，梦见自己在太空飞行。&quot;,  </span></span><br><span class="line"><span class="comment">#     &quot;我的手机突然关机了，让我有些焦虑。&quot;,  </span></span><br><span class="line"><span class="comment">#     &quot;阅读是我每天都会做的事情，我觉得很充实。&quot;,  </span></span><br><span class="line"><span class="comment">#     &quot;他们一起计划了一次周末的野餐，希望天气能好。&quot;,  </span></span><br><span class="line"><span class="comment">#     &quot;我的狗喜欢追逐球，看起来非常开心。&quot;,  </span></span><br><span class="line"><span class="comment"># ], embeddings)  </span></span><br><span class="line"><span class="comment">#  </span></span><br><span class="line"><span class="comment"># print(db.index.ntotal)  </span></span><br><span class="line"><span class="comment"># print(db.save_local(&quot;vector-store/&quot;))  </span></span><br><span class="line">  </span><br><span class="line">db = FAISS.load_local(<span class="string">&quot;vector-store/&quot;</span>, embeddings, allow_dangerous_deserialization=<span class="literal">True</span>)  </span><br><span class="line">  </span><br><span class="line"><span class="built_in">print</span>(db.similarity_search_with_score(<span class="string">&quot;我养了一只猫，叫笨笨&quot;</span>))</span><br></pre></td></tr></table></figure>

<h4 id="1-5-1-2-删除指定数据"><a href="#1-5-1-2-删除指定数据" class="headerlink" title="1.5.1.2 删除指定数据"></a>1.5.1.2 删除指定数据</h4><p>在 Faiss 中，支持删除向量数据库中特定的数据，目前仅支持传入数据条目 id 进行删除，并不支持条件筛选（但是可以通过条件筛选找到符合的数据，然后提取 id 列表，然后批量删除）。</p>
<p>代码示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;删除前数量:&quot;</span>, db.index.ntotal)</span><br><span class="line"><span class="comment"># 获取向量数据库的索引id列表信息</span></span><br><span class="line">db.delete([db.index_to_docstore_id[<span class="number">0</span>]])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;删除后数量:&quot;</span>, db.index.ntotal)</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">删除前数量: 10</span><br><span class="line">删除后数量: 9</span><br></pre></td></tr></table></figure>

<h4 id="1-5-1-3-带过滤的相似性搜索"><a href="#1-5-1-3-带过滤的相似性搜索" class="headerlink" title="1.5.1.3 带过滤的相似性搜索"></a>1.5.1.3 带过滤的相似性搜索</h4><p>在绝大部分向量数据库中，除了存储向量数据，还支持存储对应的元数据，这里的元数据可以是<strong>文本原文、扩展信息、页码、归属文档id、作者、创建时间</strong>等等任何自定义信息，一般在向量数据库中，会通过元数据来实现对数据的检索。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">向量数据库记录 = 向量(vector)+元数据(metadata)+<span class="built_in">id</span></span><br></pre></td></tr></table></figure>
<p>Faiss 原生并不支持过滤，所以在 LangChain 封装的 FAISS 中对过滤功能进行了相应的处理。首先获取比 k 更多的结果 fetch_k（默认为 20 条），然后先进行搜索，接下来再搜索得到的 fetch_k 条结果上进行过滤，得到 k 条结果，从而实现带过滤的相似性搜索。<br>而且 Faiss 的搜索都是针对 元数据 的，在 Faiss 中执行带过滤的相似性搜索非常简单，只需要在搜索时传递 filter 参数即可，filter 可以传递一个元数据字典，也可以接收一个函数（函数的参数为元数据字典，返回值为布尔值）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> dotenv  </span><br><span class="line"><span class="keyword">from</span> langchain_community.vectorstores <span class="keyword">import</span> FAISS  </span><br><span class="line"><span class="comment"># from langchain_openai import OpenAIEmbeddings  </span></span><br><span class="line"><span class="keyword">from</span> langchain_huggingface <span class="keyword">import</span> HuggingFaceEmbeddings  </span><br><span class="line">  </span><br><span class="line">dotenv.load_dotenv()  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># embedding = OpenAIEmbeddings(model=&quot;text-embedding-3-small&quot;)  </span></span><br><span class="line">  </span><br><span class="line">embeddings = HuggingFaceEmbeddings(  </span><br><span class="line">    model_name=<span class="string">&quot;sentence-transformers/all-MiniLM-L12-v2&quot;</span>,  </span><br><span class="line">    cache_folder=<span class="string">&quot;../22-其他Embedding嵌入模型的配置与使用/embeddings/&quot;</span>  </span><br><span class="line">)  </span><br><span class="line">  </span><br><span class="line">metadatas: <span class="built_in">list</span> = [  </span><br><span class="line">    &#123;<span class="string">&quot;page&quot;</span>: <span class="number">1</span>&#125;,  </span><br><span class="line">    &#123;<span class="string">&quot;page&quot;</span>: <span class="number">2</span>&#125;,  </span><br><span class="line">    &#123;<span class="string">&quot;page&quot;</span>: <span class="number">3</span>&#125;,  </span><br><span class="line">    &#123;<span class="string">&quot;page&quot;</span>: <span class="number">4</span>&#125;,  </span><br><span class="line">    &#123;<span class="string">&quot;page&quot;</span>: <span class="number">5</span>&#125;,  </span><br><span class="line">    &#123;<span class="string">&quot;page&quot;</span>: <span class="number">6</span>&#125;,  </span><br><span class="line">    &#123;<span class="string">&quot;page&quot;</span>: <span class="number">7</span>&#125;,  </span><br><span class="line">    &#123;<span class="string">&quot;page&quot;</span>: <span class="number">8</span>&#125;,  </span><br><span class="line">    &#123;<span class="string">&quot;page&quot;</span>: <span class="number">9</span>&#125;,  </span><br><span class="line">    &#123;<span class="string">&quot;page&quot;</span>: <span class="number">10</span>&#125;,  </span><br><span class="line">]  </span><br><span class="line">  </span><br><span class="line">db = FAISS.from_texts([  </span><br><span class="line">    <span class="string">&quot;笨笨是一只很喜欢睡觉的猫咪&quot;</span>,  </span><br><span class="line">    <span class="string">&quot;我喜欢在夜晚听音乐，这让我感到放松。&quot;</span>,  </span><br><span class="line">    <span class="string">&quot;猫咪在窗台上打盹，看起来非常可爱。&quot;</span>,  </span><br><span class="line">    <span class="string">&quot;学习新技能是每个人都应该追求的目标。&quot;</span>,  </span><br><span class="line">    <span class="string">&quot;我最喜欢的食物是意大利面，尤其是番茄酱的那种。&quot;</span>,  </span><br><span class="line">    <span class="string">&quot;昨晚我做了一个奇怪的梦，梦见自己在太空飞行。&quot;</span>,  </span><br><span class="line">    <span class="string">&quot;我的手机突然关机了，让我有些焦虑。&quot;</span>,  </span><br><span class="line">    <span class="string">&quot;阅读是我每天都会做的事情，我觉得很充实。&quot;</span>,  </span><br><span class="line">    <span class="string">&quot;他们一起计划了一次周末的野餐，希望天气能好。&quot;</span>,  </span><br><span class="line">    <span class="string">&quot;我的狗喜欢追逐球，看起来非常开心。&quot;</span>,  </span><br><span class="line">], embeddings, metadatas)  </span><br><span class="line">  </span><br><span class="line"><span class="built_in">print</span>(db.index_to_docstore_id)  </span><br><span class="line"><span class="built_in">print</span>(db.similarity_search_with_score(<span class="string">&quot;我养了一只猫，叫笨笨&quot;</span>, <span class="built_in">filter</span>=<span class="keyword">lambda</span> x: x[<span class="string">&quot;page&quot;</span>] &gt; <span class="number">5</span>))</span><br></pre></td></tr></table></figure>

<h3 id="1-5-2-Pinecone-向量数据库"><a href="#1-5-2-Pinecone-向量数据库" class="headerlink" title="1.5.2 Pinecone 向量数据库"></a>1.5.2 Pinecone 向量数据库</h3><h4 id="1-5-2-1-Pinecone-配置"><a href="#1-5-2-1-Pinecone-配置" class="headerlink" title="1.5.2.1 Pinecone 配置"></a>1.5.2.1 Pinecone 配置</h4><p>Pinecone 是一个托管的、云原生的向量数据库，具有极简的 API，并且无需在本地部署即可快速使用，Pinecone 服务提供商还为每个账户设置了足够的免费空间，<strong>在开发阶段，可以快速基于 Pinecone 快速开发 AI 应用</strong>。<br>相关资料：<br>1. Pinecone 官网：<a target="_blank" rel="noopener" href="https://www.pinecone.io/">https://www.pinecone.io/</a><br>2. Pinecone 翻译文档：<a target="_blank" rel="noopener" href="https://www.pinecone-io.com/">https://www.pinecone-io.com/</a><br>3. langchain-pinecone 翻译文档：<a target="_blank" rel="noopener" href="http://imooc-langchain.shortvar.com/docs/integrations/vectorstores/pinecone/">http://imooc-langchain.shortvar.com/docs/integrations/vectorstores/pinecone/</a><br>Pinecone 向量数据库的设计架构与 Faiss 差异较大，Pinecone 由于是一个面向商业端的向量数据库，在功能和概念上会更加丰富，有几个核心概念+架构图如下：<br><img src="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20250429174019.png" alt="image.png"></p>
<p>概念的解释如下：<br>1. <strong>组织</strong>：组织是使用相同结算方式的一个或者多个项目的集合，例如个人账号、公司账号等都算是一个组织。<br>2. <strong>项目</strong>：项目是用来管理向量数据库、索引、硬件资源等内容的整合，可以将不同的项目数据进行区分。<br>3. <strong>索引</strong>：索引是 Pinecone 中数据的最高组织单位，在索引中需要定义向量的存储维度、查询时使用的相似性指标，并且在 Pinecone 中支持两种类型的索引：无服务器索引（根据数据大小自动扩容）和 Pod 索引（预设空间&#x2F;硬件）。<br>4. <strong>命名空间</strong>：命名空间是索引内的分区，用于将索引中的数据区分成不同的组，以便于在不同的组内存储不同的数据，例如知识库、记忆的数据可以存储到不同的组中，类似 Excel 中的 Sheet表。<br>5. <strong>记录</strong>：记录是数据的基本单位，一条记录涵盖了 ID、向量(values)、元数据(metadata) 等。</p>
<p>所以在 Pinecone 中使用向量数据库，要确保 组织、项目、索引、命名空间、记录 等内容均配置好才可以使用，并且由于 Pinecone 是云端向量数据库，使用时还需配置对应的 API 秘钥（可在注册好 Pinecone 后管理页面的 API Key 中设置）。<br>对于 Pinecone，LangChain 团队也封装了响应的包，安装命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -U langchain-pinecone</span><br></pre></td></tr></table></figure>

<p>然后在 .env 文件中配置对应的 API 秘钥，如下</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PINECONE_API_KEY=xxx</span><br></pre></td></tr></table></figure>

<h4 id="1-5-2-2-Pinecone-使用"><a href="#1-5-2-2-Pinecone-使用" class="headerlink" title="1.5.2.2 Pinecone 使用"></a>1.5.2.2 Pinecone 使用</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> dotenv  </span><br><span class="line"><span class="comment"># from langchain_openai import OpenAIEmbeddings  </span></span><br><span class="line"><span class="keyword">from</span> langchain_pinecone <span class="keyword">import</span> PineconeVectorStore  </span><br><span class="line"><span class="keyword">from</span> langchain_huggingface <span class="keyword">import</span> HuggingFaceEmbeddings  </span><br><span class="line">  </span><br><span class="line">dotenv.load_dotenv()  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># embedding = OpenAIEmbeddings(model=&quot;text-embedding-3-small&quot;)  </span></span><br><span class="line">  </span><br><span class="line">embeddings = HuggingFaceEmbeddings(  </span><br><span class="line">    model_name=<span class="string">&quot;sentence-transformers/all-MiniLM-L12-v2&quot;</span>,  </span><br><span class="line">    cache_folder=<span class="string">&quot;../22-其他Embedding嵌入模型的配置与使用/embeddings/&quot;</span>  </span><br><span class="line">)  </span><br><span class="line">  </span><br><span class="line">texts: <span class="built_in">list</span> = [  </span><br><span class="line">    <span class="string">&quot;笨笨是一只很喜欢睡觉的猫咪&quot;</span>,  </span><br><span class="line">    <span class="string">&quot;我喜欢在夜晚听音乐，这让我感到放松。&quot;</span>,  </span><br><span class="line">    <span class="string">&quot;猫咪在窗台上打盹，看起来非常可爱。&quot;</span>,  </span><br><span class="line">    <span class="string">&quot;学习新技能是每个人都应该追求的目标。&quot;</span>,  </span><br><span class="line">    <span class="string">&quot;我最喜欢的食物是意大利面，尤其是番茄酱的那种。&quot;</span>,  </span><br><span class="line">    <span class="string">&quot;昨晚我做了一个奇怪的梦，梦见自己在太空飞行。&quot;</span>,  </span><br><span class="line">    <span class="string">&quot;我的手机突然关机了，让我有些焦虑。&quot;</span>,  </span><br><span class="line">    <span class="string">&quot;阅读是我每天都会做的事情，我觉得很充实。&quot;</span>,  </span><br><span class="line">    <span class="string">&quot;他们一起计划了一次周末的野餐，希望天气能好。&quot;</span>,  </span><br><span class="line">    <span class="string">&quot;我的狗喜欢追逐球，看起来非常开心。&quot;</span>,  </span><br><span class="line">]  </span><br><span class="line">metadatas: <span class="built_in">list</span> = [  </span><br><span class="line">    &#123;<span class="string">&quot;page&quot;</span>: <span class="number">1</span>&#125;,  </span><br><span class="line">    &#123;<span class="string">&quot;page&quot;</span>: <span class="number">2</span>&#125;,  </span><br><span class="line">    &#123;<span class="string">&quot;page&quot;</span>: <span class="number">3</span>&#125;,  </span><br><span class="line">    &#123;<span class="string">&quot;page&quot;</span>: <span class="number">4</span>&#125;,  </span><br><span class="line">    &#123;<span class="string">&quot;page&quot;</span>: <span class="number">5</span>&#125;,  </span><br><span class="line">    &#123;<span class="string">&quot;page&quot;</span>: <span class="number">6</span>, <span class="string">&quot;account_id&quot;</span>: <span class="number">1</span>&#125;,  </span><br><span class="line">    &#123;<span class="string">&quot;page&quot;</span>: <span class="number">7</span>&#125;,  </span><br><span class="line">    &#123;<span class="string">&quot;page&quot;</span>: <span class="number">8</span>&#125;,  </span><br><span class="line">    &#123;<span class="string">&quot;page&quot;</span>: <span class="number">9</span>&#125;,  </span><br><span class="line">    &#123;<span class="string">&quot;page&quot;</span>: <span class="number">10</span>&#125;,  </span><br><span class="line">]  </span><br><span class="line">db = PineconeVectorStore(index_name=<span class="string">&quot;llmops&quot;</span>, embedding=embeddings, namespace=<span class="string">&quot;dataset&quot;</span>,  </span><br><span class="line">                         pinecone_api_key=<span class="string">&quot;pcsk_Qz5bt_JMBCg1A6oJPbnceUnhwYf6CA1M57kBTxgVTDda96FkwCECAAhwPYrUvyytinYE2&quot;</span>)  </span><br><span class="line">db.add_texts(texts, metadatas, namespace=<span class="string">&quot;dataset&quot;</span>)  </span><br><span class="line">  </span><br><span class="line">query = <span class="string">&quot;我养了一只猫，叫笨笨&quot;</span>  </span><br><span class="line"><span class="built_in">print</span>(db.similarity_search_with_relevance_scores(query))</span><br></pre></td></tr></table></figure>

<h3 id="1-5-3-Weaviate-向量数据库"><a href="#1-5-3-Weaviate-向量数据库" class="headerlink" title="1.5.3 Weaviate 向量数据库"></a>1.5.3 Weaviate 向量数据库</h3><h4 id="1-5-3-1-Weaviate-介绍"><a href="#1-5-3-1-Weaviate-介绍" class="headerlink" title="1.5.3.1 Weaviate 介绍"></a>1.5.3.1 Weaviate 介绍</h4><p>Weaviate 是完全使用 Go 语言构建的开源向量数据库，提供了强大的数据存储和检索功能。并且 Weaviate 提供了多种部署方式，以满足不同用户和用例的需求，部署方式如下：<br>1. <strong>Weaviate 云</strong>：使用 Weaviate 官方提供的云服务，支持数据复制、零停机更新、无缝扩容等功能，适用于评估、开发和生产场景。<br>2. <strong>Docker 部署</strong>：使用 Docker 容器部署 Weaviate 向量数据库，适用于评估和开发等场景。<br>3. <strong>K8s 部署</strong>：在 K8s 上部署 Weaviate 向量数据库，适用于开发和生产场景。<br>4. 嵌入式 Weaviate：基于本地文件的方式构建 Weaviate 向量数据库，适用于评估场景，不过嵌入式 Weaviate 只适用于 Linux、macOS 系统，在 Windows 下不支持。</p>
<p>Weaviate 和 Pinecone&#x2F;TCVectorDB 一样，也存在着集合的概念，在 Weaviate 中集合类似传统关系型数据库中的表，负责管理一类数据&#x2F;数据对象，要使用 Weaviate 的流程其实也非常简单：<br>1. 创建部署 Weaviate 数据库（使用 Weaviate 云、Docker 部署）。<br>2. 安装 Python 客户端&#x2F;LangChain 集成包。<br>3. 连接 Weaviate（本地连接、云端连接）。<br>4. 创建数据集&#x2F;集合（代码创建、可视化管理界面创建），在 Weaviate 中，集合的名字必须以大写字母开头，并且只能包含字母、数字和下划线，否则创建的时候会出错，和 Python 的类名规范几乎一致。<br>5. 添加数据&#x2F;向量。<br>6. 相似性搜索&#x2F;带过滤器的相似性搜索。</p>
<p><strong>参考资料：</strong><br>1. Weaviate 官网：<a target="_blank" rel="noopener" href="https://weaviate.io/">https://weaviate.io/</a><br>2. Weaviate 快速上手指南：<a target="_blank" rel="noopener" href="https://weaviate.io/developers/weaviate/quickstart">https://weaviate.io/developers/weaviate/quickstart</a></p>
<p>LangChain Weaviate 集成包翻译文档：<a target="_blank" rel="noopener" href="https://imooc-langchain.shortvar.com/docs/integrations/vectorstores/weaviate">https://imooc-langchain.shortvar.com/docs/integrations/vectorstores/weaviate</a></p>
<h4 id="1-5-3-2-Weaviate-向量数据库的使用"><a href="#1-5-3-2-Weaviate-向量数据库的使用" class="headerlink" title="1.5.3.2 Weaviate 向量数据库的使用"></a>1.5.3.2 Weaviate 向量数据库的使用</h4><p><strong>Docker 部署 Weaviate 向量数据库：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --name weaviate-dev -p 8080:8080 -p 50051:50051 cr.weaviate.io/semitechnologies/weaviate:1.24.20</span><br></pre></td></tr></table></figure>
<p>创建好 Weaviate 数据库服务后，接下来就可以安装 Python 客户端&#x2F;LangChain 集成包，命令如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -Uqq langchain-weaviate</span><br></pre></td></tr></table></figure>
<p>如果使用的是 Weaviate 云服务，可以直接从可视化界面创建 Collection，亦或者在使用时 LangChain 自动检测对应的数据集是否存在，如果不存在则直接创建。<br>然后就可以考虑连接 Weaviate 服务了，Weaviate 框架针对不同的部署方式提供的不同的连接方法：<br>1. weaviate.connect_to_local()：连接到本地的部署服务，需配置连接 URL、端口号。<br>2. weaviate.connect_to_wcs()：连接到远程的 Weaviate 服务，需配置连接 URL、连接秘钥。</p>
<p><strong>代码示例</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> weaviate</span><br><span class="line"></span><br><span class="line"><span class="comment"># 连接192.168.2.120:8080并创建weaviate客户端</span></span><br><span class="line">client = weaviate.connect_to_local(<span class="string">&quot;192.168.2.120&quot;</span>, <span class="string">&quot;8080&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>连接到远程的 Weaviate 服务代码如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> weaviate</span><br><span class="line"><span class="keyword">from</span> weaviate.auth <span class="keyword">import</span> AuthApiKey</span><br><span class="line">client = weaviate.connect_to_wcs(</span><br><span class="line">    cluster_url=<span class="string">&quot;https://2j9jgyhprd2yej3c3rwog.c0.us-west3.gcp.weaviate.cloud&quot;</span>,</span><br><span class="line">    auth_credentials=AuthApiKey(<span class="string">&quot;BAn9bGZdZbdGCmUyfdegQoKFctyMmxaQdDFb&quot;</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>创建好客户端后，接下来可以基于客户端创建 LangChain 向量数据库实例，在实例化 LangChain VectorDB 时，需要传递 client（客户端）、 index_name（集合名字）、text（原始文本的存储键）、embedding（文本嵌入模型），如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> dotenv</span><br><span class="line"><span class="keyword">import</span> weaviate</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> OpenAIEmbeddings</span><br><span class="line"><span class="keyword">from</span> langchain_weaviate <span class="keyword">import</span> WeaviateVectorStore</span><br><span class="line"></span><br><span class="line">dotenv.load_dotenv()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.连接weaviate向量数据库</span></span><br><span class="line">client = weaviate.connect_to_local(<span class="string">&quot;192.168.2.120&quot;</span>, <span class="string">&quot;8080&quot;</span>)</span><br><span class="line"><span class="comment"># 2.实例化WeaviateVectorStore</span></span><br><span class="line">embedding = OpenAIEmbeddings(model=<span class="string">&quot;text-embedding-3-small&quot;</span>)</span><br><span class="line">db = WeaviateVectorStore(client=client, index_name=<span class="string">&quot;DatasetTest&quot;</span>, text_key=<span class="string">&quot;text&quot;</span>, embedding=embedding)</span><br></pre></td></tr></table></figure>

<p>实例化 LangChain VectorDB 后，就可以像 Faiss、Pinecone、TCVectorDB 一样去使用了，例如执行新增数据后完成检索示例如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> dotenv</span><br><span class="line"><span class="keyword">import</span> weaviate</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> OpenAIEmbeddings</span><br><span class="line"><span class="keyword">from</span> langchain_weaviate <span class="keyword">import</span> WeaviateVectorStore</span><br><span class="line">dotenv.load_dotenv()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.连接weaviate向量数据库</span></span><br><span class="line">client = weaviate.connect_to_local(<span class="string">&quot;192.168.2.120&quot;</span>, <span class="string">&quot;8080&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.实例化WeaviateVectorStore</span></span><br><span class="line">embedding = OpenAIEmbeddings(model=<span class="string">&quot;text-embedding-3-small&quot;</span>)</span><br><span class="line">db = WeaviateVectorStore(client=client, index_name=<span class="string">&quot;dataset-test&quot;</span>, text_key=<span class="string">&quot;text&quot;</span>, embedding=embedding)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.新增数据</span></span><br><span class="line">ids = db.add_texts([</span><br><span class="line">    <span class="string">&quot;笨笨是一只很喜欢睡觉的猫咪&quot;</span>,</span><br><span class="line">    <span class="string">&quot;我喜欢在夜晚听音乐，这让我感到放松。&quot;</span>,</span><br><span class="line">    <span class="string">&quot;猫咪在窗台上打盹，看起来非常可爱。&quot;</span>,</span><br><span class="line">    <span class="string">&quot;学习新技能是每个人都应该追求的目标。&quot;</span>,</span><br><span class="line">    <span class="string">&quot;我最喜欢的食物是意大利面，尤其是番茄酱的那种。&quot;</span>,</span><br><span class="line">    <span class="string">&quot;昨晚我做了一个奇怪的梦，梦见自己在太空飞行。&quot;</span>,</span><br><span class="line">    <span class="string">&quot;我的手机突然关机了，让我有些焦虑。&quot;</span>,</span><br><span class="line">    <span class="string">&quot;阅读是我每天都会做的事情，我觉得很充实。&quot;</span>,</span><br><span class="line">    <span class="string">&quot;他们一起计划了一次周末的野餐，希望天气能好。&quot;</span>,</span><br><span class="line">    <span class="string">&quot;我的狗喜欢追逐球，看起来非常开心。&quot;</span>,</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.检索数据</span></span><br><span class="line"><span class="built_in">print</span>(db.similarity_search_with_score(<span class="string">&quot;笨笨&quot;</span>))</span><br></pre></td></tr></table></figure>

<p>输出内容：<br>[(Document(page_content&#x3D;’笨笨是一只很喜欢睡觉的猫咪’), 0.699999988079071), (Document(page_content&#x3D;’猫咪在窗台上打盹，看起来非常可爱。’), 0.2090398222208023), (Document(page_content&#x3D;’我的狗喜欢追逐球，看起来非常开心。’), 0.19787956774234772), (Document(page_content&#x3D;’我的手机突然关机了，让我有些焦虑。’), 0.11435992270708084)]</p>
<p>在 Weaviate 中，也支持带过滤器的相似性筛选，并且 LangChain Weaviate 社区包并没有对筛选过滤器进行二次封装，所以直接传递原生的 weaviate 过滤器即可，参考文档：<a target="_blank" rel="noopener" href="https://weaviate.io/developers/weaviate/search/filters">https://weaviate.io/developers/weaviate/search/filters</a></p>
<p>例如需要检索 page 属性大于等于 5 的所有数据，可以构建一个 filters 后传递给检索方法，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> weaviate.classes.query <span class="keyword">import</span> Filter</span><br><span class="line">filters = Filter.by_property(<span class="string">&quot;page&quot;</span>).greater_or_equal(<span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(db.similarity_search_with_score(<span class="string">&quot;笨笨&quot;</span>, filters=filters))</span><br></pre></td></tr></table></figure>

<p>输出结果如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[(Document(page_content=<span class="string">&#x27;我的狗喜欢追逐球，看起来非常开心。&#x27;</span>, metadata=&#123;<span class="string">&#x27;page&#x27;</span>: 10.0, <span class="string">&#x27;account_id&#x27;</span>: None&#125;), 0.699999988079071), (Document(page_content=<span class="string">&#x27;我的手机突然关机了，让我有些焦虑。&#x27;</span>, metadata=&#123;<span class="string">&#x27;page&#x27;</span>: 7.0, <span class="string">&#x27;account_id&#x27;</span>: None&#125;), 0.4045487940311432), (Document(page_content=<span class="string">&#x27;昨晚我做了一个奇怪的梦，梦见自己在太空飞行。&#x27;</span>, metadata=&#123;<span class="string">&#x27;page&#x27;</span>: 6.0, <span class="string">&#x27;account_id&#x27;</span>: 1.0&#125;), 0.318904846906662), (Document(page_content=<span class="string">&#x27;我最喜欢的食物是意大利面，尤其是番茄酱的那种。&#x27;</span>, metadata=&#123;<span class="string">&#x27;page&#x27;</span>: 5.0, <span class="string">&#x27;account_id&#x27;</span>: None&#125;), 0.2671944797039032)]</span><br></pre></td></tr></table></figure>

<p>如果想获取 Weaviate 原始集合的实例，可以通过 <code>db._collection</code> 快速获得，从而去执行一些原始操作，例如</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> weaviate.classes.query <span class="keyword">import</span> MetadataQuery</span><br><span class="line"></span><br><span class="line">collection = db._collection</span><br><span class="line"></span><br><span class="line">response = collection.query.near_text(</span><br><span class="line">    query=<span class="string">&quot;a sweet German white wine&quot;</span>,</span><br><span class="line">    limit=<span class="number">2</span>,</span><br><span class="line">    target_vector=<span class="string">&quot;title_country&quot;</span>,  <span class="comment"># Specify the target vector for named vector collections</span></span><br><span class="line">    return_metadata=MetadataQuery(distance=<span class="literal">True</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> o <span class="keyword">in</span> response.objects:</span><br><span class="line">    <span class="built_in">print</span>(o.properties)</span><br><span class="line">    <span class="built_in">print</span>(o.metadata.distance)</span><br></pre></td></tr></table></figure>

<h3 id="1-5-4-自定义向量数据库"><a href="#1-5-4-自定义向量数据库" class="headerlink" title="1.5.4 自定义向量数据库"></a>1.5.4 自定义向量数据库</h3><p>向量数据库的发展非常迅猛，几乎间隔几天就有新的向量数据库发布，LangChain 不可能将所有向量数据库都进行集成，亦或者封装的包存在这一些 bug 或错误，这个时候就需要考虑创建自定义向量数据库，去实现特定的方法。</p>
<p>在 LangChain 实现自定义向量数据库的类有两种模式，一种是继承封装好的数据库类，一种是继承基类 VectorStore。前一种一般继承后重写部分方法进行扩展或者修复 bug，后面一种是对接新的向量数据库。</p>
<p>在 LangChain 中，继承 VectorStore 只需实现最基础的 3 个方法即可正常使用：<br>1. <strong>add_texts</strong>：将对应的数据添加到向量数据库中。<br>2. <strong>similarity_search</strong>：最基础的相似性搜索。<br>3. <strong>from_texts</strong>：从特定的文本列表、元数据列表中构建向量数据库。</p>
<p>其他方法因为使用频率并不高，VectorStore 并没有设置成虚拟方法，但是再没有实现的情况下，直接调用会报错，涵盖：<br>1. delete()：删除向量数据库中的数据。<br>2. <code>_select_relevance_score_fn()</code>：根据距离计算相似性得分函数。<br>3. <code>similarity_search_with_score()</code>：携带得分的相似性搜索函数。<br>4. similarity_search_by_vector()：传递向量进行相似性搜索。<br>5. max_marginal_relevance_search()：最大边界相似性搜索。<br>6. max_marginal_relevance_search_by_vector()：传递向量进行最大边界相关性搜索。</p>
<p><strong>代码示例：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> uuid  </span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">List</span>, <span class="type">Optional</span>, <span class="type">Any</span>, Iterable, <span class="type">Type</span>  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">import</span> dotenv  </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np  </span><br><span class="line"><span class="keyword">from</span> langchain_core.documents <span class="keyword">import</span> Document  </span><br><span class="line"><span class="keyword">from</span> langchain_core.embeddings <span class="keyword">import</span> Embeddings  </span><br><span class="line"><span class="keyword">from</span> langchain_core.vectorstores <span class="keyword">import</span> VectorStore  </span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> OpenAIEmbeddings  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MemoryVectorStore</span>(<span class="title class_ inherited__">VectorStore</span>):  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;基于内存+欧几里得距离的向量数据库&quot;&quot;&quot;</span>  </span><br><span class="line">    store: <span class="built_in">dict</span> = &#123;&#125;  <span class="comment"># 存储向量的临时变量  </span></span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, embedding: Embeddings</span>):  </span><br><span class="line">        self._embedding = embedding  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add_texts</span>(<span class="params">self, texts: Iterable[<span class="built_in">str</span>], metadatas: <span class="type">Optional</span>[<span class="type">List</span>[<span class="built_in">dict</span>]] = <span class="literal">None</span>, **kwargs: <span class="type">Any</span></span>) -&gt; <span class="type">List</span>[<span class="built_in">str</span>]:  </span><br><span class="line">        <span class="string">&quot;&quot;&quot;将数据添加到向量数据库中&quot;&quot;&quot;</span>  </span><br><span class="line">        <span class="comment"># 1.检测metadata的数据格式  </span></span><br><span class="line">        <span class="keyword">if</span> metadatas <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> <span class="built_in">len</span>(metadatas) != <span class="built_in">len</span>(texts):  </span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;metadatas格式错误&quot;</span>)  </span><br><span class="line">  </span><br><span class="line">        <span class="comment"># 2.将数据转换成文本嵌入/向量和ids  </span></span><br><span class="line">        embeddings = self._embedding.embed_documents(texts)  </span><br><span class="line">        ids = [<span class="built_in">str</span>(uuid.uuid4()) <span class="keyword">for</span> _ <span class="keyword">in</span> texts]  </span><br><span class="line">  </span><br><span class="line">        <span class="comment"># 3.通过for循环组装数据记录  </span></span><br><span class="line">        <span class="keyword">for</span> idx, text <span class="keyword">in</span> <span class="built_in">enumerate</span>(texts):  </span><br><span class="line">            self.store[ids[idx]] = &#123;  </span><br><span class="line">                <span class="string">&quot;id&quot;</span>: ids[idx],  </span><br><span class="line">                <span class="string">&quot;text&quot;</span>: text,  </span><br><span class="line">                <span class="string">&quot;vector&quot;</span>: embeddings[idx],  </span><br><span class="line">                <span class="string">&quot;metadata&quot;</span>: metadatas[idx] <span class="keyword">if</span> metadatas <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> &#123;&#125;,  </span><br><span class="line">            &#125;  </span><br><span class="line">  </span><br><span class="line">        <span class="keyword">return</span> ids  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">similarity_search</span>(<span class="params">self, query: <span class="built_in">str</span>, k: <span class="built_in">int</span> = <span class="number">4</span>, **kwargs: <span class="type">Any</span></span>) -&gt; <span class="type">List</span>[Document]:  </span><br><span class="line">        <span class="string">&quot;&quot;&quot;传入对应的query执行相似性搜索&quot;&quot;&quot;</span>  </span><br><span class="line">        <span class="comment"># 1.将query转换成向量  </span></span><br><span class="line">        embedding = self._embedding.embed_query(query)  </span><br><span class="line">  </span><br><span class="line">        <span class="comment"># 2.循环和store中的每一个向量进行比较，计算欧几里得距离  </span></span><br><span class="line">        result = []  </span><br><span class="line">        <span class="keyword">for</span> key, record <span class="keyword">in</span> self.store.items():  </span><br><span class="line">            distance = self._euclidean_distance(embedding, record[<span class="string">&quot;vector&quot;</span>])  </span><br><span class="line">            result.append(&#123;<span class="string">&quot;distance&quot;</span>: distance, **record&#125;)  </span><br><span class="line">  </span><br><span class="line">        <span class="comment"># 3.排序，欧几里得距离越小越靠前  </span></span><br><span class="line">        sorted_result = <span class="built_in">sorted</span>(result, key=<span class="keyword">lambda</span> x: x[<span class="string">&quot;distance&quot;</span>])  </span><br><span class="line">  </span><br><span class="line">        <span class="comment"># 4.取数据，取k条数据  </span></span><br><span class="line">        result_k = sorted_result[:k]  </span><br><span class="line">  </span><br><span class="line">        <span class="keyword">return</span> [  </span><br><span class="line">            Document(page_content=item[<span class="string">&quot;text&quot;</span>], metadata=&#123;**item[<span class="string">&quot;metadata&quot;</span>], <span class="string">&quot;score&quot;</span>: item[<span class="string">&quot;distance&quot;</span>]&#125;)  </span><br><span class="line">            <span class="keyword">for</span> item <span class="keyword">in</span> result_k  </span><br><span class="line">        ]  </span><br><span class="line">  </span><br><span class="line"><span class="meta">    @classmethod  </span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">from_texts</span>(<span class="params">cls: <span class="type">Type</span>[<span class="string">&quot;MemoryVectorStore&quot;</span>], texts: <span class="type">List</span>[<span class="built_in">str</span>], embedding: Embeddings,  </span></span><br><span class="line"><span class="params">                   metadatas: <span class="type">Optional</span>[<span class="type">List</span>[<span class="built_in">dict</span>]] = <span class="literal">None</span>,  </span></span><br><span class="line"><span class="params">                   **kwargs: <span class="type">Any</span></span>) -&gt; <span class="string">&quot;MemoryVectorStore&quot;</span>:  </span><br><span class="line">        <span class="string">&quot;&quot;&quot;从文本和元数据中去构建向量数据库&quot;&quot;&quot;</span>  </span><br><span class="line">        memory_vector_store = cls(embedding=embedding)  </span><br><span class="line">        memory_vector_store.add_texts(texts, metadatas, **kwargs)  </span><br><span class="line">        <span class="keyword">return</span> memory_vector_store  </span><br><span class="line">  </span><br><span class="line"><span class="meta">    @classmethod  </span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_euclidean_distance</span>(<span class="params">cls, vec1: <span class="built_in">list</span>, vec2: <span class="built_in">list</span></span>) -&gt; <span class="built_in">float</span>:  </span><br><span class="line">        <span class="string">&quot;&quot;&quot;计算两个向量的欧几里得距离&quot;&quot;&quot;</span>  </span><br><span class="line">        <span class="keyword">return</span> np.linalg.norm(np.array(vec1) - np.array(vec2))  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">dotenv.load_dotenv()  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 1.创建初始数据与嵌入模型  </span></span><br><span class="line">texts = [  </span><br><span class="line">    <span class="string">&quot;笨笨是一只很喜欢睡觉的猫咪&quot;</span>,  </span><br><span class="line">    <span class="string">&quot;我喜欢在夜晚听音乐，这让我感到放松。&quot;</span>,  </span><br><span class="line">    <span class="string">&quot;猫咪在窗台上打盹，看起来非常可爱。&quot;</span>,  </span><br><span class="line">    <span class="string">&quot;学习新技能是每个人都应该追求的目标。&quot;</span>,  </span><br><span class="line">    <span class="string">&quot;我最喜欢的食物是意大利面，尤其是番茄酱的那种。&quot;</span>,  </span><br><span class="line">    <span class="string">&quot;昨晚我做了一个奇怪的梦，梦见自己在太空飞行。&quot;</span>,  </span><br><span class="line">    <span class="string">&quot;我的手机突然关机了，让我有些焦虑。&quot;</span>,  </span><br><span class="line">    <span class="string">&quot;阅读是我每天都会做的事情，我觉得很充实。&quot;</span>,  </span><br><span class="line">    <span class="string">&quot;他们一起计划了一次周末的野餐，希望天气能好。&quot;</span>,  </span><br><span class="line">    <span class="string">&quot;我的狗喜欢追逐球，看起来非常开心。&quot;</span>,  </span><br><span class="line">]  </span><br><span class="line">metadatas = [  </span><br><span class="line">    &#123;<span class="string">&quot;page&quot;</span>: <span class="number">1</span>&#125;,  </span><br><span class="line">    &#123;<span class="string">&quot;page&quot;</span>: <span class="number">2</span>&#125;,  </span><br><span class="line">    &#123;<span class="string">&quot;page&quot;</span>: <span class="number">3</span>&#125;,  </span><br><span class="line">    &#123;<span class="string">&quot;page&quot;</span>: <span class="number">4</span>&#125;,  </span><br><span class="line">    &#123;<span class="string">&quot;page&quot;</span>: <span class="number">5</span>&#125;,  </span><br><span class="line">    &#123;<span class="string">&quot;page&quot;</span>: <span class="number">6</span>, <span class="string">&quot;account_id&quot;</span>: <span class="number">1</span>&#125;,  </span><br><span class="line">    &#123;<span class="string">&quot;page&quot;</span>: <span class="number">7</span>&#125;,  </span><br><span class="line">    &#123;<span class="string">&quot;page&quot;</span>: <span class="number">8</span>&#125;,  </span><br><span class="line">    &#123;<span class="string">&quot;page&quot;</span>: <span class="number">9</span>&#125;,  </span><br><span class="line">    &#123;<span class="string">&quot;page&quot;</span>: <span class="number">10</span>&#125;,  </span><br><span class="line">]  </span><br><span class="line">embedding = OpenAIEmbeddings(model=<span class="string">&quot;text-embedding-3-small&quot;</span>)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 2.构建自定义向量数据库  </span></span><br><span class="line">db = MemoryVectorStore(embedding=embedding)  </span><br><span class="line">  </span><br><span class="line">ids = db.add_texts(texts, metadatas)  </span><br><span class="line"><span class="built_in">print</span>(ids)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 3.执行检索  </span></span><br><span class="line"><span class="built_in">print</span>(db.similarity_search(<span class="string">&quot;笨笨是谁？&quot;</span>))</span><br></pre></td></tr></table></figure>

<h1 id="2-嵌入模型介绍和使用"><a href="#2-嵌入模型介绍和使用" class="headerlink" title="2 嵌入模型介绍和使用"></a>2 嵌入模型介绍和使用</h1><h2 id="2-1-嵌入模型介绍"><a href="#2-1-嵌入模型介绍" class="headerlink" title="2.1 嵌入模型介绍"></a>2.1 嵌入模型介绍</h2><p>要想使用向量数据库的相似性搜索，存储的数据必须是向量，那么如何将高维度的文字、图片、视频等非结构化数据转换成向量呢？这个时候就需要使用到 Embedding 嵌入模型了，例如下方就是 Embedding 嵌入模型的运行流程：<br><img src="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20250429164235.png" alt="image.png"></p>
<p>Embedding 模型是一种在机器学习和自然语言处理中广泛应用的技术，它旨在将高纬度的数据（如文字、图片、视频）映射到低纬度的空间。Embedding 向量是一个 N 维的实值向量，它将输入的数据表示成一个连续的数值空间中的点。这种嵌入可以是一个词、一个类别特征（如商品、电影、物品等）或时间序列特征等。<br>而且通过学习，<strong>Embedding 向量可以更准确地表示对应特征的内在含义，使几何距离相近的向量对应的物体有相近的含义</strong>，甚至对向量进行加减乘除算法都有意义！<br>一句话理解 Embedding：<strong>一种模型生成方法，可以将非结构化的数据，例如文本&#x2F;图片&#x2F;视频等数据映射成有意义的向量数据</strong>。</p>
<p>目前生成 embedding 方法的模型有以下 4 类：<br>1. <strong>Word2Vec（词嵌入模型）</strong>：这个模型通过学习将单词转化为连续的向量表示，以便计算机更好地理解和处理文本。Word2Vec 模型基于两种主要算法 CBOW 和 Skip-gram。<br>2. <strong>Glove</strong>：一种用于自然语言处理的词嵌入模型，它与其他常见的词嵌入模型（如 Word2Vec 和 FastText）类似，可以将单词转化为连续的向量表示。GloVe 模型的原理是通过观察单词在语料库中的共现关系，学习得到单词之间的语义关系。具体来说，GloVe 模型将共现概率矩阵表示为两个词向量之间的点积和偏差的关系，然后通过迭代优化来训练得到最佳的词向量表示。<br><strong>GloVe</strong> 模型的优点是它能够在大规模语料库上进行有损压缩，得到较小维度的词向量，同时保持了单词之间的语义关系。这些词向量可以被用于多种自然语言处理任务，如词义相似度计算、情感分析、文本分类等。<br>3. <strong>FastText</strong>：一种基于词袋模型的词嵌入技术，与其他常见的词嵌入模型（如 Word2Vec 和 GloVe）不同之处在于，FastText考虑了单词的子词信息。其核心思想是将单词视为字符的 n-grams 的集合，在训练过程中，模型会同时学习单词级别和n-gram级别的表示。这样可以捕捉到单词内部的细粒度信息，从而更好地处理各种形态和变体的单词。<br>4. <strong>大模型 Embeddings（重点）</strong>：和大模型相关的嵌入模型，如 OpenAI 官方发布的第二代模型：text-embedding-ada-002。它最长的输入是 8191 个tokens，输出的维度是 1536。</p>
<h2 id="2-3-Embedding-的价值"><a href="#2-3-Embedding-的价值" class="headerlink" title="2.3 Embedding 的价值"></a>2.3 Embedding 的价值</h2><p>1. <strong>降维</strong>：在许多实际问题中，原始数据的维度往往非常高。例如，在自然语言处理中，如果使用 Token 词表编码来表示词汇，其维度等于词汇表的大小，可能达到数十万甚至更高。通过 Embedding，我们可以将这些高维数据映射到一个低维空间，大大减少了模型的复杂度。<br>2. <strong>捕捉语义信息</strong>：Embedding 不仅仅是降维，更重要的是，它能够捕捉到数据的语义信息。例如，在词嵌入中，语义上相近的词在向量空间中也会相近。这意味着Embedding可以保留并利用原始数据的一些重要信息。<br>3. <strong>适应性</strong>： 与一些传统的特征提取方法相比，Embedding 是通过数据驱动的方式学习的。这意味着它能够自动适应数据的特性，而无需人工设计特征。<br>4. <strong>泛化能力</strong>：在实际问题中，我们经常需要处理一些在训练数据中没有出现过的数据。由于Embedding能够捕捉到数据的一些内在规律，因此对于这些未见过的数据，Embedding仍然能够给出合理的表示。<br>5. <strong>可解释性</strong>：尽管 Embedding 是高维的，但我们可以通过一些可视化工具（如t-SNE）来观察和理解 Embedding 的结构。这对于理解模型的行为，以及发现数据的一些潜在规律是非常有用的。</p>
<h2 id="2-4-CacheBackEmbedding-组件"><a href="#2-4-CacheBackEmbedding-组件" class="headerlink" title="2.4 CacheBackEmbedding 组件"></a>2.4 CacheBackEmbedding 组件</h2><p>通过嵌入模型计算传递数据的向量需要昂贵的算力，对于重复的内容，Embeddings 计算的结果肯定是一致的，如果数据重复仍然二次计算，会导致效率非常低，而且增加无用功。</p>
<p>所以在 LangChain 中提供了一个叫 CacheBackEmbedding 的包装类，一般通过类方法 from_bytes_store 进行实例化，它接受以下参数：<br>1. underlying_embedder：用于嵌入的嵌入模型。<br>2. document_embedding_cache：用于缓存文档嵌入的任何存储库（ByteStore）。<br>3. batch_size：可选参数，默认为 None，在存储更新之间要嵌入的文档数量。<br>4. namespace：可选参数，默认为“”，用于文档缓存的命名空间。此命名空间用于避免与其他缓存发生冲突。例如，将其设置为所使用的嵌入模型的名称。<br>5. query_embedding_cache：可选默认为 None 或者不缓存，用于缓存查询&#x2F;文本嵌入的 ByteStore，或这是为 True 以使用与 document_embedding_cache 相同的存储。</p>
<blockquote>
<p>CacheBackEmbedding 默认不缓存 embed_query 生成的向量，如果要缓存，需要设置 query_embedding_cache 的值，另外请尽可能设置 namespace，以避免使用不同嵌入模型嵌入的相同文本发生冲突。</p>
</blockquote>
<p><strong>代码示例：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> dotenv  </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np  </span><br><span class="line"><span class="keyword">from</span> langchain.embeddings <span class="keyword">import</span> CacheBackedEmbeddings  </span><br><span class="line"><span class="keyword">from</span> langchain.storage <span class="keyword">import</span> LocalFileStore  </span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> OpenAIEmbeddings  </span><br><span class="line"><span class="keyword">from</span> numpy.linalg <span class="keyword">import</span> norm  </span><br><span class="line">  </span><br><span class="line">dotenv.load_dotenv()  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cosine_similarity</span>(<span class="params">vector1: <span class="built_in">list</span>, vector2: <span class="built_in">list</span></span>) -&gt; <span class="built_in">float</span>:  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算传入两个向量的余弦相似度&quot;&quot;&quot;</span>  </span><br><span class="line">    <span class="comment"># 1.计算内积/点积  </span></span><br><span class="line">    dot_product = np.dot(vector1, vector2)  </span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 2.计算向量的范数/长度  </span></span><br><span class="line">    norm_vec1 = norm(vector1)  </span><br><span class="line">    norm_vec2 = norm(vector2)  </span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 3.计算余弦相似度  </span></span><br><span class="line">    <span class="keyword">return</span> dot_product / (norm_vec1 * norm_vec2)  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">embeddings = OpenAIEmbeddings(model=<span class="string">&quot;text-embedding-3-small&quot;</span>)  </span><br><span class="line">embeddings_with_cache = CacheBackedEmbeddings.from_bytes_store(  </span><br><span class="line">    embeddings,  </span><br><span class="line">    LocalFileStore(<span class="string">&quot;./cache/&quot;</span>),  </span><br><span class="line">    namespace=embeddings.model,  </span><br><span class="line">    query_embedding_cache=<span class="literal">True</span>,  </span><br><span class="line">)  </span><br><span class="line">  </span><br><span class="line">query_vector = embeddings_with_cache.embed_query(<span class="string">&quot;你好，我是慕小课，我喜欢打篮球&quot;</span>)  </span><br><span class="line">documents_vector = embeddings_with_cache.embed_documents([  </span><br><span class="line">    <span class="string">&quot;你好，我是慕小课，我喜欢打篮球&quot;</span>,  </span><br><span class="line">    <span class="string">&quot;这个喜欢打篮球的人叫慕小课&quot;</span>,  </span><br><span class="line">    <span class="string">&quot;求知若渴，虚心若愚&quot;</span>  </span><br><span class="line">])  </span><br><span class="line">  </span><br><span class="line"><span class="built_in">print</span>(query_vector)  </span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(query_vector))  </span><br><span class="line">  </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;============&quot;</span>)  </span><br><span class="line">  </span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(documents_vector))  </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;vector1与vector2的余弦相似度:&quot;</span>, cosine_similarity(documents_vector[<span class="number">0</span>], documents_vector[<span class="number">1</span>]))  </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;vector2与vector3的余弦相似度:&quot;</span>, cosine_similarity(documents_vector[<span class="number">0</span>], documents_vector[<span class="number">2</span>]))</span><br></pre></td></tr></table></figure>

<h2 id="2-5-HuggingFace-Embedding-模型的配置和使用"><a href="#2-5-HuggingFace-Embedding-模型的配置和使用" class="headerlink" title="2.5 HuggingFace Embedding 模型的配置和使用"></a>2.5 HuggingFace Embedding 模型的配置和使用</h2><h3 id="2-5-1-HuggingFace-本地模型"><a href="#2-5-1-HuggingFace-本地模型" class="headerlink" title="2.5.1 HuggingFace 本地模型"></a>2.5.1 HuggingFace 本地模型</h3><p>在某些对数据保密要求极高的场合下，数据不允许传递到外网，这个时候就可以考虑使用本地的文本嵌入模型——Hugging Face 本地嵌入模型，安装 langchain-huggingface 与 sentence-transformers 包，命令如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -U langchain-huggingface sentence-transformers</span><br></pre></td></tr></table></figure>

<p>其中 langchain-huggingface 是 langchain 团队基于 huggingface 封装的第三方社区包，sentence-transformers 是一个用于生成和使用预训练的文本嵌入，基于 transformer 架构，也是目前使用量最大的本地文本嵌入模型。<br>配置好后，就可以像正常的文本嵌入模型一样使用了，示例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_huggingface <span class="keyword">import</span> HuggingFaceEmbeddings  </span><br><span class="line">  </span><br><span class="line">embeddings = HuggingFaceEmbeddings(  </span><br><span class="line">    model_name=<span class="string">&quot;sentence-transformers/all-MiniLM-L12-v2&quot;</span>,  </span><br><span class="line">    cache_folder=<span class="string">&quot;./embeddings/&quot;</span>  </span><br><span class="line">)  </span><br><span class="line"></span><br><span class="line">query_vector = embeddings.embed_query(<span class="string">&quot;你好，我是慕小课，我喜欢打篮球游泳&quot;</span>)  </span><br><span class="line">  </span><br><span class="line"><span class="built_in">print</span>(query_vector)  </span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(query_vector))</span><br></pre></td></tr></table></figure>

<h3 id="2-5-2-HuggingFace远程嵌入模型"><a href="#2-5-2-HuggingFace远程嵌入模型" class="headerlink" title="2.5.2 HuggingFace远程嵌入模型"></a>2.5.2 HuggingFace远程嵌入模型</h3><p>部分模型的文件比较大，如果只是短期内调试，可以考虑使用 HuggingFace 提供的远程嵌入模型，首先安装对应的依赖</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install huggingface_hub</span><br></pre></td></tr></table></figure>
<p>然后在 Hugging Face 官网（<a target="_blank" rel="noopener" href="https://huggingface.com/">https://huggingface.co/</a>) 的 setting 中添加对应的访问秘钥，并配置到 .env 文件中</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HUGGINGFACEHUB_API_TOKEN=xxx</span><br></pre></td></tr></table></figure>
<p>接下来就可以使用 HuggingFace 提供的推理服务，这样在本地服务器上就无需配置对应的文本嵌入模型了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> dotenv  </span><br><span class="line"><span class="keyword">from</span> langchain_huggingface <span class="keyword">import</span> HuggingFaceEndpointEmbeddings  </span><br><span class="line">  </span><br><span class="line">dotenv.load_dotenv()  </span><br><span class="line">  </span><br><span class="line">embeddings = HuggingFaceEndpointEmbeddings(model=<span class="string">&quot;sentence-transformers/all-MiniLM-L12-v2&quot;</span>)  </span><br><span class="line">  </span><br><span class="line">query_vector = embeddings.embed_query(<span class="string">&quot;你好，我是慕小课，我喜欢打篮球游泳&quot;</span>)  </span><br><span class="line">  </span><br><span class="line"><span class="built_in">print</span>(query_vector)  </span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(query_vector))</span><br></pre></td></tr></table></figure>

<p>相关资料信息：<br>1. Hugging Face 官网：<a target="_blank" rel="noopener" href="https://huggingface.co/">https://huggingface.co/</a><br>2. HuggingFace 嵌入文档：<a target="_blank" rel="noopener" href="https://python.langchain.com/v0.2/docs/integrations/text_embedding/sentence_transformers/">https://python.langchain.com/v0.2/docs/integrations/text_embedding&#x2F;sentence_transformers&#x2F;</a><br>3. HuggingFace 嵌入翻译文档：<a target="_blank" rel="noopener" href="http://imooc-langchain.shortvar.com/docs/integrations/text_embedding/sentence_transformers/">http://imooc-langchain.shortvar.com/docs/integrations/text_embedding&#x2F;sentence_transformers&#x2F;</a></p>
<h1 id="3-文档加载器"><a href="#3-文档加载器" class="headerlink" title="3 文档加载器"></a>3 文档加载器</h1><h2 id="3-1-Document-与文档加载器"><a href="#3-1-Document-与文档加载器" class="headerlink" title="3.1 Document 与文档加载器"></a>3.1 Document 与文档加载器</h2><p>Document 类是 LangChain 中的核心组件，这个类定义了一个文档对象的结构，涵盖了文本内容和相关的元数据，Document 也是文档加载器、文档分割器、向量数据库、检索器这几个组件之间交互传递的状态数据。<br>在 LangChain 中所有文档加载器的基类为 BaseLoader，封装了统一的 5 个方法：<br>1. load()&#x2F;aload()：加载和异步加载文档，返回的数据为文档列表。<br>2. load_and_split()：传递分割器，加载并将大文档按照传入的分割器进行切割，返回的数据为分割后的文档列表。<br>3. lazy_load()&#x2F;alazy_load()：懒加载和异步懒加载文档，返回的是一个迭代器，适用于传递的数据源有多份文档的情况，例如文件夹加载器，可以每次获得最新的加载文档，不需要等到所有文档都加载完毕。</p>
<p>在 LangChain 中封装了上百种文档加载器，几乎所有的文件都可以使用这些加载器完成数据的读取，而不需要手动去封装<br><img src="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20250430095626.png" alt="image.png"></p>
<p><strong>代码示例：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_community.document_loaders <span class="keyword">import</span> TextLoader  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 1.构建加载器  </span></span><br><span class="line">loader = TextLoader(<span class="string">&quot;./电商产品数据.txt&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 2.加载数据  </span></span><br><span class="line">documents = loader.load()  </span><br><span class="line">  </span><br><span class="line"><span class="built_in">print</span>(documents)  </span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(documents))  </span><br><span class="line"><span class="built_in">print</span>(documents[<span class="number">0</span>].metadata)</span><br></pre></td></tr></table></figure>

<h2 id="3-2-内置文档加载器的使用技巧"><a href="#3-2-内置文档加载器的使用技巧" class="headerlink" title="3.2 内置文档加载器的使用技巧"></a>3.2 内置文档加载器的使用技巧</h2><p>LangChain 内置文档加载器文档：<a target="_blank" rel="noopener" href="https://imooc-langchain.shortvar.com/docs/integrations/document_loaders/">https://imooc-langchain.shortvar.com/docs/integrations/document_loaders&#x2F;</a></p>
<h3 id="3-2-1-Markdown-文档加载器"><a href="#3-2-1-Markdown-文档加载器" class="headerlink" title="3.2.1 Markdown 文档加载器"></a>3.2.1 Markdown 文档加载器</h3><p>LangChain 中封装了一个 UnstructuredMarkdownLoader 对象，要使用这个加载器，必须安装 unstructured 包，安装命令</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install unstructured</span><br></pre></td></tr></table></figure>

<p><strong>代码示例：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_community.document_loaders <span class="keyword">import</span> UnstructuredMarkdownLoader  </span><br><span class="line">  </span><br><span class="line">loader = UnstructuredMarkdownLoader(<span class="string">&quot;./项目API资料.md&quot;</span>)  </span><br><span class="line">documents = loader.load()  </span><br><span class="line">  </span><br><span class="line"><span class="built_in">print</span>(documents)  </span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(documents))  </span><br><span class="line"><span class="built_in">print</span>(documents[<span class="number">0</span>].metadata)</span><br></pre></td></tr></table></figure>

<h3 id="3-2-2-Office-文档加载器"><a href="#3-2-2-Office-文档加载器" class="headerlink" title="3.2.2 Office 文档加载器"></a>3.2.2 Office 文档加载器</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_community.document_loaders <span class="keyword">import</span> (  </span><br><span class="line">    UnstructuredPowerPointLoader,  </span><br><span class="line">)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># excel_loader = UnstructuredExcelLoader(&quot;./员工考勤表.xlsx&quot;, mode=&quot;elements&quot;)  </span></span><br><span class="line"><span class="comment"># excel_documents = excel_loader.load()  </span></span><br><span class="line">  </span><br><span class="line"><span class="comment"># word_loader = UnstructuredWordDocumentLoader(&quot;./喵喵.docx&quot;)  </span></span><br><span class="line"><span class="comment"># documents = word_loader.load()  </span></span><br><span class="line">  </span><br><span class="line">ppt_loader = UnstructuredPowerPointLoader(<span class="string">&quot;./章节介绍.pptx&quot;</span>)  </span><br><span class="line">documents = ppt_loader.load()  </span><br><span class="line">  </span><br><span class="line"><span class="built_in">print</span>(documents)  </span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(documents))  </span><br><span class="line"><span class="built_in">print</span>(documents[<span class="number">0</span>].metadata)</span><br></pre></td></tr></table></figure>

<h3 id="3-2-3-通用文档加载器"><a href="#3-2-3-通用文档加载器" class="headerlink" title="3.2.3 通用文档加载器"></a>3.2.3 通用文档加载器</h3><p>在实际的 LLM 应用开发中，由于数据的种类是无穷的，没办法单独为每一种数据配置一个加载器（也不现实），所以对于一些无法判断的数据类型或者想进行通用性文件加载，可以统一使用非结构化文件加载器 UnstructuredFileLoader 来实现对文件的加载。</p>
<p>UnstructuredFileLoader 是所有 UnstructuredXxxLoader 文档类的基类，其核心是将文档划分为元素，当传递一个文件时，库将读取文档，将其分割为多个部分，对这些部分进行分类，然后提取每个部分的文本，然后根据模式决定是否合并（single、paged、elements）。<br>一个 UnstructuredFileLoader 可以加载多种类型的文件，涵盖了：文本文件、PowerPoint 文件、HTML、PDF、图像、Markdown、Excel、Word 等</p>
<p>例如通过检测文件的扩展名来加载不同的文件加载器，对于没校验到的文件类型，才考虑使用 UnstructuredFileLoader，如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> file_extension <span class="keyword">in</span> [<span class="string">&quot;.xlsx&quot;</span>, <span class="string">&quot;.xls&quot;</span>]:</span><br><span class="line">    loader = UnstructuredExcelLoader(file_path)</span><br><span class="line"><span class="keyword">elif</span> file_extension == <span class="string">&quot;.pdf&quot;</span>:</span><br><span class="line">    loader = UnstructuredPDFLoader(file_path)</span><br><span class="line"><span class="keyword">elif</span> file_extension <span class="keyword">in</span> [<span class="string">&quot;.md&quot;</span>, <span class="string">&quot;.markdown&quot;</span>]:</span><br><span class="line">    loader = UnstructuredMarkdownLoader(file_path)</span><br><span class="line"><span class="keyword">elif</span> file_extension <span class="keyword">in</span> [<span class="string">&quot;.htm&quot;</span>, <span class="string">&quot;html&quot;</span>]:</span><br><span class="line">    loader = UnstructuredHTMLLoader(file_path)</span><br><span class="line"><span class="keyword">elif</span> file_extension <span class="keyword">in</span> [<span class="string">&quot;.docx&quot;</span>, <span class="string">&quot;.doc&quot;</span>]:</span><br><span class="line">    loader = UnstructuredWordDocumentLoader(file_path)</span><br><span class="line"><span class="keyword">elif</span> file_extension == <span class="string">&quot;.csv&quot;</span>:</span><br><span class="line">    loader = UnstructuredCSVLoader(file_path)</span><br><span class="line"><span class="keyword">elif</span> file_extension <span class="keyword">in</span> [<span class="string">&quot;.ppt&quot;</span>, <span class="string">&quot;.pptx&quot;</span>]:</span><br><span class="line">    loader = UnstructuredPowerPointLoader(file_path)</span><br><span class="line"><span class="keyword">elif</span> file_extension == <span class="string">&quot;.xml&quot;</span>:</span><br><span class="line">    loader = UnstructuredXMLLoader(file_path)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    loader = UnstructuredFileLoader(file_path) <span class="keyword">if</span> is_unstructured <span class="keyword">else</span> TextLoader(file_path)</span><br></pre></td></tr></table></figure>

<h3 id="3-2-4-自定义文档加载器"><a href="#3-2-4-自定义文档加载器" class="headerlink" title="3.2.4 自定义文档加载器"></a>3.2.4 自定义文档加载器</h3><p><strong>代码示例：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> Iterator, AsyncIterator  </span><br><span class="line"><span class="keyword">from</span> langchain_core.document_loaders <span class="keyword">import</span> BaseLoader  </span><br><span class="line"><span class="keyword">from</span> langchain_core.documents <span class="keyword">import</span> Document  </span><br><span class="line">    </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CustomDocumentLoader</span>(<span class="title class_ inherited__">BaseLoader</span>):  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;自定义文档加载器，将文本文件的每一行都解析成Document&quot;&quot;&quot;</span>  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, file_path: <span class="built_in">str</span></span>) -&gt; <span class="literal">None</span>:  </span><br><span class="line">        self.file_path = file_path  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">lazy_load</span>(<span class="params">self</span>) -&gt; Iterator[Document]:  </span><br><span class="line">        <span class="comment"># 1.读取对应的文件  </span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(self.file_path, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:  </span><br><span class="line">            line_number = <span class="number">0</span>  </span><br><span class="line">            <span class="comment"># 2.提取文件的每一行  </span></span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> f:  </span><br><span class="line">                <span class="comment"># 3.将每一行生成一个Document实例并通过yield返回  </span></span><br><span class="line">                <span class="keyword">yield</span> Document(  </span><br><span class="line">                    page_content=line,  </span><br><span class="line">                    metadata=&#123;<span class="string">&quot;score&quot;</span>: self.file_path, <span class="string">&quot;line_number&quot;</span>: line_number&#125;  </span><br><span class="line">                )  </span><br><span class="line">                line_number += <span class="number">1</span>  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">alazy_load</span>(<span class="params">self</span>) -&gt; AsyncIterator[Document]:  </span><br><span class="line">        <span class="keyword">import</span> aiofiles  </span><br><span class="line">        <span class="keyword">async</span> <span class="keyword">with</span> aiofiles.<span class="built_in">open</span>(self.file_path, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:  </span><br><span class="line">            line_number = <span class="number">0</span>  </span><br><span class="line">            <span class="keyword">async</span> <span class="keyword">for</span> line <span class="keyword">in</span> f:  </span><br><span class="line">                <span class="keyword">yield</span> Document(  </span><br><span class="line">                    page_content=line,  </span><br><span class="line">                    metadata=&#123;<span class="string">&quot;score&quot;</span>: self.file_path, <span class="string">&quot;line_number&quot;</span>: line_number&#125;  </span><br><span class="line">                )  </span><br><span class="line">                line_number += <span class="number">1</span>  </span><br><span class="line">    </span><br><span class="line">loader = CustomDocumentLoader(<span class="string">&quot;./喵喵.txt&quot;</span>)  </span><br><span class="line">documents = loader.load()  </span><br><span class="line">  </span><br><span class="line"><span class="built_in">print</span>(documents)  </span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(documents))  </span><br><span class="line"><span class="built_in">print</span>(documents[<span class="number">0</span>].metadata)</span><br></pre></td></tr></table></figure>

<p>lazy_load() 方法的两个核心步骤就是：读取文件数据、将文件数据解析成Document，并且绝大部分文档加载器都有着两个核心步骤，而且 读取文件数据 这个步骤大家都大差不差。</p>
<p>就像 <code>*.md、*.txt、*.py</code> 这类文本文件，甚至是 <code>*.pdf、*.doc</code> 等这类非文本文件，都可以使用同一个 读取文件数据 步骤将文件读取为 二进制内容，然后在使用不同的解析逻辑来解析对应的二进制内容，所以很容易可以得出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">文档加载器 = 二进制数据读取 + 解析逻辑</span><br></pre></td></tr></table></figure>
<p>因此，在项目开发中，如果大量配置自定义文档解析器的话，将解析逻辑与加载逻辑分离，维护起来会更容易，而且也更容易复用相应的逻辑（具体使用哪种方式取决于开发）。</p>
<p>这样原先的 DocumentLoader 运行流程就变成了如下：<br><img src="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20250430100854.png" alt="image.png"></p>
<h1 id="4-文本分割器"><a href="#4-文本分割器" class="headerlink" title="4 文本分割器"></a>4 文本分割器</h1><h2 id="4-1-DocumentTransformer-组件"><a href="#4-1-DocumentTransformer-组件" class="headerlink" title="4.1 DocumentTransformer 组件"></a>4.1 DocumentTransformer 组件</h2><p>在 LangChain 中针对文档的转换也统一封装了一个基类 BaseDocumentTransformer，<strong>所有涉及到文档的转换的类均是该类的子类</strong>，将大块文档切割成 chunk 分块的文档分割器也是 BaseDocumentTransformer 的子类实现。</p>
<p>BaseDocumentTransformer 基类封装了两个方法：<br>1. transform_documents()：抽象方法，传递文档列表，返回转换后的文档列表。<br>2. atransform_documents()：转换文档列表函数的异步实现，如果没有实现，则会委托 transform_documents() 函数实现。</p>
<p>在 LangChain 中，文档转换组件分成了两类：文档分割器(使用频率高)、文档处理转换器(使用频率低，老版本写法)。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -qU langchain-text-splitters</span><br></pre></td></tr></table></figure>
<h2 id="4-2-字符分割器"><a href="#4-2-字符分割器" class="headerlink" title="4.2 字符分割器"></a>4.2 字符分割器</h2><p>在文档分割器中，最简单的分割器就是——<strong>字符串分割器</strong>，这个组件会基于给定的字符串进行分割，默认为 \n\n，并且在分割时会尽可能保证数据的连续性。分割出来每一块的长度是通过字符数来衡量的，使用起来也非常简单，实例化 CharacterTextSplitter 需传递多个参数，信息如下：</p>
<p>1. separator：分隔符，默认为 <code>\n\n</code>。<br>2. is_separator_regex：是否正则表达式，默认为 False。<br>3. chunk_size：每块文档的内容大小，默认为 4000。<br>4. chunk_overlap：块与块之间重叠的内容大小，默认为 200。<br>5. length_function：计算文本长度的函数，默认为 len。<br>6. keep_separator：是否将分隔符保留到分割的块中，默认为 False。<br>7. add_start_index：是否添加开始索引，默认为 False，如果是的话会在元数据中添加该切块的起点。<br>8. strip_whitespace：是否删除文档头尾的空白，默认为 True。</p>
<p>如果想将文档切割为不超过 500 字符，并且每块之间文本重叠 50 个字符，可以使用 CharacterTextSplitter 来实现，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_community.document_loaders <span class="keyword">import</span> UnstructuredMarkdownLoader  </span><br><span class="line"><span class="keyword">from</span> langchain_text_splitters <span class="keyword">import</span> CharacterTextSplitter  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 1.加载对应的文档  </span></span><br><span class="line">loader = UnstructuredMarkdownLoader(<span class="string">&quot;./项目API文档.md&quot;</span>)  </span><br><span class="line">documents = loader.load()  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 2.创建文本分割器  </span></span><br><span class="line">text_splitter = CharacterTextSplitter(  </span><br><span class="line">    separator=<span class="string">&quot;\n\n&quot;</span>,  </span><br><span class="line">    chunk_size=<span class="number">500</span>,  </span><br><span class="line">    chunk_overlap=<span class="number">50</span>,  </span><br><span class="line">    add_start_index=<span class="literal">True</span>,  </span><br><span class="line">)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 3.分割文本  </span></span><br><span class="line">chunks = text_splitter.split_documents(documents)  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">for</span> chunk <span class="keyword">in</span> chunks:  </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;块大小:<span class="subst">&#123;<span class="built_in">len</span>(chunk.page_content)&#125;</span>, 元数据:<span class="subst">&#123;chunk.metadata&#125;</span>&quot;</span>)  </span><br><span class="line">  </span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(chunks))</span><br></pre></td></tr></table></figure>

<h2 id="4-3-递归字符文本分割器"><a href="#4-3-递归字符文本分割器" class="headerlink" title="4.3 递归字符文本分割器"></a>4.3 递归字符文本分割器</h2><p>普通的字符文本分割器只能使用单个分隔符对文本内容进行划分，在划分的过程中，可能会出现文档块 过小 或者 过大 的情况，这会让 RAG 变得不可控，例如：<br>1. <strong>文档块可能会变得非常大</strong>，极端的情况下某个块的内容长度可能就超过了 LLM 的上下文长度限制，这样这个文本块永远不会被引用到，相当于存储了数据，但是数据又丢失了。<br>2. <strong>文档块可能会远远小于窗口大小</strong>，导致文档块的信息密度太低，块内容即使填充到 Prompt 中，LLM 也无法提取出有用的信息。</p>
<p>RecursiveCharacterTextSplitter，即<strong>递归字符串分割</strong>，这个分割器可以传递 一组分隔符 和 设定块内容大小，根据分隔符的优先顺序对文本进行预分割，然后将小块进行合并，将大块进行递归分割，直到获得所需块的大小，最终这些文档块的大小并不能完全相同，但是仍然会逼近指定长度。<br>RecursiveCharacterTextSplitter 的分隔符参数默认为 <code>[&quot;\n\n&quot;, &quot;\n&quot;, &quot; &quot;, &quot;&quot;]</code>，即优先使用换两行的数据进行分割，然后在使用单个换行符，如果块内容还是太大，则使用空格，最后再拆分成单个字符。<br>所以如果使用默认参数，这个字符文本分割器最后得到的文档块长度一定不会超过预设的大小，但是仍然会有小概率出现远小于的情况（目前也没有很好的解决方案）。</p>
<p><strong>代码示例：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_community.document_loaders <span class="keyword">import</span> UnstructuredMarkdownLoader  </span><br><span class="line"><span class="keyword">from</span> langchain_text_splitters <span class="keyword">import</span> RecursiveCharacterTextSplitter  </span><br><span class="line">  </span><br><span class="line">loader = UnstructuredMarkdownLoader(<span class="string">&quot;./项目API文档.md&quot;</span>)  </span><br><span class="line">documents = loader.load()  </span><br><span class="line">  </span><br><span class="line">text_splitter = RecursiveCharacterTextSplitter(  </span><br><span class="line">    chunk_size=<span class="number">500</span>,  </span><br><span class="line">    chunk_overlap=<span class="number">50</span>,  </span><br><span class="line">    add_start_index=<span class="literal">True</span>,  </span><br><span class="line">)  </span><br><span class="line">  </span><br><span class="line">chunks = text_splitter.split_documents(documents)  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">for</span> chunk <span class="keyword">in</span> chunks:  </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;块大小: <span class="subst">&#123;<span class="built_in">len</span>(chunk.page_content)&#125;</span>, 元数据: <span class="subst">&#123;chunk.metadata&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20250430102610.png" alt="image.png"></p>
<h2 id="4-4-语义文档分割器"><a href="#4-4-语义文档分割器" class="headerlink" title="4.4 语义文档分割器"></a>4.4 语义文档分割器</h2><p>语义相似性分割器，SemanticChunker 在使用上和其他的文档分割器存在一些差异，并且该类并没有继承 TextSplitter，实例化参数含义如下：</p>
<p>1. embeddings：文本嵌入模型，在该分类器底层使用向量的 余弦相似度 来识别语句之间的相似性。<br>2. buffer_size：文本缓冲区大小，默认为 1，即在计算相似性时，该文本会叠加前后各 1 条文本，如果不够则不叠加（例如第 1 条和最后 1 条）。<br>3. add_start_index：是否添加起点索引，默认为 False。<br>4. breakpoint_threshold_type：断点阈值类型，默认为 percentile 即百分位<br>5. breakpoint_threshold_amount：断点阈值金额&#x2F;得分。<br>6. number_of_chunks：分割后的文档块个数，默认为 None。<br>7. sentence_split_regex：句子切割正则，默认为 <code>(?&lt;=[.?!])\s+</code>，即以英文的点、问号、感叹号切割语句，不同的文档需要传递不同的切割正则表达式。</p>
<p><strong>代码示例：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> dotenv  </span><br><span class="line"><span class="keyword">from</span> langchain_community.document_loaders <span class="keyword">import</span> UnstructuredFileLoader  </span><br><span class="line"><span class="keyword">from</span> langchain_experimental.text_splitter <span class="keyword">import</span> SemanticChunker  </span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> OpenAIEmbeddings  </span><br><span class="line">  </span><br><span class="line">dotenv.load_dotenv()  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 1.构建加载器和文本分割器  </span></span><br><span class="line">loader = UnstructuredFileLoader(<span class="string">&quot;./科幻短篇.txt&quot;</span>)  </span><br><span class="line">text_splitter = SemanticChunker(  </span><br><span class="line">    embeddings=OpenAIEmbeddings(model=<span class="string">&quot;text-embedding-3-small&quot;</span>),  </span><br><span class="line">    number_of_chunks=<span class="number">10</span>,  </span><br><span class="line">    add_start_index=<span class="literal">True</span>,  </span><br><span class="line">    sentence_split_regex=<span class="string">r&quot;(?&lt;=[。？！.?!])&quot;</span>  </span><br><span class="line">)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 2.加载文本与分割  </span></span><br><span class="line">documents = loader.load()  </span><br><span class="line">chunks = text_splitter.split_documents(documents)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 3.循环打印  </span></span><br><span class="line"><span class="keyword">for</span> chunk <span class="keyword">in</span> chunks:  </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;块大小: <span class="subst">&#123;<span class="built_in">len</span>(chunk.page_content)&#125;</span>, 元数据: <span class="subst">&#123;chunk.metadata&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="4-5-自定义文档分割器"><a href="#4-5-自定义文档分割器" class="headerlink" title="4.5 自定义文档分割器"></a>4.5 自定义文档分割器</h2><p><strong>代码示例：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">List</span>  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">import</span> jieba.analyse  </span><br><span class="line"><span class="keyword">from</span> langchain_community.document_loaders <span class="keyword">import</span> UnstructuredFileLoader  </span><br><span class="line"><span class="keyword">from</span> langchain_text_splitters <span class="keyword">import</span> TextSplitter  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CustomTextSplitter</span>(<span class="title class_ inherited__">TextSplitter</span>):  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;自定义文本分割器&quot;&quot;&quot;</span>  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, seperator: <span class="built_in">str</span>, top_k: <span class="built_in">int</span> = <span class="number">10</span>, **kwargs</span>):  </span><br><span class="line">        <span class="string">&quot;&quot;&quot;构造函数，传递分割器还有需要提取的关键词数，默认为10&quot;&quot;&quot;</span>  </span><br><span class="line">        <span class="built_in">super</span>().__init__(**kwargs)  </span><br><span class="line">        self._seperator = seperator  </span><br><span class="line">        self._top_k = top_k  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">split_text</span>(<span class="params">self, text: <span class="built_in">str</span></span>) -&gt; <span class="type">List</span>[<span class="built_in">str</span>]:  </span><br><span class="line">        <span class="string">&quot;&quot;&quot;传递对应的文本执行分割并提取分割数据的关键词，组成文档列表返回&quot;&quot;&quot;</span>  </span><br><span class="line">        <span class="comment"># 1.根据传递的分隔符分割传入的文本  </span></span><br><span class="line">        split_texts = text.split(self._seperator)  </span><br><span class="line">  </span><br><span class="line">        <span class="comment"># 2.提取分割出来的每一段文本的关键词，数量为self._top_k个  </span></span><br><span class="line">        text_keywords = []  </span><br><span class="line">        <span class="keyword">for</span> split_text <span class="keyword">in</span> split_texts:  </span><br><span class="line">            text_keywords.append(jieba.analyse.extract_tags(split_text, self._top_k))  </span><br><span class="line">  </span><br><span class="line">        <span class="comment"># 3.将关键词使用逗号进行拼接组成字符串列表并返回  </span></span><br><span class="line">        <span class="keyword">return</span> [<span class="string">&quot;,&quot;</span>.join(keywords) <span class="keyword">for</span> keywords <span class="keyword">in</span> text_keywords]  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 1.创建加载器与分割器  </span></span><br><span class="line">loader = UnstructuredFileLoader(<span class="string">&quot;./科幻短篇.txt&quot;</span>)  </span><br><span class="line">text_splitter = CustomTextSplitter(<span class="string">&quot;\n\n&quot;</span>, <span class="number">10</span>)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 2.加载文档并分割  </span></span><br><span class="line">documents = loader.load()  </span><br><span class="line">chunks = text_splitter.split_documents(documents)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 3.循环遍历文档信息  </span></span><br><span class="line"><span class="keyword">for</span> chunk <span class="keyword">in</span> chunks:  </span><br><span class="line">    <span class="built_in">print</span>(chunk.page_content)</span><br></pre></td></tr></table></figure>
<h2 id="4-6-非分割类型的文档分割器"><a href="#4-6-非分割类型的文档分割器" class="headerlink" title="4.6 非分割类型的文档分割器"></a>4.6 非分割类型的文档分割器</h2><p>在 LangChain 中，还存在另一种非分割类型的文档转换器，这类转换器也是传递 文档列表 并返回 文档列表，一般是将某种文档按照需求转换成另外一种格式（例如：<strong>翻译文档、文档重排、HTML 转文本、文档元数据提取、文档转问答</strong>等）</p>
<h3 id="4-6-1-问答转换器"><a href="#4-6-1-问答转换器" class="headerlink" title="4.6.1 问答转换器"></a>4.6.1 问答转换器</h3><p>在 RAG 的外挂知识库中，向量存储知识库中使用的文档通常以叙述或对话格式存储。但是，绝大部分用户的查询都是问题格式，所以如果我们在对文档进行向量化之前先将其转换为 问答格式，可以在一定程度上增加检索相关文档的可能性，降低检索不相关文档的可能性。</p>
<p>这个技巧也是 RAG 应用开发中常见的一种优化策略，即将原始数据转换成 QA 数据后进行存储，除此之外，对于绝大部分 LLM 的微调，使用的也是 QA问答数据 也可以考虑使用该问答转换器进行转换。</p>
<p>在 LangChain 中封装了 Doctran 库并实现了 DoctranQATransformer 类可以快捷实现该功能，这个库底层使用 OpenAI 的函数回调来实现对问答数据的提取，首先安装该库</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -U doctran</span><br></pre></td></tr></table></figure>

<p><strong>代码示例：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> dotenv  </span><br><span class="line"><span class="keyword">from</span> doctran <span class="keyword">import</span> Doctran  </span><br><span class="line"><span class="keyword">from</span> langchain_community.document_transformers <span class="keyword">import</span> DoctranQATransformer  </span><br><span class="line"><span class="keyword">from</span> langchain_core.documents <span class="keyword">import</span> Document  </span><br><span class="line">  </span><br><span class="line">_ = Doctran  </span><br><span class="line">  </span><br><span class="line">dotenv.load_dotenv()  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 1.构建文档列表  </span></span><br><span class="line">page_content = <span class="string">&quot;&quot;&quot;机密文件 - 仅供内部使用  </span></span><br><span class="line"><span class="string">日期：2023年7月1日  </span></span><br><span class="line"><span class="string">主题：各种话题的更新和讨论  </span></span><br><span class="line"><span class="string">亲爱的团队，  </span></span><br><span class="line"><span class="string">希望这封邮件能找到你们一切安好。在这份文件中，我想向你们提供一些重要的更新，并讨论需要我们关注的各种话题。请将此处包含的信息视为高度机密。  </span></span><br><span class="line"><span class="string">安全和隐私措施  </span></span><br><span class="line"><span class="string">作为我们不断致力于确保客户数据安全和隐私的一部分，我们已在所有系统中实施了强有力的措施。我们要赞扬IT部门的John Doe（电子邮件：john.doe@example.com）在增强我们网络安全方面的勤奋工作。未来，我们提醒每个人严格遵守我们的数据保护政策和准则。此外，如果您发现任何潜在的安全风险或事件，请立即向我们专门的团队报告，联系邮箱为security@example.com。  </span></span><br><span class="line"><span class="string">人力资源更新和员工福利  </span></span><br><span class="line"><span class="string">最近，我们迎来了几位为各自部门做出重大贡献的新团队成员。我要表扬Jane Smith（社保号：049-45-5928）在客户服务方面的出色表现。Jane一直受到客户的积极反馈。此外，请记住我们的员工福利计划的开放报名期即将到来。如果您有任何问题或需要帮助，请联系我们的人力资源代表Michael Johnson（电话：418-492-3850，电子邮件：michael.johnson@example.com）。  </span></span><br><span class="line"><span class="string">营销倡议和活动  </span></span><br><span class="line"><span class="string">我们的营销团队一直在积极制定新策略，以提高品牌知名度并推动客户参与。我们要感谢Sarah Thompson（电话：415-555-1234）在管理我们的社交媒体平台方面的杰出努力。Sarah在过去一个月内成功将我们的关注者基数增加了20%。此外，请记住7月15日即将举行的产品发布活动。我们鼓励所有团队成员参加并支持我们公司的这一重要里程碑。  </span></span><br><span class="line"><span class="string">研发项目  </span></span><br><span class="line"><span class="string">在追求创新的过程中，我们的研发部门一直在为各种项目不懈努力。我要赞扬David Rodriguez（电子邮件：david.rodriguez@example.com）在项目负责人角色中的杰出工作。David对我们尖端技术的发展做出了重要贡献。此外，我们希望每个人在7月10日定期举行的研发头脑风暴会议上分享他们的想法和建议，以开展潜在的新项目。  </span></span><br><span class="line"><span class="string">请将此文档中的信息视为最机密，并确保不与未经授权的人员分享。如果您对讨论的话题有任何疑问或顾虑，请随时直接联系我。  </span></span><br><span class="line"><span class="string">感谢您的关注，让我们继续共同努力实现我们的目标。  </span></span><br><span class="line"><span class="string">此致，  </span></span><br><span class="line"><span class="string">Jason Fan  </span></span><br><span class="line"><span class="string">联合创始人兼首席执行官  </span></span><br><span class="line"><span class="string">Psychic  </span></span><br><span class="line"><span class="string">jason@psychic.dev&quot;&quot;&quot;</span>  </span><br><span class="line">documents = [Document(page_content=page_content)]  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 2.构建问答转换器并转换  </span></span><br><span class="line">qa_transformer = DoctranQATransformer(openai_api_model=<span class="string">&quot;gpt-3.5-turbo-16k&quot;</span>)  </span><br><span class="line">transformer_documents = qa_transformer.transform_documents(documents)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 3.输出内容  </span></span><br><span class="line"><span class="keyword">for</span> qa <span class="keyword">in</span> transformer_documents[<span class="number">0</span>].metadata.get(<span class="string">&quot;questions_and_answers&quot;</span>):  </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;问答数据:&quot;</span>, qa)</span><br></pre></td></tr></table></figure>
<p><strong>输出内容：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;question&#x27;</span>: <span class="string">&#x27;文件日期是什么？&#x27;</span>, <span class="string">&#x27;answer&#x27;</span>: <span class="string">&#x27;2023年7月1日&#x27;</span>&#125;</span><br><span class="line">&#123;<span class="string">&#x27;question&#x27;</span>: <span class="string">&#x27;文件主题是什么？&#x27;</span>, <span class="string">&#x27;answer&#x27;</span>: <span class="string">&#x27;各种话题的更新和讨论&#x27;</span>&#125;</span><br><span class="line">&#123;<span class="string">&#x27;question&#x27;</span>: <span class="string">&#x27;谁是IT部门的网络安全负责人？&#x27;</span>, <span class="string">&#x27;answer&#x27;</span>: <span class="string">&#x27;John Doe（电子邮件：john.doe@example.com）&#x27;</span>&#125;</span><br><span class="line">&#123;<span class="string">&#x27;question&#x27;</span>: <span class="string">&#x27;如果发现安全风险或事件，应该向谁报告？&#x27;</span>, <span class="string">&#x27;answer&#x27;</span>: <span class="string">&#x27;专门的团队，联系邮箱为security@example.com&#x27;</span>&#125;</span><br><span class="line">&#123;<span class="string">&#x27;question&#x27;</span>: <span class="string">&#x27;谁在客户服务方面表现出色？&#x27;</span>, <span class="string">&#x27;answer&#x27;</span>: <span class="string">&#x27;Jane Smith（社保号：049-45-5928）&#x27;</span>&#125;</span><br><span class="line">&#123;<span class="string">&#x27;question&#x27;</span>: <span class="string">&#x27;员工福利计划的开放报名期是什么时候？&#x27;</span>, <span class="string">&#x27;answer&#x27;</span>: <span class="string">&#x27;即将到来&#x27;</span>&#125;</span><br><span class="line">&#123;<span class="string">&#x27;question&#x27;</span>: <span class="string">&#x27;人力资源代表的联系信息是什么？&#x27;</span>, <span class="string">&#x27;answer&#x27;</span>: <span class="string">&#x27;Michael Johnson（电话：418-492-3850，电子邮件：michael.johnson@example.com）&#x27;</span>&#125;</span><br><span class="line">&#123;<span class="string">&#x27;question&#x27;</span>: <span class="string">&#x27;谁在管理社交媒体平台方面做出了杰出努力？&#x27;</span>, <span class="string">&#x27;answer&#x27;</span>: <span class="string">&#x27;Sarah Thompson（电话：415-555-1234）&#x27;</span>&#125;</span><br><span class="line">&#123;<span class="string">&#x27;question&#x27;</span>: <span class="string">&#x27;产品发布活动的日期是什么时候？&#x27;</span>, <span class="string">&#x27;answer&#x27;</span>: <span class="string">&#x27;7月15日&#x27;</span>&#125;</span><br><span class="line">&#123;<span class="string">&#x27;question&#x27;</span>: <span class="string">&#x27;谁在研发部门担任项目负责人角色？&#x27;</span>, <span class="string">&#x27;answer&#x27;</span>: <span class="string">&#x27;David Rodriguez（电子邮件：david.rodriguez@example.com）&#x27;</span>&#125;</span><br><span class="line">&#123;<span class="string">&#x27;question&#x27;</span>: <span class="string">&#x27;研发头脑风暴会议的日期是什么时候？&#x27;</span>, <span class="string">&#x27;answer&#x27;</span>: <span class="string">&#x27;7月10日&#x27;</span>&#125;</span><br></pre></td></tr></table></figure>
<h3 id="4-6-2-翻译转换器"><a href="#4-6-2-翻译转换器" class="headerlink" title="4.6.2 翻译转换器"></a>4.6.2 翻译转换器</h3><p>在 RAG 应用开发中，将文档通过嵌入&#x2F;向量的方式进行比较的好处在于能跨语言工作，例如：你好，世界！、Hello, World! 和 こんにちは、世界！ 分别是 中英日 三国的语言，但是因为语义相近，所以在向量空间中的位置也是非常接近的。</p>
<p>当一个 RAG 应用需要跨语言工作时，一般有两种策略：<br>1. 在将文档切块并嵌入存储到向量数据库时，同时将文档翻译成多国语言并进行相同的操作。<br>2. 在进行检索操作时，将检索出来的文档执行翻译功能，然后使用翻译后的文档。<br>这两种策略都涉及到一个功能，就是 文档的翻译，或者是说将 文档 转换成另外一种形式的 文档，这类操作其实和 文档转换器 的作用一模一样，所以可以考虑使用该组件来实现这个功能，LangChain 中针对翻译的转换器就提供了不少，例如 Doctran。</p>
<h1 id="5-文档检索器"><a href="#5-文档检索器" class="headerlink" title="5 文档检索器"></a>5 文档检索器</h1><h2 id="5-1-带得分阈值的相似性搜索"><a href="#5-1-带得分阈值的相似性搜索" class="headerlink" title="5.1 带得分阈值的相似性搜索"></a>5.1 带得分阈值的相似性搜索</h2><p>在 LangChain 的相似性搜索中，无论结果多不匹配，只要向量数据库中存在数据，一定会查找出相应的结果，在 RAG 应用开发中，一般是将高相似文档插入到 Prompt 中，所以可以考虑添加一个 相似性得分阈值，超过该数值的部分才等同于有相似性。<br>在 similarity_search_with_relevance_scores() 函数中，可以传递 score_threshold 阈值参数，过滤低于该得分的文档。<br>例如没有添加阈值检索 我养了一只猫，叫笨笨，示例与输出如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> dotenv  </span><br><span class="line"><span class="keyword">from</span> langchain_community.vectorstores <span class="keyword">import</span> FAISS  </span><br><span class="line"><span class="keyword">from</span> langchain_core.documents <span class="keyword">import</span> Document  </span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> OpenAIEmbeddings  </span><br><span class="line">  </span><br><span class="line">dotenv.load_dotenv()  </span><br><span class="line">  </span><br><span class="line">embedding = OpenAIEmbeddings(model=<span class="string">&quot;text-embedding-3-small&quot;</span>)  </span><br><span class="line">  </span><br><span class="line">documents = [  </span><br><span class="line">    Document(page_content=<span class="string">&quot;笨笨是一只很喜欢睡觉的猫咪&quot;</span>, metadata=&#123;<span class="string">&quot;page&quot;</span>: <span class="number">1</span>&#125;),  </span><br><span class="line">    Document(page_content=<span class="string">&quot;我喜欢在夜晚听音乐，这让我感到放松。&quot;</span>, metadata=&#123;<span class="string">&quot;page&quot;</span>: <span class="number">2</span>&#125;),  </span><br><span class="line">    Document(page_content=<span class="string">&quot;猫咪在窗台上打盹，看起来非常可爱。&quot;</span>, metadata=&#123;<span class="string">&quot;page&quot;</span>: <span class="number">3</span>&#125;),  </span><br><span class="line">    Document(page_content=<span class="string">&quot;学习新技能是每个人都应该追求的目标。&quot;</span>, metadata=&#123;<span class="string">&quot;page&quot;</span>: <span class="number">4</span>&#125;),  </span><br><span class="line">    Document(page_content=<span class="string">&quot;我最喜欢的食物是意大利面，尤其是番茄酱的那种。&quot;</span>, metadata=&#123;<span class="string">&quot;page&quot;</span>: <span class="number">5</span>&#125;),  </span><br><span class="line">    Document(page_content=<span class="string">&quot;昨晚我做了一个奇怪的梦，梦见自己在太空飞行。&quot;</span>, metadata=&#123;<span class="string">&quot;page&quot;</span>: <span class="number">6</span>&#125;),  </span><br><span class="line">    Document(page_content=<span class="string">&quot;我的手机突然关机了，让我有些焦虑。&quot;</span>, metadata=&#123;<span class="string">&quot;page&quot;</span>: <span class="number">7</span>&#125;),  </span><br><span class="line">    Document(page_content=<span class="string">&quot;阅读是我每天都会做的事情，我觉得很充实。&quot;</span>, metadata=&#123;<span class="string">&quot;page&quot;</span>: <span class="number">8</span>&#125;),  </span><br><span class="line">    Document(page_content=<span class="string">&quot;他们一起计划了一次周末的野餐，希望天气能好。&quot;</span>, metadata=&#123;<span class="string">&quot;page&quot;</span>: <span class="number">9</span>&#125;),  </span><br><span class="line">    Document(page_content=<span class="string">&quot;我的狗喜欢追逐球，看起来非常开心。&quot;</span>, metadata=&#123;<span class="string">&quot;page&quot;</span>: <span class="number">10</span>&#125;),  </span><br><span class="line">]  </span><br><span class="line">db = FAISS.from_documents(documents, embedding)  </span><br><span class="line">  </span><br><span class="line"><span class="built_in">print</span>(db.similarity_search_with_relevance_scores(<span class="string">&quot;我养了一只猫，叫笨笨&quot;</span>, score_threshold=<span class="number">0.4</span>))</span><br></pre></td></tr></table></figure>

<h2 id="5-2-as-retriever-检索器"><a href="#5-2-as-retriever-检索器" class="headerlink" title="5.2 as_retriever() 检索器"></a>5.2 as_retriever() 检索器</h2><p>在 LangChain 中，VectorStore 可以通过 as_retriever() 方法转换成检索器，在 as_retriever() 中可以传递一下参数：</p>
<p>1. search_type：搜索类型，支持 similarity(基础相似性搜索)、similarity_score_threshold(携带相似性得分+阈值判断的相似性搜索)、mmr(最大边际相关性搜索)。<br>2. search_kwargs：其他键值对搜索参数，类型为字典，例如：k、filter、score_threshold、fetch_k、lambda_mult 等，当搜索类型配置为 similarity_score_threshold 后，必须添加 score_threshold 配置选项，否则会报错，参数的具体信息要看 search_type 类型对应的函数配合使用。</p>
<p>并且由于检索器是 Runnable 可运行组件，所以可以使用 Runnable 组件的所有功能（组件替换、参数配置、重试、回退、并行等）。</p>
<p>例如将向量数据库转换成 携带得分+阈值判断的相似性搜索，并设置得分阈值为0.5，数据条数为10条，代码示例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> dotenv  </span><br><span class="line"><span class="keyword">import</span> weaviate  </span><br><span class="line"><span class="keyword">from</span> langchain_community.document_loaders <span class="keyword">import</span> UnstructuredMarkdownLoader  </span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> OpenAIEmbeddings  </span><br><span class="line"><span class="keyword">from</span> langchain_text_splitters <span class="keyword">import</span> RecursiveCharacterTextSplitter  </span><br><span class="line"><span class="keyword">from</span> langchain_weaviate <span class="keyword">import</span> WeaviateVectorStore  </span><br><span class="line"><span class="keyword">from</span> weaviate.auth <span class="keyword">import</span> AuthApiKey  </span><br><span class="line">  </span><br><span class="line">dotenv.load_dotenv()  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 1.构建加载器与分割器  </span></span><br><span class="line">loader = UnstructuredMarkdownLoader(<span class="string">&quot;./项目API文档.md&quot;</span>)  </span><br><span class="line">text_splitter = RecursiveCharacterTextSplitter(  </span><br><span class="line">    separators=[<span class="string">&quot;\n\n&quot;</span>, <span class="string">&quot;\n&quot;</span>, <span class="string">&quot;。|！|？&quot;</span>, <span class="string">&quot;\.\s|\!\s|\?\s&quot;</span>, <span class="string">&quot;；|;\s&quot;</span>, <span class="string">&quot;，|,\s&quot;</span>, <span class="string">&quot; &quot;</span>, <span class="string">&quot;&quot;</span>, ],  </span><br><span class="line">    is_separator_regex=<span class="literal">True</span>,  </span><br><span class="line">    chunk_size=<span class="number">500</span>,  </span><br><span class="line">    chunk_overlap=<span class="number">50</span>,  </span><br><span class="line">    add_start_index=<span class="literal">True</span>,  </span><br><span class="line">)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 2.加载文档并分割  </span></span><br><span class="line">documents = loader.load()  </span><br><span class="line">chunks = text_splitter.split_documents(documents)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 3.将数据存储到向量数据库  </span></span><br><span class="line">db = WeaviateVectorStore(  </span><br><span class="line">    client=weaviate.connect_to_wcs(  </span><br><span class="line">        cluster_url=<span class="string">&quot;https://eftofnujtxqcsa0sn272jw.c0.us-west3.gcp.weaviate.cloud&quot;</span>,  </span><br><span class="line">        auth_credentials=AuthApiKey(<span class="string">&quot;21pzYy0orl2dxH9xCoZG1O2b0euDeKJNEbB0&quot;</span>),  </span><br><span class="line">    ),  </span><br><span class="line">    index_name=<span class="string">&quot;DatasetDemo&quot;</span>,  </span><br><span class="line">    text_key=<span class="string">&quot;text&quot;</span>,  </span><br><span class="line">    embedding=OpenAIEmbeddings(model=<span class="string">&quot;text-embedding-3-small&quot;</span>),  </span><br><span class="line">)  </span><br><span class="line">db.add_documents(chunks)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 4.转换检索器（带阈值的相似性搜索，数据为10条，得分阈值为0.5）  </span></span><br><span class="line">retriever = db.as_retriever(  </span><br><span class="line">    search_type=<span class="string">&quot;similarity_score_threshold&quot;</span>,  </span><br><span class="line">    search_kwargs=&#123;<span class="string">&quot;k&quot;</span>: <span class="number">10</span>, <span class="string">&quot;score_threshold&quot;</span>: <span class="number">0.5</span>&#125;,  </span><br><span class="line">)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 5.检索结果  </span></span><br><span class="line">documents = retriever.invoke(<span class="string">&quot;关于配置接口的信息有哪些&quot;</span>)  </span><br><span class="line">  </span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">list</span>(document.page_content[:<span class="number">50</span>] <span class="keyword">for</span> document <span class="keyword">in</span> documents))  </span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(documents))</span><br></pre></td></tr></table></figure>

<h2 id="5-3-MMR-最大边际相关性"><a href="#5-3-MMR-最大边际相关性" class="headerlink" title="5.3 MMR 最大边际相关性"></a>5.3 MMR 最大边际相关性</h2><p>最大边际相关性（MMR，max_marginal_relevance_search）的基本思想是同时考量查询与文档的 相关度，以及文档之间的 相似度。相关度 确保返回结果对查询高度相关，相似度 则鼓励不同语义的文档被包含进结果集。具体来说，它计算每个候选文档与查询的 相关度，并减去与已经入选结果集的文档的最大 相似度，这样更不相似的文档会有更高分。</p>
<p>而在 LangChain 中MMR 的实现过程和 FAISS 的 带过滤器的相似性搜索 非常接近，同样也是先执行相似性搜索，并得到一个远大于 k 的结果列表，例如 fetch_k 条数据，然后对搜索得到的 fetch_k 条数据计算文档之间的相似度，通过加权得分找到最终的 k 条数据。</p>
<p>简单来说，MMR 就是在一大堆最相似的文档中查找最不相似的，从而保证 结果多样化。</p>
<p>执行一个 MMR 最大边际相似性搜索需要的参数为：搜索语句、k条搜索结果数据、fetch_k条中间数据、多样性系数(0代表最大多样性，1代表最小多样性)，在 LangChain 中也是基于这个思想进行封装，max_marginal_relevance_search() 函数的参数如下：</p>
<p>1. query：搜索语句，类型为字符串，必填参数。<br>2. k：搜索的结果条数，类型为整型，默认为 4。<br>3. fetch_k：要传递给 MMR 算法的的文档数，默认为 20。<br>4. lambda_mult：函数系数，数值范围从0-1，<code>底层计算得分 = lambda_mult *相关性 - (1 - lambda_mult)*相似性</code>，所以 0 代表最大多样性、1 代表最小多样性。<br>5. kwargs：其他传递给搜索方法的参数，例如 filter 等，这个参数使用和相似性搜索类似，具体取决于使用的向量数据库。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> dotenv  </span><br><span class="line"><span class="keyword">import</span> weaviate  </span><br><span class="line"><span class="keyword">from</span> langchain_community.document_loaders <span class="keyword">import</span> UnstructuredMarkdownLoader  </span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> OpenAIEmbeddings  </span><br><span class="line"><span class="keyword">from</span> langchain_text_splitters <span class="keyword">import</span> RecursiveCharacterTextSplitter  </span><br><span class="line"><span class="keyword">from</span> langchain_weaviate <span class="keyword">import</span> WeaviateVectorStore  </span><br><span class="line"><span class="keyword">from</span> weaviate.auth <span class="keyword">import</span> AuthApiKey  </span><br><span class="line">  </span><br><span class="line">dotenv.load_dotenv()  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 1.构建加载器与分割器  </span></span><br><span class="line">loader = UnstructuredMarkdownLoader(<span class="string">&quot;./项目API文档.md&quot;</span>)  </span><br><span class="line">text_splitter = RecursiveCharacterTextSplitter(  </span><br><span class="line">    separators=[<span class="string">&quot;\n\n&quot;</span>, <span class="string">&quot;\n&quot;</span>, <span class="string">&quot;。|！|？&quot;</span>, <span class="string">&quot;\.\s|\!\s|\?\s&quot;</span>, <span class="string">&quot;；|;\s&quot;</span>, <span class="string">&quot;，|,\s&quot;</span>, <span class="string">&quot; &quot;</span>, <span class="string">&quot;&quot;</span>, ],  </span><br><span class="line">    is_separator_regex=<span class="literal">True</span>,  </span><br><span class="line">    chunk_size=<span class="number">500</span>,  </span><br><span class="line">    chunk_overlap=<span class="number">50</span>,  </span><br><span class="line">    add_start_index=<span class="literal">True</span>,  </span><br><span class="line">)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 2.加载文档并分割  </span></span><br><span class="line">documents = loader.load()  </span><br><span class="line">chunks = text_splitter.split_documents(documents)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 3.将数据存储到向量数据库  </span></span><br><span class="line">db = WeaviateVectorStore(  </span><br><span class="line">    client=weaviate.connect_to_wcs(  </span><br><span class="line">        cluster_url=<span class="string">&quot;https://eftofnujtxqcsa0sn272jw.c0.us-west3.gcp.weaviate.cloud&quot;</span>,  </span><br><span class="line">        auth_credentials=AuthApiKey(<span class="string">&quot;21pzYy0orl2dxH9xCoZG1O2b0euDeKJNEbB0&quot;</span>),  </span><br><span class="line">    ),  </span><br><span class="line">    index_name=<span class="string">&quot;DatasetDemo&quot;</span>,  </span><br><span class="line">    text_key=<span class="string">&quot;text&quot;</span>,  </span><br><span class="line">    embedding=OpenAIEmbeddings(model=<span class="string">&quot;text-embedding-3-small&quot;</span>),  </span><br><span class="line">)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 4.执行最大边际相关性搜索（可以去除重复数据）  </span></span><br><span class="line"><span class="comment"># search_documents = db.similarity_search(&quot;关于应用配置的接口有哪些？&quot;)  # 会查到重复数据  </span></span><br><span class="line">search_documents = db.max_marginal_relevance_search(<span class="string">&quot;关于应用配置的接口有哪些？&quot;</span>)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 5.打印搜索的结果  </span></span><br><span class="line"><span class="comment"># print(list(document.page_content[:100] for document in search_documents))  </span></span><br><span class="line"><span class="keyword">for</span> document <span class="keyword">in</span> search_documents:  </span><br><span class="line">    <span class="built_in">print</span>(document.page_content[:<span class="number">100</span>])  </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;===========&quot;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="5-4-检索器组件"><a href="#5-4-检索器组件" class="headerlink" title="5.4 检索器组件"></a>5.4 检索器组件</h2><p>在 LangChain 中，传递一段 query 并返回与这段文本相关联文档的组件被称为 检索器，并且 LangChain 为所有检索器设计了一个基类——BaseRetriever，该类继承了 RunnableSerializable，所以该类是一个 Runnable 可运行组件，支持使用 Runnable 组件的所有配置，在 BaseRetriever 下封装了一些通用的方法，类图如下<br><img src="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20250430104950.png" alt="image.png"></p>
<p>其中 get_relevance_documents() 方法将在 0.3.0 版本开始被遗弃（老版本非 Runnable 写法），使用检索器的技巧也非常简单，按照特定的规则创建好检索器后（通过 as_retriever() 或者 构造函数），调用 invoke() 方法即可。</p>
<p>并且针对所有 向量数据库，LangChain 都配置了 as_retriever() 方法，便于快捷将向量数据库转换成检索器，不同的检索器传递的参数会有所差异，需要查看源码或者查看文档搭配使用，例如下方是一个向量数据库检索器的使用示例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> dotenv  </span><br><span class="line"><span class="keyword">import</span> weaviate  </span><br><span class="line"><span class="keyword">from</span> langchain_core.runnables <span class="keyword">import</span> ConfigurableField  </span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> OpenAIEmbeddings  </span><br><span class="line"><span class="keyword">from</span> langchain_weaviate <span class="keyword">import</span> WeaviateVectorStore  </span><br><span class="line"><span class="keyword">from</span> weaviate.auth <span class="keyword">import</span> AuthApiKey  </span><br><span class="line">  </span><br><span class="line">dotenv.load_dotenv()  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 1.构建向量数据库  </span></span><br><span class="line">db = WeaviateVectorStore(  </span><br><span class="line">    client=weaviate.connect_to_wcs(  </span><br><span class="line">        cluster_url=<span class="string">&quot;https://eftofnujtxqcsa0sn272jw.c0.us-west3.gcp.weaviate.cloud&quot;</span>,  </span><br><span class="line">        auth_credentials=AuthApiKey(<span class="string">&quot;21pzYy0orl2dxH9xCoZG1O2b0euDeKJNEbB0&quot;</span>),  </span><br><span class="line">    ),  </span><br><span class="line">    index_name=<span class="string">&quot;DatasetDemo&quot;</span>,  </span><br><span class="line">    text_key=<span class="string">&quot;text&quot;</span>,  </span><br><span class="line">    embedding=OpenAIEmbeddings(model=<span class="string">&quot;text-embedding-3-small&quot;</span>),  </span><br><span class="line">)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 2.转换检索器  </span></span><br><span class="line">retriever = db.as_retriever(  </span><br><span class="line">    search_type=<span class="string">&quot;similarity_score_threshold&quot;</span>,  </span><br><span class="line">    search_kwargs=&#123;<span class="string">&quot;k&quot;</span>: <span class="number">10</span>, <span class="string">&quot;score_threshold&quot;</span>: <span class="number">0.5</span>&#125;,  </span><br><span class="line">).configurable_fields(  </span><br><span class="line">    search_type=ConfigurableField(<span class="built_in">id</span>=<span class="string">&quot;db_search_type&quot;</span>),  </span><br><span class="line">    search_kwargs=ConfigurableField(<span class="built_in">id</span>=<span class="string">&quot;db_search_kwargs&quot;</span>),  </span><br><span class="line">)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 3.修改运行时配置执行MMR搜索，并返回4条数据  </span></span><br><span class="line">mmr_documents = retriever.with_config(  </span><br><span class="line">    configurable=&#123;  </span><br><span class="line">        <span class="string">&quot;db_search_type&quot;</span>: <span class="string">&quot;mmr&quot;</span>,  </span><br><span class="line">        <span class="string">&quot;db_search_kwargs&quot;</span>: &#123;  </span><br><span class="line">            <span class="string">&quot;k&quot;</span>: <span class="number">4</span>,  </span><br><span class="line">        &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">).invoke(<span class="string">&quot;关于应用配置的接口有哪些？&quot;</span>)  </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;相似性搜索: &quot;</span>, mmr_documents)  </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;内容长度:&quot;</span>, <span class="built_in">len</span>(mmr_documents))  </span><br><span class="line">  </span><br><span class="line"><span class="built_in">print</span>(mmr_documents[<span class="number">0</span>].page_content[:<span class="number">20</span>])  </span><br><span class="line"><span class="built_in">print</span>(mmr_documents[<span class="number">1</span>].page_content[:<span class="number">20</span>])</span><br></pre></td></tr></table></figure>

<h2 id="5-5-自定义检索器"><a href="#5-5-自定义检索器" class="headerlink" title="5.5 自定义检索器"></a>5.5 自定义检索器</h2><p>在 LangChain 中实现自定义检索器的技巧其实非常简单，只需要继承 BaseRetriever 类，然后实现 <code>_get_relevant_documents()</code> 方法即可，从 query 到 <code>list[document]</code> 的逻辑全部都在这个函数内部实现，异步的方法也可以不需要实现，底层会委托同步方法来执行。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">List</span>  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">from</span> langchain_core.callbacks <span class="keyword">import</span> CallbackManagerForRetrieverRun  </span><br><span class="line"><span class="keyword">from</span> langchain_core.documents <span class="keyword">import</span> Document  </span><br><span class="line"><span class="keyword">from</span> langchain_core.retrievers <span class="keyword">import</span> BaseRetriever  </span><br><span class="line">    </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CustomRetriever</span>(<span class="title class_ inherited__">BaseRetriever</span>):  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;自定义检索器&quot;&quot;&quot;</span>  </span><br><span class="line">    documents: <span class="built_in">list</span>[Document]  </span><br><span class="line">    k: <span class="built_in">int</span>  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_get_relevant_documents</span>(<span class="params">self, query: <span class="built_in">str</span>, *, run_manager: CallbackManagerForRetrieverRun</span>) -&gt; <span class="type">List</span>[Document]:  </span><br><span class="line">        <span class="string">&quot;&quot;&quot;根据传入的query，获取相关联的文档列表&quot;&quot;&quot;</span>  </span><br><span class="line">        matching_documents = []  </span><br><span class="line">        <span class="keyword">for</span> document <span class="keyword">in</span> self.documents:  </span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(matching_documents) &gt; self.k:  </span><br><span class="line">                <span class="keyword">return</span> matching_documents  </span><br><span class="line">            <span class="keyword">if</span> query.lower() <span class="keyword">in</span> document.page_content.lower():  </span><br><span class="line">                matching_documents.append(document)  </span><br><span class="line">        <span class="keyword">return</span> matching_documents  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 1.定义预设文档  </span></span><br><span class="line">documents = [  </span><br><span class="line">    Document(page_content=<span class="string">&quot;笨笨是一只很喜欢睡觉的猫咪&quot;</span>, metadata=&#123;<span class="string">&quot;page&quot;</span>: <span class="number">1</span>&#125;),  </span><br><span class="line">    Document(page_content=<span class="string">&quot;我喜欢在夜晚听音乐，这让我感到放松。&quot;</span>, metadata=&#123;<span class="string">&quot;page&quot;</span>: <span class="number">2</span>&#125;),  </span><br><span class="line">    Document(page_content=<span class="string">&quot;猫咪在窗台上打盹，看起来非常可爱。&quot;</span>, metadata=&#123;<span class="string">&quot;page&quot;</span>: <span class="number">3</span>&#125;),  </span><br><span class="line">    Document(page_content=<span class="string">&quot;学习新技能是每个人都应该追求的目标。&quot;</span>, metadata=&#123;<span class="string">&quot;page&quot;</span>: <span class="number">4</span>&#125;),  </span><br><span class="line">    Document(page_content=<span class="string">&quot;我最喜欢的食物是意大利面，尤其是番茄酱的那种。&quot;</span>, metadata=&#123;<span class="string">&quot;page&quot;</span>: <span class="number">5</span>&#125;),  </span><br><span class="line">    Document(page_content=<span class="string">&quot;昨晚我做了一个奇怪的梦，梦见自己在太空飞行。&quot;</span>, metadata=&#123;<span class="string">&quot;page&quot;</span>: <span class="number">6</span>&#125;),  </span><br><span class="line">    Document(page_content=<span class="string">&quot;我的手机突然关机了，让我有些焦虑。&quot;</span>, metadata=&#123;<span class="string">&quot;page&quot;</span>: <span class="number">7</span>&#125;),  </span><br><span class="line">    Document(page_content=<span class="string">&quot;阅读是我每天都会做的事情，我觉得很充实。&quot;</span>, metadata=&#123;<span class="string">&quot;page&quot;</span>: <span class="number">8</span>&#125;),  </span><br><span class="line">    Document(page_content=<span class="string">&quot;他们一起计划了一次周末的野餐，希望天气能好。&quot;</span>, metadata=&#123;<span class="string">&quot;page&quot;</span>: <span class="number">9</span>&#125;),  </span><br><span class="line">    Document(page_content=<span class="string">&quot;我的狗喜欢追逐球，看起来非常开心。&quot;</span>, metadata=&#123;<span class="string">&quot;page&quot;</span>: <span class="number">10</span>&#125;),  </span><br><span class="line">]  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 2.创建检索器  </span></span><br><span class="line">retriever = CustomRetriever(documents=documents, k=<span class="number">3</span>)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 3.调用检索器获取搜索结果并打印  </span></span><br><span class="line">retriever_documents = retriever.invoke(<span class="string">&quot;猫&quot;</span>)  </span><br><span class="line"><span class="built_in">print</span>(retriever_documents)  </span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(retriever_documents))</span><br></pre></td></tr></table></figure>

<h1 id="6-RAG-优化策略"><a href="#6-RAG-优化策略" class="headerlink" title="6 RAG 优化策略"></a>6 RAG 优化策略</h1><h3 id="6-1-RAG-开发6个阶段优化策略"><a href="#6-1-RAG-开发6个阶段优化策略" class="headerlink" title="6.1 RAG 开发6个阶段优化策略"></a>6.1 RAG 开发6个阶段优化策略</h3><p>在 RAG 应用开发中，无论架构多复杂，接入了多少组件，使用了多少优化策略与特性，所有优化的最终目标都是 提升LLM生成内容的准确性，而对于 Transformer架构类型 的大模型来说，要实现这个目标，一般只需要 3 个步骤：</p>
<p>1. 传递更准确的内容：传递和提问准确性更高的内容，会让 LLM 能识别到关联的内容， 生成的内容准确性更高。<br>2. 让重要的内容更靠前：GPT 模型的注意力机制会让传递 Prompt 中更靠前的内容权重更高，越靠后权重越低。<br>3. 尽可能不传递不相关内容：缩短每个块的大小，尽可能让每个块只包含关联的内容，缩小不相关内容的比例。</p>
<p>看起来很简单，但是目前针对这 3 个步骤 N 多研究员提出了不少方案，比较遗憾的是，目前也没有一种统一的方案，不同的场合仍然需要考虑不同的方案结合才能实现相对好一点的效果，并不是所有场合都适合配置很复杂的优化策略。</p>
<p>在 RAG 应用开发中，使用的优化策略越多，单次响应成本越高，性能越差，需要合理使用。映射到 RAG 中，其实就是 切割合适的文档块、更准确的搜索语句、正确地排序文档、剔除重复无关的检索内容，所以在 RAG应用开发 中，想进行优化，可以针对 query(提问查询)、TextSplitter(文本分割器)、VectorStore(向量数据库)、Retriever(检索器)、Prompt(基础prompt编写) 这几个组件。</p>
<p><img src="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20250430140325.png" alt="image.png"><br>在完整的 LLM 应用流程中拆解 RAG 开发阶段并进行优化看起来相对繁琐，可以考虑单独将 RAG 开发阶段的流程拎出来，并针对性对每个阶段进行优化与调整，按照不同的功能模块，共可以划分成 6 个阶段：查询转换、路由、查询构建、索引、检索 和 生成。</p>
<p>在 RAG 开发的 6 个阶段中，不同的阶段拥有不同的优化策略，需要针对不同的应用进行特定性的优化，目前市面上常见的优化方案有：问题转换、多路召回、混合检索、搜索重排、动态路由、图查询、问题重建、自检索 等数十种优化策略，每种策略所在的阶段并不一致，效果也有差异，并且相互影响。<br>并且 RAG 优化和 LangChain 并没有关系，无论使用任何框架、任何编程语言，进行 RAG 开发时，掌握优化的思路才是最重要的！<br>将对应的优化策略整理到 RAG 运行流程中，优化策略与开发阶段对应图如下：<br><img src="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20250430140425.png" alt="image.png"></p>
<h2 id="6-2-多查询重写策略"><a href="#6-2-多查询重写策略" class="headerlink" title="6.2 多查询重写策略"></a>6.2 多查询重写策略</h2><h3 id="6-2-1-Muliti-Query-多查询策略"><a href="#6-2-1-Muliti-Query-多查询策略" class="headerlink" title="6.2.1 Muliti-Query 多查询策略"></a>6.2.1 Muliti-Query 多查询策略</h3><p>多查询策略 也被称为 子查询，是一种用于生成子问题的技术，其核心思想是在问答过程中，为了更好地理解和回答主问题，系统会自动生成并提出与主问题相关的子问题，这些子问题通常具有更具体的细节，可以帮助大语言模型更深入地理解主问题，从而进行更加准确的检索并提供正确的答案。<br>多查询策略 会从多个角度重写用户问题，为每个重写的问题执行检索，然后将检索到的文档列表进行合并后去重，返回唯一文档，该策略的运行流程非常简单，如下<br><img src="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2025/20250506160213.png" alt="image.png"><br>在 LangChain 中，针对 多查询策略 封装了一个检索器 MultiQueryRetriever，该检索器可以通过构造函数亦或者 from_llm 类方法进行实例化，参数如下：<br>1. retriever：基础检索器，必填参数。<br>2. llm：大语言模型，用于将原始问题转换成多个问题，必填参数。<br>3. prompt：转换原始问题为多个问题的提示模板，非必填，已有默认值。<br>4. parser_key：解析键，该参数在未来将被抛弃，非必填，已弃用，新版本中保留参数，但没有任何使用的地方。<br>5. inclued_original：是否保留原始问题，默认为 False，如果设置为 True，则除了检索新问题，还会检索原始问题。</p>
<p><strong>代码示例：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> dotenv  </span><br><span class="line"><span class="keyword">import</span> weaviate  </span><br><span class="line"><span class="keyword">from</span> langchain.retrievers <span class="keyword">import</span> MultiQueryRetriever  </span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI  </span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> OpenAIEmbeddings  </span><br><span class="line"><span class="keyword">from</span> langchain_weaviate <span class="keyword">import</span> WeaviateVectorStore  </span><br><span class="line"><span class="keyword">from</span> weaviate.auth <span class="keyword">import</span> AuthApiKey  </span><br><span class="line">  </span><br><span class="line">dotenv.load_dotenv()  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 1.构建向量数据库与检索器  </span></span><br><span class="line">db = WeaviateVectorStore(  </span><br><span class="line">    client=weaviate.connect_to_wcs(  </span><br><span class="line">        cluster_url=<span class="string">&quot;https://eftofnujtxqcsa0sn272jw.c0.us-west3.gcp.weaviate.cloud&quot;</span>,  </span><br><span class="line">        auth_credentials=AuthApiKey(<span class="string">&quot;21pzYy0orl2dxH9xCoZG1O2b0euDeKJNEbB0&quot;</span>),  </span><br><span class="line">    ),  </span><br><span class="line">    index_name=<span class="string">&quot;DatasetDemo&quot;</span>,  </span><br><span class="line">    text_key=<span class="string">&quot;text&quot;</span>,  </span><br><span class="line">    embedding=OpenAIEmbeddings(model=<span class="string">&quot;text-embedding-3-small&quot;</span>),  </span><br><span class="line">)  </span><br><span class="line">retriever = db.as_retriever(search_type=<span class="string">&quot;mmr&quot;</span>)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 2.创建多查询检索器  </span></span><br><span class="line">multi_query_retriever = MultiQueryRetriever.from_llm(  </span><br><span class="line">    retriever=retriever,  </span><br><span class="line">    llm=ChatOpenAI(model=<span class="string">&quot;gpt-3.5-turbo-16k&quot;</span>, temperature=<span class="number">0</span>),  </span><br><span class="line">    include_original=<span class="literal">True</span>,  </span><br><span class="line">)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 3.执行检索  </span></span><br><span class="line">docs = multi_query_retriever.invoke(<span class="string">&quot;关于LLMOps应用配置的文档有哪些&quot;</span>)  </span><br><span class="line"><span class="built_in">print</span>(docs)  </span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(docs))</span><br></pre></td></tr></table></figure>
<p><strong>输出：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[Document(metadata=&#123;<span class="string">&#x27;source&#x27;</span>: <span class="string">&#x27;./项目API文档.md&#x27;</span>, <span class="string">&#x27;start_index&#x27;</span>: 0.0&#125;, page_content=<span class="string">&#x27;LLMOps 项目 API 文档\n\n应用 API 接口统一以 JSON 格式返回，并且包含 3 个字段：code、data 和 message，分别代表业务状态码、业务数据和接口附加信息。\n\n业务状态码共有 6 种，其中只有 success(成功) 代表业务操作成功，其他 5 种状态均代表失败，并且失败时会附加相关的信息：fail(通用失败)、not_found(未找到)、unauthorized(未授权)、forbidden(无权限)和validate_error(数据验证失败)。\n\n接口示例：\n\njson &#123; &quot;code&quot;: &quot;success&quot;, &quot;data&quot;: &#123; &quot;redirect_url&quot;: &quot;https://github.com/login/oauth/authorize?client_id=f69102c6b97d90d69768&amp;redirect_uri=http%3A%2F%2Flocalhost%3A5001%2Foauth%2Fauthorize%2Fgithub&amp;scope=user%3Aemail&quot; &#125;, &quot;message&quot;: &quot;&quot; &#125;&#x27;</span>), Document(metadata=&#123;<span class="string">&#x27;source&#x27;</span>: <span class="string">&#x27;./项目API文档.md&#x27;</span>, <span class="string">&#x27;start_index&#x27;</span>: 3042.0&#125;, page_content=<span class="string">&#x27;1.2 [todo]更新应用草稿配置信息\n\n接口说明：更新应用的草稿配置信息，涵盖：模型配置、长记忆模式等，该接口会查找该应用原始的草稿配置并进行更新，如果没有原始草稿配置，则创建一个新配置作为草稿配置。\n\n接口信息：授权+POST:/apps/:app_id/config\n\n接口参数：\n\n请求参数：\n\napp_id -&gt; str：需要修改配置的应用 id。\n\nmodel_config -&gt; json：模型配置信息。\n\ndialog_round -&gt; int：携带上下文轮数，类型为非负整型。\n\nmemory_mode -&gt; string：记忆类型，涵盖长记忆 long_term_memory 和 none 代表无。\n\n请求示例：\n\njson &#123; &quot;model_config&quot;: &#123; &quot;dialog_round&quot;: 10 &#125;, &quot;memory_mode&quot;: &quot;long_term_memory&quot; &#125;\n\n响应示例：\n\njson &#123; &quot;code&quot;: &quot;success&quot;, &quot;data&quot;: &#123;&#125;, &quot;message&quot;: &quot;更新AI应用配置成功&quot; &#125;\n\n1.3 [todo]获取应用调试长记忆&#x27;</span>), Document(metadata=&#123;<span class="string">&#x27;source&#x27;</span>: <span class="string">&#x27;./项目API文档.md&#x27;</span>, <span class="string">&#x27;start_index&#x27;</span>: 5818.0&#125;, page_content=<span class="string">&#x27;json &#123; &quot;code&quot;: &quot;success&quot;, &quot;data&quot;: &#123; &quot;list&quot;: [ &#123; &quot;id&quot;: &quot;1550b71a-1444-47ed-a59d-c2f080fbae94&quot;, &quot;conversation_id&quot;: &quot;2d7d3e3f-95c9-4d9d-ba9c-9daaf09cc8a8&quot;, &quot;query&quot;: &quot;能详细讲解下LLM是什么吗？&quot;, &quot;answer&quot;: &quot;LLM 即 Large Language Model，大语言模型，是一种基于深度学习的自然语言处理模型，具有很高的语言理解和生成能力，能够处理各式各样的自然语言任务，例如文本生成、问答、翻译、摘要等。它通过在大量的文本数据上进行训练，学习到语言的模式、结构和语义知识&#x27;</span>), Document(metadata=&#123;<span class="string">&#x27;source&#x27;</span>: <span class="string">&#x27;./项目API文档.md&#x27;</span>, <span class="string">&#x27;start_index&#x27;</span>: 675.0&#125;, page_content=<span class="string">&#x27;json &#123; &quot;code&quot;: &quot;success&quot;, &quot;data&quot;: &#123; &quot;list&quot;: [ &#123; &quot;app_count&quot;: 0, &quot;created_at&quot;: 1713105994, &quot;description&quot;: &quot;这是专门用来存储慕课LLMOps课程信息的知识库&quot;, &quot;document_count&quot;: 13, &quot;icon&quot;: &quot;https://imooc-llmops-1257184990.cos.ap-guangzhou.myqcloud.com/2024/04/07/96b5e270-c54a-4424-aece-ff8a2b7e4331.png&quot;, &quot;id&quot;: &quot;c0759ca8-2d35-4480-83a8-1f41f29d1401&quot;, &quot;name&quot;: &quot;慕课LLMOps课程知识库&quot;, &quot;updated_at&quot;: 1713106758, &quot;word_count&quot;: 8850 &#125; ], &quot;paginator&quot;: &#123; &quot;current_page&quot;: 1, &quot;page_size&quot;: 20, &quot;total_page&quot;: 1, &quot;total_record&quot;: 2 &#125; &#125;&#x27;</span>), Document(metadata=&#123;<span class="string">&#x27;source&#x27;</span>: <span class="string">&#x27;./项目API文档.md&#x27;</span>, <span class="string">&#x27;start_index&#x27;</span>: 2324.0&#125;, page_content=<span class="string">&#x27;json &#123; &quot;code&quot;: &quot;success&quot;, &quot;data&quot;: &#123; &quot;id&quot;: &quot;5e7834dc-bbca-4ee5-9591-8f297f5acded&quot;, &quot;name&quot;: &quot;慕课LLMOps聊天机器人&quot;, &quot;icon&quot;: &quot;https://imooc-llmops-1257184990.cos.ap-guangzhou.myqcloud.com/2024/04/23/e4422149-4cf7-41b3-ad55-ca8d2caa8f13.png&quot;, &quot;description&quot;: &quot;这是一个慕课LLMOps的Agent应用&quot;, &quot;published_app_config_id&quot;: null, &quot;drafted_app_config_id&quot;: null, &quot;debug_conversation_id&quot;: &quot;1550b71a-1444-47ed-a59d-c2f080fbae94&quot;, &quot;published_app_config&quot;: null, &quot;drafted_app_config&quot;: &#123; &quot;id&quot;: &quot;755dc464-67cd-42ef-9c56-b7528b44e7c8&quot;&#x27;</span>), Document(metadata=&#123;<span class="string">&#x27;source&#x27;</span>: <span class="string">&#x27;./项目API文档.md&#x27;</span>, <span class="string">&#x27;start_index&#x27;</span>: 2042.0&#125;, page_content=<span class="string">&#x27;dialog_round -&gt; int：携带上下文轮数，类型为非负整型。\n\nmemory_mode -&gt; string：记忆类型，涵盖长记忆 long_term_memory 和 none 代表无。\n\nstatus -&gt; string：应用配置的状态，drafted 代表草稿、published 代表已发布配置。\n\nupdated_at -&gt; int：应用配置的更新时间。\n\ncreated_at -&gt; int：应用配置的创建时间。\n\nupdated_at -&gt; int：应用的更新时间。\n\ncreated_at -&gt; int：应用的创建时间。\n\n响应示例：&#x27;</span>)]</span><br><span class="line"></span><br><span class="line">6</span><br></pre></td></tr></table></figure>
<h3 id="6-2-2-核心执行逻辑"><a href="#6-2-2-核心执行逻辑" class="headerlink" title="6.2.2 核心执行逻辑"></a>6.2.2 核心执行逻辑</h3><p>从 LangSmith 平台记录的运行流程，可以很清晰看到这个检索器会先调用大语言模型生成 3 条与原始问题相关的 子问题，然后再逐个使用传递的检索器检索 3 个子问题，得到对应的文档列表，最后再将所有文档列表进行合并去重，得到最终的文档。<br><img src="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2025/20250506160756.png" alt="image.png"><br>在 MultiQueryRetriever 这个检索器中，预设了一段 prompt，用于将原始问题生成 3 个关联子问题，并使用 \n 分割得到具体问题。<br>这段 prompt 如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># langchain/retrievers/multi_query.py</span></span><br><span class="line"></span><br><span class="line">DEFAULT_QUERY_PROMPT = PromptTemplate(</span><br><span class="line">    input_variables=[<span class="string">&quot;question&quot;</span>],</span><br><span class="line">    template=<span class="string">&quot;&quot;&quot;You are an AI language model assistant. Your task is to generate 3 different versions of the given user question to retrieve relevant documents from a vector  database. By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of distance-based similarity search. Provide these alternative questions separated by newlines. Original question: &#123;question&#125;&quot;&quot;&quot;</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>在 LangChain 中，所有预设的 prompt 绝大部分场景都是使用 OpenAI 的大语言模型进行调试的，所以效果会比较好，对于其他的模型，例如国内的模型，一般来说还需要将对应的提示换成 中文语言，所以可以考虑使用 ChatGPT 翻译原有的 prompt，更新后</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">multi_query_retriever = MultiQueryRetriever.from_llm(</span><br><span class="line">    retriever=retriever,</span><br><span class="line">    llm=ChatOpenAI(model=<span class="string">&quot;gpt-3.5-turbo-16k&quot;</span>, temperature=<span class="number">0</span>),</span><br><span class="line">    prompt=ChatPromptTemplate.from_template(</span><br><span class="line">        <span class="string">&quot;你是一个AI语言模型助手。你的任务是生成给定用户问题的3个不同版本，以从向量数据库中检索相关文档。&quot;</span></span><br><span class="line">        <span class="string">&quot;通过提供用户问题的多个视角，你的目标是帮助用户克服基于距离的相似性搜索的一些限制。&quot;</span></span><br><span class="line">        <span class="string">&quot;请用换行符分隔这些替代问题。&quot;</span></span><br><span class="line">        <span class="string">&quot;原始问题：&#123;question&#125;&quot;</span></span><br><span class="line">    )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>基于中文 prompt 生成的问题列表如下</p>
<ol>
<li>LLMOps应用配置的文档有哪些资源可供参考？</li>
<li>我可以在哪里找到关于LLMOps应用配置的文档？</li>
<li>有哪些文档可以帮助我了解LLMOps应用配置的相关信息？</li>
</ol>
<p>对于该检索器，不同的模型生成的 query 格式可能并不一样，某些模型生成的多条 query 可能并不是按照 \n 进行分割，这个时候查询的效果可能不如原始问题，所以在使用该检索器时，一定要多次测试 prompt 的效果，或者设置 inclued_original 为 True，确保生成内容不符合规范时，仍然可以使用原始问题进行检索。<br>另外，在 MultiQueryRetriever 的底层进行合并去重时，并没有任何特别的，仅仅只做了循环遍历并记录唯一的文档而已，核心代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># langchain/retrievers/multi_query.py</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_unique_documents</span>(<span class="params">documents: <span class="type">Sequence</span>[Document]</span>) -&gt; <span class="type">List</span>[Document]:</span><br><span class="line">	<span class="keyword">return</span> [doc <span class="keyword">for</span> i, doc <span class="keyword">in</span> <span class="built_in">enumerate</span>(documents) <span class="keyword">if</span> doc <span class="keyword">not</span> <span class="keyword">in</span> documents[:i]]</span><br></pre></td></tr></table></figure>
<p>多查询策略是最基础+最简单的 RAG 优化，不涉及到复杂的逻辑与算法，会稍微影响单次对话的耗时。<br>并且由于需要转换 query 一般较小，以及生成 sub-queries 时对 LLM 的能力要求并不高，在实际的 LLM 应用开发中，通常使用参数较小的本地模型+针对性优化的 prompt 即可完成任务。<br>而且为了减少模型的幻觉以及胡说八道，一般都将 temperature 设置为 0，确保生成的文本更加有确定性。</p>
<h2 id="6-3-多查询结果融合"><a href="#6-3-多查询结果融合" class="headerlink" title="6.3 多查询结果融合"></a>6.3 多查询结果融合</h2><h3 id="6-3-1-多查询结果融合策略及RRF"><a href="#6-3-1-多查询结果融合策略及RRF" class="headerlink" title="6.3.1 多查询结果融合策略及RRF"></a>6.3.1 多查询结果融合策略及RRF</h3><p>在 多查询重写策略 中，虽然可以生成多条查询并执行多次检索器检索，但是在合并数据的时候，并没有考虑最终结果的文档数，极端情况下，原始的 k 设置为 4，可能会返回 16 个文档（3 条子查询的文档，1 条原始问题查询的文档），除此之外，多查询重写策略 并不会考虑对应文档的权重，只按默认顺序进行合并。<br>于是就诞生了 RAG融合 的概念，它的主要思想是在 Multi-Query 的基础上，对其检索结果进行重新排序（即 reranking）后输出 Top K 个结果，最后再将这 Top K 个结果喂给 LLM 并生成最终答案，运行流程如下：<br><img src="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2025/20250506161726.png" alt="image.png"><br>在 RAG融合 中，对文档列表进行排序&amp;去重合并的算法为 RRF(Reciprocal Rank Fusion)，即倒排序排名算法，该算法是滑铁卢大学（CAN）和 Google 合作开发的，而且该算法的原理其实非常简单，公式如下：<br><img src="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2025/20250506161754.png" alt="image.png"><br>在 RRF 算法中，D 表示相关文档的全集，k 是固定常数 60，r(d) 表示当前文档 d 在其子集中的位置，该算法会对全集 D 进行二重遍历，外层遍历文档全集 D，内层遍历文档子集，在做内层遍历的时候，我们会累计当前文档在其所在子集中的位置并取倒数作为其权重。<br>常数 k 被设定为 60，这个值是在进行初步调查时确定的，在论文中，通过四个试点实验，每个实验结合了 30 种搜索配置应用于不同的 TREC 集合的结果，发现 k&#x3D;60 接近最优值，k 值是多少并不是关键，主要是通过 k 值，可以很容易发现一个事实：<br>虽然高排名的文档更加重要，但低排名文档的重要性并不会像使用指数函数那样消失。</p>
<h3 id="6-3-2-多查询结果融合策略实现"><a href="#6-3-2-多查询结果融合策略实现" class="headerlink" title="6.3.2 多查询结果融合策略实现"></a>6.3.2 多查询结果融合策略实现</h3><p>在 LangChain 中并没有直接实现 RAG多查询结果融合策略 的检索器，所以可以考虑自定义实现，或者是继承 MultiQueryRetriever 并重写 retrieve_docments() 与 unique_union() 方法来实现对文档的 RRF 排名计算与合并。在方法内部将每次检索到的内容填充到一个两层列表中，然后传递给 RRF 函数即可。<br><strong>示例代码</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">List</span>  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">import</span> dotenv  </span><br><span class="line"><span class="keyword">import</span> weaviate  </span><br><span class="line"><span class="keyword">from</span> langchain.load <span class="keyword">import</span> dumps, loads  </span><br><span class="line"><span class="keyword">from</span> langchain.retrievers <span class="keyword">import</span> MultiQueryRetriever  </span><br><span class="line"><span class="keyword">from</span> langchain_core.callbacks <span class="keyword">import</span> CallbackManagerForRetrieverRun  </span><br><span class="line"><span class="keyword">from</span> langchain_core.documents <span class="keyword">import</span> Document  </span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> OpenAIEmbeddings, ChatOpenAI  </span><br><span class="line"><span class="keyword">from</span> langchain_weaviate <span class="keyword">import</span> WeaviateVectorStore  </span><br><span class="line"><span class="keyword">from</span> weaviate.auth <span class="keyword">import</span> AuthApiKey  </span><br><span class="line">  </span><br><span class="line">dotenv.load_dotenv()  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RAGFusionRetriever</span>(<span class="title class_ inherited__">MultiQueryRetriever</span>):  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;RAG多查询结果融合策略检索器&quot;&quot;&quot;</span>  </span><br><span class="line">    k: <span class="built_in">int</span> = <span class="number">4</span>  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">retrieve_documents</span>(<span class="params">  </span></span><br><span class="line"><span class="params">            self, queries: <span class="type">List</span>[<span class="built_in">str</span>], run_manager: CallbackManagerForRetrieverRun  </span></span><br><span class="line"><span class="params">    </span>) -&gt; <span class="type">List</span>[<span class="type">List</span>]:  </span><br><span class="line">        <span class="string">&quot;&quot;&quot;重写检索文档函数，返回值变成一个嵌套的列表&quot;&quot;&quot;</span>  </span><br><span class="line">        documents = []  </span><br><span class="line">        <span class="keyword">for</span> query <span class="keyword">in</span> queries:  </span><br><span class="line">            docs = self.retriever.invoke(  </span><br><span class="line">                query, config=&#123;<span class="string">&quot;callbacks&quot;</span>: run_manager.get_child()&#125;  </span><br><span class="line">            )  </span><br><span class="line">            documents.append(docs)  </span><br><span class="line">        <span class="keyword">return</span> documents  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">unique_union</span>(<span class="params">self, documents: <span class="type">List</span>[<span class="type">List</span>]</span>) -&gt; <span class="type">List</span>[Document]:  </span><br><span class="line">        <span class="string">&quot;&quot;&quot;使用RRF算法来去重合并对应的文档，参数为嵌套列表，返回值为文档列表&quot;&quot;&quot;</span>  </span><br><span class="line">        <span class="comment"># 1.定义一个变量存储每个文档的得分信息  </span></span><br><span class="line">        fused_result = &#123;&#125;  </span><br><span class="line">  </span><br><span class="line">        <span class="comment"># 2.循环两层获取每一个文档信息  </span></span><br><span class="line">        <span class="keyword">for</span> docs <span class="keyword">in</span> documents:  </span><br><span class="line">            <span class="keyword">for</span> rank, doc <span class="keyword">in</span> <span class="built_in">enumerate</span>(docs):  </span><br><span class="line">                <span class="comment"># 3.使用dumps函数将类示例转换成字符串  </span></span><br><span class="line">                doc_str = dumps(doc)  </span><br><span class="line">                <span class="comment"># 4.判断下该文档的字符串是否已经计算过得分  </span></span><br><span class="line">                <span class="keyword">if</span> doc_str <span class="keyword">not</span> <span class="keyword">in</span> fused_result:  </span><br><span class="line">                    fused_result[doc_str] = <span class="number">0</span>  </span><br><span class="line">                <span class="comment"># 5.计算新的分  </span></span><br><span class="line">                fused_result[doc_str] += <span class="number">1</span> / (rank + <span class="number">60</span>)  </span><br><span class="line">  </span><br><span class="line">        <span class="comment"># 6.执行排序操作，获取相应的数据，使用的是降序  </span></span><br><span class="line">        reranked_results = [  </span><br><span class="line">            (loads(doc), score)  </span><br><span class="line">            <span class="keyword">for</span> doc, score <span class="keyword">in</span> <span class="built_in">sorted</span>(fused_result.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)  </span><br><span class="line">        ]  </span><br><span class="line">  </span><br><span class="line">        <span class="keyword">return</span> [item[<span class="number">0</span>] <span class="keyword">for</span> item <span class="keyword">in</span> reranked_results[:self.k]]  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 1.构建向量数据库与检索器  </span></span><br><span class="line">db = WeaviateVectorStore(  </span><br><span class="line">    client=weaviate.connect_to_wcs(  </span><br><span class="line">        cluster_url=<span class="string">&quot;https://mbakeruerziae6psyex7ng.c0.us-west3.gcp.weaviate.cloud&quot;</span>,  </span><br><span class="line">        auth_credentials=AuthApiKey(<span class="string">&quot;ZltPVa9ZSOxUcfafelsggGyyH6tnTYQYJvBx&quot;</span>),  </span><br><span class="line">    ),  </span><br><span class="line">    index_name=<span class="string">&quot;DatasetDemo&quot;</span>,  </span><br><span class="line">    text_key=<span class="string">&quot;text&quot;</span>,  </span><br><span class="line">    embedding=OpenAIEmbeddings(model=<span class="string">&quot;text-embedding-3-small&quot;</span>),  </span><br><span class="line">)  </span><br><span class="line">retriever = db.as_retriever(search_type=<span class="string">&quot;mmr&quot;</span>)  </span><br><span class="line">  </span><br><span class="line">rag_fusion_retriever = RAGFusionRetriever.from_llm(  </span><br><span class="line">    retriever=retriever,  </span><br><span class="line">    llm=ChatOpenAI(model=<span class="string">&quot;gpt-3.5-turbo-16k&quot;</span>, temperature=<span class="number">0</span>),  </span><br><span class="line">)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 3.执行检索  </span></span><br><span class="line">docs = rag_fusion_retriever.invoke(<span class="string">&quot;关于LLMOps应用配置的文档有哪些&quot;</span>)  </span><br><span class="line"><span class="built_in">print</span>(docs)  </span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(docs))</span><br></pre></td></tr></table></figure>
<p><strong>输出：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[Document(metadata=&#123;<span class="string">&#x27;source&#x27;</span>: <span class="string">&#x27;./项目API文档.md&#x27;</span>, <span class="string">&#x27;start_index&#x27;</span>: 0.0&#125;, page_content=<span class="string">&#x27;LLMOps 项目 API 文档\n\n应用 API 接口统一以 JSON 格式返回，并且包含 3 个字段：code、data 和 message，分别代表业务状态码、业务数据和接口附加信息。\n\n业务状态码共有 6 种，其中只有 success(成功) 代表业务操作成功，其他 5 种状态均代表失败，并且失败时会附加相关的信息：fail(通用失败)、not_found(未找到)、unauthorized(未授权)、forbidden(无权限)和validate_error(数据验证失败)。\n\n接口示例：\n\njson &#123; &quot;code&quot;: &quot;success&quot;, &quot;data&quot;: &#123; &quot;redirect_url&quot;: &quot;https://github.com/login/oauth/authorize?client_id=f69102c6b97d90d69768&amp;redirect_uri=http%3A%2F%2Flocalhost%3A5001%2Foauth%2Fauthorize%2Fgithub&amp;scope=user%3Aemail&quot; &#125;, &quot;message&quot;: &quot;&quot; &#125;&#x27;</span>), Document(metadata=&#123;<span class="string">&#x27;source&#x27;</span>: <span class="string">&#x27;./项目API文档.md&#x27;</span>, <span class="string">&#x27;start_index&#x27;</span>: 5818.0&#125;, page_content=<span class="string">&#x27;json &#123; &quot;code&quot;: &quot;success&quot;, &quot;data&quot;: &#123; &quot;list&quot;: [ &#123; &quot;id&quot;: &quot;1550b71a-1444-47ed-a59d-c2f080fbae94&quot;, &quot;conversation_id&quot;: &quot;2d7d3e3f-95c9-4d9d-ba9c-9daaf09cc8a8&quot;, &quot;query&quot;: &quot;能详细讲解下LLM是什么吗？&quot;, &quot;answer&quot;: &quot;LLM 即 Large Language Model，大语言模型，是一种基于深度学习的自然语言处理模型，具有很高的语言理解和生成能力，能够处理各式各样的自然语言任务，例如文本生成、问答、翻译、摘要等。它通过在大量的文本数据上进行训练，学习到语言的模式、结构和语义知识&#x27;</span>), Document(metadata=&#123;<span class="string">&#x27;source&#x27;</span>: <span class="string">&#x27;./项目API文档.md&#x27;</span>, <span class="string">&#x27;start_index&#x27;</span>: 3042.0&#125;, page_content=<span class="string">&#x27;1.2 [todo]更新应用草稿配置信息\n\n接口说明：更新应用的草稿配置信息，涵盖：模型配置、长记忆模式等，该接口会查找该应用原始的草稿配置并进行更新，如果没有原始草稿配置，则创建一个新配置作为草稿配置。\n\n接口信息：授权+POST:/apps/:app_id/config\n\n接口参数：\n\n请求参数：\n\napp_id -&gt; str：需要修改配置的应用 id。\n\nmodel_config -&gt; json：模型配置信息。\n\ndialog_round -&gt; int：携带上下文轮数，类型为非负整型。\n\nmemory_mode -&gt; string：记忆类型，涵盖长记忆 long_term_memory 和 none 代表无。\n\n请求示例：\n\njson &#123; &quot;model_config&quot;: &#123; &quot;dialog_round&quot;: 10 &#125;, &quot;memory_mode&quot;: &quot;long_term_memory&quot; &#125;\n\n响应示例：\n\njson &#123; &quot;code&quot;: &quot;success&quot;, &quot;data&quot;: &#123;&#125;, &quot;message&quot;: &quot;更新AI应用配置成功&quot; &#125;\n\n1.3 [todo]获取应用调试长记忆&#x27;</span>), Document(metadata=&#123;<span class="string">&#x27;source&#x27;</span>: <span class="string">&#x27;./项目API文档.md&#x27;</span>, <span class="string">&#x27;start_index&#x27;</span>: 675.0&#125;, page_content=<span class="string">&#x27;json &#123; &quot;code&quot;: &quot;success&quot;, &quot;data&quot;: &#123; &quot;list&quot;: [ &#123; &quot;app_count&quot;: 0, &quot;created_at&quot;: 1713105994, &quot;description&quot;: &quot;这是专门用来存储慕课LLMOps课程信息的知识库&quot;, &quot;document_count&quot;: 13, &quot;icon&quot;: &quot;https://imooc-llmops-1257184990.cos.ap-guangzhou.myqcloud.com/2024/04/07/96b5e270-c54a-4424-aece-ff8a2b7e4331.png&quot;, &quot;id&quot;: &quot;c0759ca8-2d35-4480-83a8-1f41f29d1401&quot;, &quot;name&quot;: &quot;慕课LLMOps课程知识库&quot;, &quot;updated_at&quot;: 1713106758, &quot;word_count&quot;: 8850 &#125; ], &quot;paginator&quot;: &#123; &quot;current_page&quot;: 1, &quot;page_size&quot;: 20, &quot;total_page&quot;: 1, &quot;total_record&quot;: 2 &#125; &#125;&#x27;</span>)]</span><br><span class="line"></span><br><span class="line">4</span><br></pre></td></tr></table></figure>
<h2 id="6-4-问题分解策略提升复杂问题检索正确率"><a href="#6-4-问题分解策略提升复杂问题检索正确率" class="headerlink" title="6.4 问题分解策略提升复杂问题检索正确率"></a>6.4 问题分解策略提升复杂问题检索正确率</h2><h3 id="6-4-1-复杂问题检索的难点与分解"><a href="#6-4-1-复杂问题检索的难点与分解" class="headerlink" title="6.4.1 复杂问题检索的难点与分解"></a>6.4.1 复杂问题检索的难点与分解</h3><p>在 RAG 应用开发中，对于一些提问相对复杂的原始问题来说，无论是使用原始问题进行检索，亦或者生成多个相关联的问题进行检索，往往都很难在向量数据库中找到关联性高的文档，导致 RAG 效果偏差。<br>例如向量数据库中存储了一份 机器的说明文档，对于这类数据，如果提问 如何完成某个部件的维修 这类问题，一般都会涉及到多个步骤与顺序，执行相似性搜索会有很大概率没法找到有关联的文档。<br>造成这个问题的原因有几种：</p>
<p>1. 复杂问题由多个问题按顺序步骤组成，执行相似性搜索时，向量数据库存储的都是基础文档数据，往往相似度低，但是这些数据在现实世界又可能存在很大的关联（文本嵌入模型的限制，一条向量不可能无损记录段落信息）。<br>2. 问题复杂度高或者涉及到数学问题，导致 LLM 没法一次性完成答案的生成，一次性传递大量的相关性文档，极大压缩了大语言模型生成内容上下文长度的限制。</p>
<p>对于这类 RAG 应用场景，可以使用 问题分解策略，将一个复杂问题分解成多个子问题，和 多查询重写策略 不一样的是，这个策略生成的子问题使用的是 深度优先，即解决完第一个问题后，对应的资料传递给第二个问题，以此类推；亦或者是并行将每个问题的答案合并成最终问题。<br>所以 问题分解策略 可以划分成两种方案：迭代式回答 与 并行式回答，两种方案的运行流程如下<br><img src="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2025/20250506162440.png" alt="image.png"><br>其中迭代式回答，会将上一次的 提问+答案，还有这一次的 检索上下文 一起传递给 LLM，让其生成答案，迭代到最后一次，就是最终答案。而 并行式回答 则会同时检索，并同时调用 LLM 生成答案，最后在将答案进行汇总，让 LLM 整理生成最终答案。</p>
<h3 id="6-4-2-迭代式回答实现"><a href="#6-4-2-迭代式回答实现" class="headerlink" title="6.4.2 迭代式回答实现"></a>6.4.2 迭代式回答实现</h3><p>在 LangChain 中，并没有针对 问题分解策略 实现对应的 检索器 或者 预设链，所以只能自行实现这个优化策略，由于问题分解策略同样也是先生成对应的子问题（深入优先），所以需要单独构建一条链先进行问题的分解，然后迭代执行相应的检索，得到上下文，并使用 LLM 回复该问题，将得到的 迭代答案+问题，传递给下一个子问题。<br><strong>代码示例：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> operator <span class="keyword">import</span> itemgetter  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">import</span> dotenv  </span><br><span class="line"><span class="keyword">import</span> weaviate  </span><br><span class="line"><span class="keyword">from</span> langchain_core.output_parsers <span class="keyword">import</span> StrOutputParser  </span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate  </span><br><span class="line"><span class="keyword">from</span> langchain_core.runnables <span class="keyword">import</span> RunnablePassthrough  </span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI, OpenAIEmbeddings  </span><br><span class="line"><span class="keyword">from</span> langchain_weaviate <span class="keyword">import</span> WeaviateVectorStore  </span><br><span class="line"><span class="keyword">from</span> weaviate.auth <span class="keyword">import</span> AuthApiKey  </span><br><span class="line">  </span><br><span class="line">dotenv.load_dotenv()  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">format_qa_pair</span>(<span class="params">question: <span class="built_in">str</span>, answer: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;格式化传递的问题+答案为单个字符串&quot;&quot;&quot;</span>  </span><br><span class="line">    <span class="keyword">return</span> <span class="string">f&quot;Question: <span class="subst">&#123;question&#125;</span>\nAnswer: <span class="subst">&#123;answer&#125;</span>\n\n&quot;</span>.strip()  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 1.定义分解子问题的prompt  </span></span><br><span class="line">decomposition_prompt = ChatPromptTemplate.from_template(  </span><br><span class="line">    <span class="string">&quot;你是一个乐于助人的AI助理，可以针对一个输入问题生成多个相关的子问题。\n&quot;</span>  </span><br><span class="line">    <span class="string">&quot;目标是将输入问题分解成一组可以独立回答的子问题或者子任务。\n&quot;</span>  </span><br><span class="line">    <span class="string">&quot;生成与一下问题相关的多个搜索查询：&#123;question&#125;\n&quot;</span>  </span><br><span class="line">    <span class="string">&quot;并使用换行符进行分割，输出（3个子问题/子查询）：&quot;</span>  </span><br><span class="line">)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 2.构建分解问题链  </span></span><br><span class="line">decomposition_chain = (  </span><br><span class="line">        &#123;<span class="string">&quot;question&quot;</span>: RunnablePassthrough()&#125;  </span><br><span class="line">        | decomposition_prompt  </span><br><span class="line">        | ChatOpenAI(model=<span class="string">&quot;gpt-3.5-turbo-16k&quot;</span>, temperature=<span class="number">0</span>)  </span><br><span class="line">        | StrOutputParser()  </span><br><span class="line">        | (<span class="keyword">lambda</span> x: x.strip().split(<span class="string">&quot;\n&quot;</span>))  </span><br><span class="line">)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 3.构建向量数据库与检索器  </span></span><br><span class="line">db = WeaviateVectorStore(  </span><br><span class="line">    client=weaviate.connect_to_wcs(  </span><br><span class="line">        cluster_url=<span class="string">&quot;https://mbakeruerziae6psyex7ng.c0.us-west3.gcp.weaviate.cloud&quot;</span>,  </span><br><span class="line">        auth_credentials=AuthApiKey(<span class="string">&quot;ZltPVa9ZSOxUcfafelsggGyyH6tnTYQYJvBx&quot;</span>),  </span><br><span class="line">    ),  </span><br><span class="line">    index_name=<span class="string">&quot;DatasetDemo&quot;</span>,  </span><br><span class="line">    text_key=<span class="string">&quot;text&quot;</span>,  </span><br><span class="line">    embedding=OpenAIEmbeddings(model=<span class="string">&quot;text-embedding-3-small&quot;</span>),  </span><br><span class="line">)  </span><br><span class="line">retriever = db.as_retriever(search_type=<span class="string">&quot;mmr&quot;</span>)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 4.执行提问获取子问题  </span></span><br><span class="line">question = <span class="string">&quot;关于LLMOps应用配置的文档有哪些&quot;</span>  </span><br><span class="line">sub_questions = decomposition_chain.invoke(question)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 5.构建迭代问答链：提示模板+链  </span></span><br><span class="line">prompt = ChatPromptTemplate.from_template(<span class="string">&quot;&quot;&quot;这是你需要回答的问题：  </span></span><br><span class="line"><span class="string">---  </span></span><br><span class="line"><span class="string">&#123;question&#125;  </span></span><br><span class="line"><span class="string">---  </span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">这是所有可用的背景问题和答案对：  </span></span><br><span class="line"><span class="string">---  </span></span><br><span class="line"><span class="string">&#123;qa_pairs&#125;  </span></span><br><span class="line"><span class="string">---  </span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">这是与问题相关的额外背景信息：  </span></span><br><span class="line"><span class="string">---  </span></span><br><span class="line"><span class="string">&#123;context&#125;  </span></span><br><span class="line"><span class="string">---&quot;&quot;&quot;</span>)  </span><br><span class="line">chain = (  </span><br><span class="line">        &#123;  </span><br><span class="line">            <span class="string">&quot;question&quot;</span>: itemgetter(<span class="string">&quot;question&quot;</span>),  </span><br><span class="line">            <span class="string">&quot;qa_pairs&quot;</span>: itemgetter(<span class="string">&quot;qa_pairs&quot;</span>),  </span><br><span class="line">            <span class="string">&quot;context&quot;</span>: itemgetter(<span class="string">&quot;question&quot;</span>) | retriever,  </span><br><span class="line">        &#125;  </span><br><span class="line">        | prompt  </span><br><span class="line">        | ChatOpenAI(model=<span class="string">&quot;gpt-3.5-turbo-16k&quot;</span>, temperature=<span class="number">0</span>)  </span><br><span class="line">        | StrOutputParser()  </span><br><span class="line">)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 5.循环遍历所有子问题进行检索并获取答案  </span></span><br><span class="line">qa_pairs = <span class="string">&quot;&quot;</span>  </span><br><span class="line"><span class="keyword">for</span> sub_question <span class="keyword">in</span> sub_questions:  </span><br><span class="line">    answer = chain.invoke(&#123;<span class="string">&quot;question&quot;</span>: sub_question, <span class="string">&quot;qa_pairs&quot;</span>: qa_pairs&#125;)  </span><br><span class="line">    qa_pair = format_qa_pair(sub_question, answer)  </span><br><span class="line">    qa_pairs += <span class="string">&quot;\n---\n&quot;</span> + qa_pair  </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;问题: <span class="subst">&#123;sub_question&#125;</span>&quot;</span>)  </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;答案: <span class="subst">&#123;answer&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="6-5-Step-Back-回答回退策略"><a href="#6-5-Step-Back-回答回退策略" class="headerlink" title="6.5 Step-Back 回答回退策略"></a>6.5 Step-Back 回答回退策略</h2><h3 id="6-5-1-少量示例模板"><a href="#6-5-1-少量示例模板" class="headerlink" title="6.5.1 少量示例模板"></a>6.5.1 少量示例模板</h3><p>在与 LLM 的对话中，提供少量的示例被称为 少量示例，这是一种简单但强大的指导生成的方式，在某些情况下可以显著提高模型性能（与之对应的是零样本），少量示例可以降低 Prompt 的复杂度，快速告知 LLM 生成内容的规范。</p>
<p>在 LangChain 中，针对少量示例也封装对应的提示模板——FewShotPromptTemplate，这个提示模板只需要传递 示例列表 与 示例模板 即可快速构建 少量示例提示模板，使用示例如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> dotenv  </span><br><span class="line"><span class="keyword">from</span> langchain_core.output_parsers <span class="keyword">import</span> StrOutputParser  </span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate, FewShotChatMessagePromptTemplate  </span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI  </span><br><span class="line">  </span><br><span class="line">dotenv.load_dotenv()  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 1.构建示例模板与示例  </span></span><br><span class="line">example_prompt = ChatPromptTemplate.from_messages([  </span><br><span class="line">    (<span class="string">&quot;human&quot;</span>, <span class="string">&quot;&#123;question&#125;&quot;</span>),  </span><br><span class="line">    (<span class="string">&quot;ai&quot;</span>, <span class="string">&quot;&#123;answer&#125;&quot;</span>),  </span><br><span class="line">])  </span><br><span class="line">examples = [  </span><br><span class="line">    &#123;<span class="string">&quot;question&quot;</span>: <span class="string">&quot;帮我计算下2+2等于多少？&quot;</span>, <span class="string">&quot;answer&quot;</span>: <span class="string">&quot;4&quot;</span>&#125;,  </span><br><span class="line">    &#123;<span class="string">&quot;question&quot;</span>: <span class="string">&quot;帮我计算下2+3等于多少？&quot;</span>, <span class="string">&quot;answer&quot;</span>: <span class="string">&quot;5&quot;</span>&#125;,  </span><br><span class="line">    &#123;<span class="string">&quot;question&quot;</span>: <span class="string">&quot;帮我计算下20*15等于多少？&quot;</span>, <span class="string">&quot;answer&quot;</span>: <span class="string">&quot;300&quot;</span>&#125;,  </span><br><span class="line">]  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 2.构建少量示例提示模板  </span></span><br><span class="line">few_shot_prompt = FewShotChatMessagePromptTemplate(  </span><br><span class="line">    example_prompt=example_prompt,  </span><br><span class="line">    examples=examples,  </span><br><span class="line">)  </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;少量示例模板:&quot;</span>, few_shot_prompt.<span class="built_in">format</span>())  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 3.构建最终提示模板  </span></span><br><span class="line">prompt = ChatPromptTemplate.from_messages([  </span><br><span class="line">    (<span class="string">&quot;system&quot;</span>, <span class="string">&quot;你是一个可以计算复杂数学问题的聊天机器人&quot;</span>),  </span><br><span class="line">    few_shot_prompt,  </span><br><span class="line">    (<span class="string">&quot;human&quot;</span>, <span class="string">&quot;&#123;question&#125;&quot;</span>),  </span><br><span class="line">])  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 4.创建大语言模型与链  </span></span><br><span class="line">llm = ChatOpenAI(model=<span class="string">&quot;gpt-3.5-turbo-16k&quot;</span>, temperature=<span class="number">0</span>)  </span><br><span class="line">chain = prompt | llm | StrOutputParser()  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 5.调用链获取结果  </span></span><br><span class="line"><span class="built_in">print</span>(chain.invoke(<span class="string">&quot;帮我计算下14*15等于多少&quot;</span>))</span><br></pre></td></tr></table></figure>
<p><strong>输出：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">少量示例模板: Human: 帮我计算下2+2等于多少？</span><br><span class="line">AI: 4</span><br><span class="line">Human: 帮我计算下2+3等于多少？</span><br><span class="line">AI: 5</span><br><span class="line">Human: 帮我计算下20*15等于多少？</span><br><span class="line">AI: 300</span><br><span class="line">210</span><br></pre></td></tr></table></figure>
<p>少量示例提示模板 在底层会根据传递的 示例模板 与 示例 格式化对应的 消息列表 或者 字符串，从而将对应的示例参考字符串信息添加到完整的提示模板中，简化了 Prompt 编写的繁琐程度。<br>对于聊天模型可以使用 FewShotChatMessagePromptTemplate，而文本补全基座模型可以使用 FewShotPromptTemplate。</p>
<h3 id="6-5-2-Step-Back-回答回退策略"><a href="#6-5-2-Step-Back-回答回退策略" class="headerlink" title="6.5.2 Step-Back 回答回退策略"></a>6.5.2 Step-Back 回答回退策略</h3><p>对于一些复杂的问题，除了使用 问题分解 来得到子问题亦或者依赖问题，还可以为复杂问题生成一个前置问题，通过前置问题来执行相应的检索，这就是 Setp-Back 回答回退策略（后退提示），这是一种用于增强语言模型的推理和问题解决能力的技巧，它鼓励 LLM 从一个给定的问题或问题后退一步，提出一个更抽象、更高级的问题，涵盖原始查询的本质。<br>后退提示背后的概念是，许多复杂的问题或任务包含很多复杂的细节和约束，这使 LLM 难以直接检索和应用相关信息。通过引入一个后退问题，这个问题通常更容易回答，并且围绕一个更广泛的概念或原则，让 LLM 可以更有效地构建它们的推理。<br>Step-Back 回答回退策略的运行流程也非常简单，构建一个 少量示例提示模板，让 LLM 根据传递的问题生成一个后退问题，使用 后退问题 执行相应的检索，利用检索到的文档+原始问题执行 RAG 索引增强生成，运行流程如下：<br><img src="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2025/20250506163224.png" alt="image.png"><br>在 LangChain 中并没有封装好的 回答回退策略检索器，所以可以执行相应的封装，实现一个自定义检索器，实现代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">List</span>  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">import</span> dotenv  </span><br><span class="line"><span class="keyword">import</span> weaviate  </span><br><span class="line"><span class="keyword">from</span> langchain_core.callbacks <span class="keyword">import</span> CallbackManagerForRetrieverRun  </span><br><span class="line"><span class="keyword">from</span> langchain_core.documents <span class="keyword">import</span> Document  </span><br><span class="line"><span class="keyword">from</span> langchain_core.language_models <span class="keyword">import</span> BaseLanguageModel  </span><br><span class="line"><span class="keyword">from</span> langchain_core.output_parsers <span class="keyword">import</span> StrOutputParser  </span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate, FewShotChatMessagePromptTemplate  </span><br><span class="line"><span class="keyword">from</span> langchain_core.retrievers <span class="keyword">import</span> BaseRetriever  </span><br><span class="line"><span class="keyword">from</span> langchain_core.runnables <span class="keyword">import</span> RunnablePassthrough  </span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> OpenAIEmbeddings, ChatOpenAI  </span><br><span class="line"><span class="keyword">from</span> langchain_weaviate <span class="keyword">import</span> WeaviateVectorStore  </span><br><span class="line"><span class="keyword">from</span> weaviate.auth <span class="keyword">import</span> AuthApiKey  </span><br><span class="line">  </span><br><span class="line">dotenv.load_dotenv()  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">StepBackRetriever</span>(<span class="title class_ inherited__">BaseRetriever</span>):  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;回答回退检索器&quot;&quot;&quot;</span>  </span><br><span class="line">    retriever: BaseRetriever  </span><br><span class="line">    llm: BaseLanguageModel  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_get_relevant_documents</span>(<span class="params">  </span></span><br><span class="line"><span class="params">            self, query: <span class="built_in">str</span>, *, run_manager: CallbackManagerForRetrieverRun  </span></span><br><span class="line"><span class="params">    </span>) -&gt; <span class="type">List</span>[Document]:  </span><br><span class="line">        <span class="string">&quot;&quot;&quot;根据传递的query执行问题回退并检索&quot;&quot;&quot;</span>  </span><br><span class="line">        <span class="comment"># 1.构建少量示例提示模板  </span></span><br><span class="line">        examples = [  </span><br><span class="line">            &#123;<span class="string">&quot;input&quot;</span>: <span class="string">&quot;慕课网上有关于AI应用开发的课程吗？&quot;</span>, <span class="string">&quot;output&quot;</span>: <span class="string">&quot;慕课网上有哪些课程？&quot;</span>&#125;,  </span><br><span class="line">            &#123;<span class="string">&quot;input&quot;</span>: <span class="string">&quot;慕小课出生在哪个国家？&quot;</span>, <span class="string">&quot;output&quot;</span>: <span class="string">&quot;慕小课的人生经历是什么样的？&quot;</span>&#125;,  </span><br><span class="line">            &#123;<span class="string">&quot;input&quot;</span>: <span class="string">&quot;司机可以开快车吗？&quot;</span>, <span class="string">&quot;output&quot;</span>: <span class="string">&quot;司机可以做什么？&quot;</span>&#125;,  </span><br><span class="line">        ]  </span><br><span class="line">        example_prompt = ChatPromptTemplate.from_messages([  </span><br><span class="line">            (<span class="string">&quot;human&quot;</span>, <span class="string">&quot;&#123;input&#125;&quot;</span>),  </span><br><span class="line">            (<span class="string">&quot;ai&quot;</span>, <span class="string">&quot;&#123;output&#125;&quot;</span>),  </span><br><span class="line">        ])  </span><br><span class="line">        few_shot_prompt = FewShotChatMessagePromptTemplate(  </span><br><span class="line">            examples=examples,  </span><br><span class="line">            example_prompt=example_prompt,  </span><br><span class="line">        )  </span><br><span class="line">  </span><br><span class="line">        <span class="comment"># 2.构建生成回退问题的模板  </span></span><br><span class="line">        prompt = ChatPromptTemplate.from_messages([  </span><br><span class="line">            (<span class="string">&quot;system&quot;</span>,  </span><br><span class="line">             <span class="string">&quot;你是一个世界知识的专家。你的任务是回退问题，将问题改述为更一般或者前置问题，这样更容易回答，请参考示例来实现。&quot;</span>),  </span><br><span class="line">            few_shot_prompt,  </span><br><span class="line">            (<span class="string">&quot;human&quot;</span>, <span class="string">&quot;&#123;question&#125;&quot;</span>),  </span><br><span class="line">        ])  </span><br><span class="line">  </span><br><span class="line">        <span class="comment"># 3.构建链应用，生成回退问题，并执行相应的检索  </span></span><br><span class="line">        chain = (  </span><br><span class="line">                &#123;<span class="string">&quot;question&quot;</span>: RunnablePassthrough()&#125;  </span><br><span class="line">                | prompt  </span><br><span class="line">                | self.llm  </span><br><span class="line">                | StrOutputParser()  </span><br><span class="line">                | self.retriever  </span><br><span class="line">        )  </span><br><span class="line">  </span><br><span class="line">        <span class="keyword">return</span> chain.invoke(query)  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 1.构建向量数据库与检索器  </span></span><br><span class="line">db = WeaviateVectorStore(  </span><br><span class="line">    client=weaviate.connect_to_wcs(  </span><br><span class="line">        cluster_url=<span class="string">&quot;https://mbakeruerziae6psyex7ng.c0.us-west3.gcp.weaviate.cloud&quot;</span>,  </span><br><span class="line">        auth_credentials=AuthApiKey(<span class="string">&quot;ZltPVa9ZSOxUcfafelsggGyyH6tnTYQYJvBx&quot;</span>),  </span><br><span class="line">    ),  </span><br><span class="line">    index_name=<span class="string">&quot;DatasetDemo&quot;</span>,  </span><br><span class="line">    text_key=<span class="string">&quot;text&quot;</span>,  </span><br><span class="line">    embedding=OpenAIEmbeddings(model=<span class="string">&quot;text-embedding-3-small&quot;</span>),  </span><br><span class="line">)  </span><br><span class="line">retriever = db.as_retriever(search_type=<span class="string">&quot;mmr&quot;</span>)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 2.创建回答回退检索器  </span></span><br><span class="line">step_back_retriever = StepBackRetriever(  </span><br><span class="line">    retriever=retriever,  </span><br><span class="line">    llm=ChatOpenAI(model=<span class="string">&quot;gpt-3.5-turbo-16k&quot;</span>, temperature=<span class="number">0</span>),  </span><br><span class="line">)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 3.检索文档  </span></span><br><span class="line">documents = step_back_retriever.invoke(<span class="string">&quot;人工智能会让世界发生翻天覆地的变化吗？&quot;</span>)  </span><br><span class="line"><span class="built_in">print</span>(documents)  </span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(documents))</span><br></pre></td></tr></table></figure>
<p><strong>输出：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[Document(metadata=&#123;<span class="string">&#x27;source&#x27;</span>: <span class="string">&#x27;./项目API文档.md&#x27;</span>, <span class="string">&#x27;start_index&#x27;</span>: 5818.0&#125;, page_content=<span class="string">&#x27;json &#123; &quot;code&quot;: &quot;success&quot;, &quot;data&quot;: &#123; &quot;list&quot;: [ &#123; &quot;id&quot;: &quot;1550b71a-1444-47ed-a59d-c2f080fbae94&quot;, &quot;conversation_id&quot;: &quot;2d7d3e3f-95c9-4d9d-ba9c-9daaf09cc8a8&quot;, &quot;query&quot;: &quot;能详细讲解下LLM是什么吗？&quot;, &quot;answer&quot;: &quot;LLM 即 Large Language Model，大语言模型，是一种基于深度学习的自然语言处理模型，具有很高的语言理解和生成能力，能够处理各式各样的自然语言任务，例如文本生成、问答、翻译、摘要等。它通过在大量的文本数据上进行训练，学习到语言的模式、结构和语义知识&#x27;</span>), Document(metadata=&#123;<span class="string">&#x27;source&#x27;</span>: <span class="string">&#x27;./项目API文档.md&#x27;</span>, <span class="string">&#x27;start_index&#x27;</span>: 6359.0&#125;, page_content=<span class="string">&#x27;1.7 [todo]删除特定的调试消息\n\n接口说明：用于删除 AI 应用调试对话过程中指定的消息，该删除会在后端执行软删除操作，并且只有当会话 id 和消息 id 都匹配上时，才会删除对应的调试消息。\n\n接口信息：授权+POST:/apps/:app_id/messages/:message_id/delete\n\n接口参数：\n\n请求参数：\n\napp_id -&gt; uuid：路由参数，需要删除消息归属的应用 id，格式为 uuid。\n\nmessage_id -&gt; uuid：路由参数，需要删除的消息 id，格式为 uuid。\n\n请求示例：\n\njson &#123; &quot;app_id&quot;: &quot;1550b71a-1444-47ed-a59d-c2f080fbae94&quot;, &quot;message_id&quot;: &quot;2d7d3e3f-95c9-4d9d-ba9c-9daaf09cc8a8&quot; &#125;\n\n响应示例：\n\njson &#123; &quot;code&quot;: &quot;success&quot;, &quot;data&quot;: &#123;&#125;, &quot;message&quot;: &quot;删除调试信息成功&quot; &#125;&#x27;</span>), Document(metadata=&#123;<span class="string">&#x27;source&#x27;</span>: <span class="string">&#x27;./项目API文档.md&#x27;</span>, <span class="string">&#x27;start_index&#x27;</span>: 490.0&#125;, page_content=<span class="string">&#x27;带有分页数据的接口会在 data 内固定传递 list 和 paginator 字段，其中 list 代表分页后的列表数据，paginator 代表分页的数据。\n\npaginator 内存在 4 个字段：current_page(当前页数) 、page_size(每页数据条数)、total_page(总页数)、total_record(总记录条数)，示例数据如下：&#x27;</span>), Document(metadata=&#123;<span class="string">&#x27;source&#x27;</span>: <span class="string">&#x27;./项目API文档.md&#x27;</span>, <span class="string">&#x27;start_index&#x27;</span>: 2042.0&#125;, page_content=<span class="string">&#x27;dialog_round -&gt; int：携带上下文轮数，类型为非负整型。\n\nmemory_mode -&gt; string：记忆类型，涵盖长记忆 long_term_memory 和 none 代表无。\n\nstatus -&gt; string：应用配置的状态，drafted 代表草稿、published 代表已发布配置。\n\nupdated_at -&gt; int：应用配置的更新时间。\n\ncreated_at -&gt; int：应用配置的创建时间。\n\nupdated_at -&gt; int：应用的更新时间。\n\ncreated_at -&gt; int：应用的创建时间。\n\n响应示例：&#x27;</span>)]</span><br><span class="line"></span><br><span class="line">4</span><br></pre></td></tr></table></figure>
<p>对比 问题分解策略，回答回退策略 仅仅多调用一次 LLM，所以相应速度更快，性能更高，并且复杂度更低，对于一些参数量较小的模型，也可以实现不错的效果，对于 问题分解策略-迭代式回答，在一些极端的情况下，模型输出了有偏差的内容，每次都在有偏差的 问题+答案 生成新内容，很有可能会导致最后的输出完全偏离开始的预设。<br>就像早些年很火的 谷歌翻译将同一句话翻译20次，输出的内容就完全偏离了原来的预设</p>
<h2 id="6-6-混合策略实现-doc-doc-对称检索"><a href="#6-6-混合策略实现-doc-doc-对称检索" class="headerlink" title="6.6 混合策略实现 doc-doc 对称检索"></a>6.6 混合策略实现 doc-doc 对称检索</h2><h3 id="6-6-1-HyDE-混合策略"><a href="#6-6-1-HyDE-混合策略" class="headerlink" title="6.6.1 HyDE 混合策略"></a>6.6.1 HyDE 混合策略</h3><p>在前面的课时中，学习的优化策略都是将对应的 查询 生成 新查询，通过 新查询 来执行相应的检索，但是在数据库中存储的数据一般都是 文档 层面上的，数据会远远比 查询 要大很多，所以 query 和 doc 之间是不对称检索，能找到的相似性文档相对来说也比较少。<br>例如：今天回家的路上看到了美丽的风景，非常开心！想学习 python 该怎么办？这个请求中，前面的风景、开心等词语均为无关信息。会对真实的请求学习 python 产生干扰。如果直接搜索用户的请求，可能会产生不正确或无法回答的 LLM 响应。因此，有必要使得用户查询的语义空间与文档的语义空间保持一致。<br>特别是 query 和对应的相关内容（答案）可能只存在弱相关性，导致难以找到最相关的文档内容。</p>
<p>在这篇论文《Precise Zero-Shot Dense Retrieval without Relevance Labels》中提出了一个 HyDE混合策略 的概念，首先利用 LLM 将问题转换为回答问题的假设性文档&#x2F;假回答，然后使用嵌入的 假设性文档 去检索真实文档，前提是因为 doc-doc 这个模式执行相似性搜索可以尝试更多的匹配项。<br>假回答和真回答虽然可能存在事实错误，但是会比较像，因此能更容易找到相关内容。<br>简单来说，就是先根据 query 生成一个 doc，然后根据 doc 生成对应的 embedding，再执行相应的检索，运行流程如下：<br><img src="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2025/20250506164404.png" alt="image.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">List</span>  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">import</span> dotenv  </span><br><span class="line"><span class="keyword">import</span> weaviate  </span><br><span class="line"><span class="keyword">from</span> langchain_core.callbacks <span class="keyword">import</span> CallbackManagerForRetrieverRun  </span><br><span class="line"><span class="keyword">from</span> langchain_core.documents <span class="keyword">import</span> Document  </span><br><span class="line"><span class="keyword">from</span> langchain_core.language_models <span class="keyword">import</span> BaseLanguageModel  </span><br><span class="line"><span class="keyword">from</span> langchain_core.output_parsers <span class="keyword">import</span> StrOutputParser  </span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate  </span><br><span class="line"><span class="keyword">from</span> langchain_core.retrievers <span class="keyword">import</span> BaseRetriever  </span><br><span class="line"><span class="keyword">from</span> langchain_core.runnables <span class="keyword">import</span> RunnablePassthrough  </span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> OpenAIEmbeddings, ChatOpenAI  </span><br><span class="line"><span class="keyword">from</span> langchain_weaviate <span class="keyword">import</span> WeaviateVectorStore  </span><br><span class="line"><span class="keyword">from</span> weaviate.auth <span class="keyword">import</span> AuthApiKey  </span><br><span class="line">  </span><br><span class="line">dotenv.load_dotenv()  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">HyDERetriever</span>(<span class="title class_ inherited__">BaseRetriever</span>):  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;HyDE混合策略检索器&quot;&quot;&quot;</span>  </span><br><span class="line">    retriever: BaseRetriever  </span><br><span class="line">    llm: BaseLanguageModel  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_get_relevant_documents</span>(<span class="params">  </span></span><br><span class="line"><span class="params">            self, query: <span class="built_in">str</span>, *, run_manager: CallbackManagerForRetrieverRun  </span></span><br><span class="line"><span class="params">    </span>) -&gt; <span class="type">List</span>[Document]:  </span><br><span class="line">        <span class="string">&quot;&quot;&quot;传递检索query实现HyDE混合策略检索&quot;&quot;&quot;</span>  </span><br><span class="line">        <span class="comment"># 1.构建生成假设性文档的prompt  </span></span><br><span class="line">        prompt = ChatPromptTemplate.from_template(  </span><br><span class="line">            <span class="string">&quot;请写一篇科学论文来回答这个问题。\n&quot;</span>  </span><br><span class="line">            <span class="string">&quot;问题: &#123;question&#125;\n&quot;</span>  </span><br><span class="line">            <span class="string">&quot;文章: &quot;</span>  </span><br><span class="line">        )  </span><br><span class="line">  </span><br><span class="line">        <span class="comment"># 2.构建HyDE混合策略检索链  </span></span><br><span class="line">        chain = (  </span><br><span class="line">                &#123;<span class="string">&quot;question&quot;</span>: RunnablePassthrough()&#125;  </span><br><span class="line">                | prompt  </span><br><span class="line">                | self.llm  </span><br><span class="line">                | StrOutputParser()  </span><br><span class="line">                | self.retriever  </span><br><span class="line">        )  </span><br><span class="line">  </span><br><span class="line">        <span class="keyword">return</span> chain.invoke(query)  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 1.构建向量数据库与检索器  </span></span><br><span class="line">db = WeaviateVectorStore(  </span><br><span class="line">    client=weaviate.connect_to_wcs(  </span><br><span class="line">        cluster_url=<span class="string">&quot;https://mbakeruerziae6psyex7ng.c0.us-west3.gcp.weaviate.cloud&quot;</span>,  </span><br><span class="line">        auth_credentials=AuthApiKey(<span class="string">&quot;ZltPVa9ZSOxUcfafelsggGyyH6tnTYQYJvBx&quot;</span>),  </span><br><span class="line">    ),  </span><br><span class="line">    index_name=<span class="string">&quot;DatasetDemo&quot;</span>,  </span><br><span class="line">    text_key=<span class="string">&quot;text&quot;</span>,  </span><br><span class="line">    embedding=OpenAIEmbeddings(model=<span class="string">&quot;text-embedding-3-small&quot;</span>),  </span><br><span class="line">)  </span><br><span class="line">retriever = db.as_retriever(search_type=<span class="string">&quot;mmr&quot;</span>)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 2.创建HyDE检索器  </span></span><br><span class="line">hyde_retriever = HyDERetriever(  </span><br><span class="line">    retriever=retriever,  </span><br><span class="line">    llm=ChatOpenAI(model=<span class="string">&quot;gpt-3.5-turbo-16k&quot;</span>, temperature=<span class="number">0</span>),  </span><br><span class="line">)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 3.检索文档  </span></span><br><span class="line">documents = hyde_retriever.invoke(<span class="string">&quot;关于LLMOps应用配置的文档有哪些？&quot;</span>)  </span><br><span class="line"><span class="built_in">print</span>(documents)  </span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(documents))</span><br></pre></td></tr></table></figure>
<p><strong>输出</strong>：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[Document(metadata=&#123;<span class="string">&#x27;source&#x27;</span>: <span class="string">&#x27;./项目API文档.md&#x27;</span>, <span class="string">&#x27;start_index&#x27;</span>: 0.0&#125;, page_content=<span class="string">&#x27;LLMOps 项目 API 文档\n\n应用 API 接口统一以 JSON 格式返回，并且包含 3 个字段：code、data 和 message，分别代表业务状态码、业务数据和接口附加信息。\n\n业务状态码共有 6 种，其中只有 success(成功) 代表业务操作成功，其他 5 种状态均代表失败，并且失败时会附加相关的信息：fail(通用失败)、not_found(未找到)、unauthorized(未授权)、forbidden(无权限)和validate_error(数据验证失败)。\n\n接口示例：\n\njson &#123; &quot;code&quot;: &quot;success&quot;, &quot;data&quot;: &#123; &quot;redirect_url&quot;: &quot;https://github.com/login/oauth/authorize?client_id=f69102c6b97d90d69768&amp;redirect_uri=http%3A%2F%2Flocalhost%3A5001%2Foauth%2Fauthorize%2Fgithub&amp;scope=user%3Aemail&quot; &#125;, &quot;message&quot;: &quot;&quot; &#125;&#x27;</span>), Document(metadata=&#123;<span class="string">&#x27;source&#x27;</span>: <span class="string">&#x27;./项目API文档.md&#x27;</span>, <span class="string">&#x27;start_index&#x27;</span>: 1621.0&#125;, page_content=<span class="string">&#x27;id -&gt; uuid：应用 id，类型为 uuid。\n\nname -&gt; string：应用名称。\n\nicon -&gt; string：应用图标。\n\ndescription -&gt; string：应用描述。\n\npublished_app_config_id -&gt; uuid：已发布应用配置 id，如果不存在则为 null。\n\ndrafted_app_config_id -&gt; uuid：草稿应用配置 id，如果不存在则为 null。\n\ndebug_conversation_id -&gt; uuid：调试会话记录 id，如果不存在则为 null。\n\npublished_app_config/drafted_app_config -&gt; json：应用配置信息，涵盖草稿配置、已发布配置，如果没有则为 null，两个配置的变量信息一致。\n\nid -&gt; uuid：应用配置 id。\n\nmodel_config -&gt; json：模型配置，类型为 json。\n\ndialog_round -&gt; int：携带上下文轮数，类型为非负整型。&#x27;</span>), Document(metadata=&#123;<span class="string">&#x27;source&#x27;</span>: <span class="string">&#x27;./项目API文档.md&#x27;</span>, <span class="string">&#x27;start_index&#x27;</span>: 5818.0&#125;, page_content=<span class="string">&#x27;json &#123; &quot;code&quot;: &quot;success&quot;, &quot;data&quot;: &#123; &quot;list&quot;: [ &#123; &quot;id&quot;: &quot;1550b71a-1444-47ed-a59d-c2f080fbae94&quot;, &quot;conversation_id&quot;: &quot;2d7d3e3f-95c9-4d9d-ba9c-9daaf09cc8a8&quot;, &quot;query&quot;: &quot;能详细讲解下LLM是什么吗？&quot;, &quot;answer&quot;: &quot;LLM 即 Large Language Model，大语言模型，是一种基于深度学习的自然语言处理模型，具有很高的语言理解和生成能力，能够处理各式各样的自然语言任务，例如文本生成、问答、翻译、摘要等。它通过在大量的文本数据上进行训练，学习到语言的模式、结构和语义知识&#x27;</span>), Document(metadata=&#123;<span class="string">&#x27;source&#x27;</span>: <span class="string">&#x27;./项目API文档.md&#x27;</span>, <span class="string">&#x27;start_index&#x27;</span>: 490.0&#125;, page_content=<span class="string">&#x27;带有分页数据的接口会在 data 内固定传递 list 和 paginator 字段，其中 list 代表分页后的列表数据，paginator 代表分页的数据。\n\npaginator 内存在 4 个字段：current_page(当前页数) 、page_size(每页数据条数)、total_page(总页数)、total_record(总记录条数)，示例数据如下：&#x27;</span>)]</span><br><span class="line"></span><br><span class="line">4</span><br></pre></td></tr></table></figure>
<h3 id="6-6-2-局限性"><a href="#6-6-2-局限性" class="headerlink" title="6.6.2 局限性"></a>6.6.2 局限性</h3><p>对于 doc-doc 类型的检索，虽然在语义空间上保持了一致，但是在 query-&gt;doc 的过程中，受限于各种因素，仍然可能产生错误信息。<br>第一个场景是在 query 没有足够上下文时，HyDE 容易误解对应的词，从而产生错误的信息。<br>例如提问 Bel是什么？，在没有执行 HyDE 混合策略而是直接查询得到答案如下<br>Bel 是由 Paul Graham 在四年的时间里（2015年3月26日至2019年10月12日），用 Arc 语言编写的一种编程语言。它基于 John McCarthy 最初的 Lisp，但添加了额外的功能。它是一个以代码形式表达的规范，旨在成为计算的形式化模型，是图灵机的一种替代方案。<br>但是执行 HyDE 混合策略生成假设性 doc 如下<br>Bel 是 Paul Graham 的化名，他是这段信息背后的作者，当时需要种子资金以维持生活，并且参与了一项交易，后来成为 Y Combinator 模式的典范。<br>在这个例子中，HyDE 在没有文档上下文的情况下错误地解释了 Bel，这会导致完全检索不到相关的文档信息。<br>第二个场景是一些 开放式的查询，HyDE 可能会产生偏见，例如提问 作者会如何评价艺术与工程的区别？，无需转换 query 即可得到正确的响应回答<br>作者可能会说，艺术和工程是两种需要不同技能和方法的学科。艺术更注重表达和创造力，而工程更专注于解决问题和技术知识。作者还暗示，艺术学校并不总是提供与工程学校同等水平的严谨性，绘画学生常常被鼓励发展个性化风格，而不是学习绘画的基础知识。此外，作者可能会指出，工程学相比艺术能提供更多的财务稳定性，正如作者自己创业初期需要种子资金来生活的经历所证明的那样。<br>在使用 HyDE混合策略 转换 query 时，生成的 doc 如下<br>作者可能会说，艺术比工程更持久和独立。他们提到，今天编写的软件几十年后就会过时，系统工作也不会长久。相比之下，他们指出绘画可以保留数百年，而且作为艺术家是可以谋生的。他们还提到，作为艺术家，你可以真正独立，不需要老板或研究资金。此外，他们指出艺术可以成为收入来源，适合那些无法接触传统就业形式的人，比如例子中的模特，能够通过为当地古董商建模和制作赝品而谋生。<br>总的来说，HyDE 是一个无监督的方法，可以帮助 RAG 提高效果。但是因为它不完全依赖于 embedding 而是强调问题的答案和查找内容的相似性，也存在一定的局限性。比如如果 LLM 无法理解用户问题，自然不会产生最佳结果，也可能导致错误增加。因此，需要根据场景决定是否选用此方法</p>
<h2 id="6-7-集成多种检索器算法的混合检索"><a href="#6-7-集成多种检索器算法的混合检索" class="headerlink" title="6.7 集成多种检索器算法的混合检索"></a>6.7 集成多种检索器算法的混合检索</h2><h3 id="6-7-1-集成检索器的优势与使用"><a href="#6-7-1-集成检索器的优势与使用" class="headerlink" title="6.7.1 集成检索器的优势与使用"></a>6.7.1 集成检索器的优势与使用</h3><p>在 LangChain 中，封装了一个集成检索器 EnsembleRetriever，这个检索器接受一个检索器列表作为输入，并根据 RRF 算法对每个检索器的 get_relevant_documents() 方法产生的文档列表进行集成和重新排序。<br>集成检索器可以利用不同算法的优势，从而获得比任何单一算法更好的性能。集成检索器一个常见的案例是将 稀疏检索器(如BM25) 和 密集检索器(如嵌入相似度) 结合起来，因为它们的优势是互补的，这种检索方式也被称为混合检索（稀疏检索器擅长基于关键词检索，密集检索器擅长基于语义相似性检索）。</p>
<p>混合检索器 被广泛应用于各类 AI 应用开发平台，例如：Dify、Coze、智谱 等平台，</p>
<p>例如使用 BM25关键词搜索 和 FAISS相似性搜索 进行结合，实现混合搜索，首先安装 rank_bm25 包，命令如下</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -U rank_bm25</span><br></pre></td></tr></table></figure>
<p><strong>代码示例</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> dotenv  </span><br><span class="line"><span class="keyword">from</span> langchain.retrievers <span class="keyword">import</span> EnsembleRetriever  </span><br><span class="line"><span class="keyword">from</span> langchain_community.retrievers <span class="keyword">import</span> BM25Retriever  </span><br><span class="line"><span class="keyword">from</span> langchain_community.vectorstores <span class="keyword">import</span> FAISS  </span><br><span class="line"><span class="keyword">from</span> langchain_core.documents <span class="keyword">import</span> Document  </span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> OpenAIEmbeddings  </span><br><span class="line">  </span><br><span class="line">dotenv.load_dotenv()  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 1.创建文档列表  </span></span><br><span class="line">documents = [  </span><br><span class="line">    Document(page_content=<span class="string">&quot;笨笨是一只很喜欢睡觉的猫咪&quot;</span>, metadata=&#123;<span class="string">&quot;page&quot;</span>: <span class="number">1</span>&#125;),  </span><br><span class="line">    Document(page_content=<span class="string">&quot;我喜欢在夜晚听音乐，这让我感到放松。&quot;</span>, metadata=&#123;<span class="string">&quot;page&quot;</span>: <span class="number">2</span>&#125;),  </span><br><span class="line">    Document(page_content=<span class="string">&quot;猫咪在窗台上打盹，看起来非常可爱。&quot;</span>, metadata=&#123;<span class="string">&quot;page&quot;</span>: <span class="number">3</span>&#125;),  </span><br><span class="line">    Document(page_content=<span class="string">&quot;学习新技能是每个人都应该追求的目标。&quot;</span>, metadata=&#123;<span class="string">&quot;page&quot;</span>: <span class="number">4</span>&#125;),  </span><br><span class="line">    Document(page_content=<span class="string">&quot;我最喜欢的食物是意大利面，尤其是番茄酱的那种。&quot;</span>, metadata=&#123;<span class="string">&quot;page&quot;</span>: <span class="number">5</span>&#125;),  </span><br><span class="line">    Document(page_content=<span class="string">&quot;昨晚我做了一个奇怪的梦，梦见自己在太空飞行。&quot;</span>, metadata=&#123;<span class="string">&quot;page&quot;</span>: <span class="number">6</span>&#125;),  </span><br><span class="line">    Document(page_content=<span class="string">&quot;我的手机突然关机了，让我有些焦虑。&quot;</span>, metadata=&#123;<span class="string">&quot;page&quot;</span>: <span class="number">7</span>&#125;),  </span><br><span class="line">    Document(page_content=<span class="string">&quot;阅读是我每天都会做的事情，我觉得很充实。&quot;</span>, metadata=&#123;<span class="string">&quot;page&quot;</span>: <span class="number">8</span>&#125;),  </span><br><span class="line">    Document(page_content=<span class="string">&quot;他们一起计划了一次周末的野餐，希望天气能好。&quot;</span>, metadata=&#123;<span class="string">&quot;page&quot;</span>: <span class="number">9</span>&#125;),  </span><br><span class="line">    Document(page_content=<span class="string">&quot;我的狗喜欢追逐球，看起来非常开心。&quot;</span>, metadata=&#123;<span class="string">&quot;page&quot;</span>: <span class="number">10</span>&#125;),  </span><br><span class="line">]  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 2.构建BM25关键词检索器  </span></span><br><span class="line">bm25_retriever = BM25Retriever.from_documents(documents)  </span><br><span class="line">bm25_retriever.k = <span class="number">4</span>  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 3.创建FAISS向量数据库检索  </span></span><br><span class="line">faiss_db = FAISS.from_documents(documents, embedding=OpenAIEmbeddings(model=<span class="string">&quot;text-embedding-3-small&quot;</span>))  </span><br><span class="line">faiss_retriever = faiss_db.as_retriever(search_kwargs=&#123;<span class="string">&quot;k&quot;</span>: <span class="number">4</span>&#125;)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 4.初始化集成检索器  </span></span><br><span class="line">ensemble_retriever = EnsembleRetriever(  </span><br><span class="line">    retrievers=[bm25_retriever, faiss_retriever],  </span><br><span class="line">    weights=[<span class="number">0.5</span>, <span class="number">0.5</span>],  </span><br><span class="line">)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 5.执行检索  </span></span><br><span class="line">docs = ensemble_retriever.invoke(<span class="string">&quot;除了猫，你养了什么宠物呢？&quot;</span>)  </span><br><span class="line"><span class="built_in">print</span>(docs)  </span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(docs))</span><br></pre></td></tr></table></figure>
<p><strong>输出：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[Document(metadata=&#123;<span class="string">&#x27;page&#x27;</span>: 10&#125;, page_content=<span class="string">&#x27;我的狗喜欢追逐球，看起来非常开心。&#x27;</span>), Document(metadata=&#123;<span class="string">&#x27;page&#x27;</span>: 3&#125;, page_content=<span class="string">&#x27;猫咪在窗台上打盹，看起来非常可爱。&#x27;</span>), Document(metadata=&#123;<span class="string">&#x27;page&#x27;</span>: 9&#125;, page_content=<span class="string">&#x27;他们一起计划了一次周末的野餐，希望天气能好。&#x27;</span>), Document(metadata=&#123;<span class="string">&#x27;page&#x27;</span>: 1&#125;, page_content=<span class="string">&#x27;笨笨是一只很喜欢睡觉的猫咪&#x27;</span>), Document(metadata=&#123;<span class="string">&#x27;page&#x27;</span>: 8&#125;, page_content=<span class="string">&#x27;阅读是我每天都会做的事情，我觉得很充实。&#x27;</span>), Document(metadata=&#123;<span class="string">&#x27;page&#x27;</span>: 7&#125;, page_content=<span class="string">&#x27;我的手机突然关机了，让我有些焦虑。&#x27;</span>), Document(metadata=&#123;<span class="string">&#x27;page&#x27;</span>: 5&#125;, page_content=<span class="string">&#x27;我最喜欢的食物是意大利面，尤其是番茄酱的那种。&#x27;</span>)]</span><br><span class="line"></span><br><span class="line">7</span><br></pre></td></tr></table></figure>
<p>在实际的开发中，除了硬编码不同检索器与对应的权重，还可以在运行时配置检索器，在检索时动态控制某个检索器输出内容的数量、权重等，例如</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">faiss_retriever = faiss_db.as_retriever(search_kwargs=&#123;<span class="string">&quot;k&quot;</span>: <span class="number">4</span>&#125;).configurable_fields(</span><br><span class="line">    search_kwargs=ConfigurableField(</span><br><span class="line">        <span class="built_in">id</span>=<span class="string">&quot;search_kwargs_faiss&quot;</span>,</span><br><span class="line">        name=<span class="string">&quot;搜索参数&quot;</span>,</span><br><span class="line">        description=<span class="string">&quot;要使用的搜索参数&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">config = &#123;<span class="string">&quot;configurable&quot;</span>: &#123;<span class="string">&quot;search_kwargs_faiss&quot;</span>: &#123;<span class="string">&quot;k&quot;</span>: <span class="number">1</span>&#125;&#125;&#125;</span><br><span class="line">docs = ensemble_retriever.invoke(<span class="string">&quot;苹果&quot;</span>, config=config)</span><br></pre></td></tr></table></figure>
<h3 id="6-7-2-查询转换阶段优化策略总结"><a href="#6-7-2-查询转换阶段优化策略总结" class="headerlink" title="6.7.2 查询转换阶段优化策略总结"></a>6.7.2 查询转换阶段优化策略总结</h3><p>在 RAG 的 查询转换 阶段，目前市面上主流的优化策略其实我们都已经讲解完了，涵盖了：<strong>多查询重写、RAG 多查询结果融合、问题分解策略、回答回退策略、HyDE 混合策略、集成检索器策略</strong>等，不同的优化策略有不同的优缺点：<br>1. 多查询重写：实现简单，使用参数较小的模型也可以完成对查询的转换（不涉及回答），因为多查询可以并行检索，所以性能较高，但是在合并的时候，没有考虑到不同文档的权重，仅仅按照顺序进行合并，会让某些高权重的文档在使用时可能被剔除<br><img src="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2025/20250506165356.png" alt="image.png"><br>2. 问题分解策略：通过将复杂问题&#x2F;数学问题分解成多个子问题，从而实现对每个子问题的 检索-生成 流程，最后再将所有子问题的 答案 进行合<br>并，在转换环节涉及到对子问题的回答，所以对于中间 LLM 的要求比较高，性能相对也比较差，在上下文长度不足的情况下，拆分问题并迭代回答，可能会让最终答案偏离原始的提问。<br><img src="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2025/20250506165449.png" alt="image.png"><br>3. 回答回退策略：通过提出一个前置问题&#x2F;通用问题用于优化原始的复杂问题，从而获得更大的搜索范围，提升检索到相关性高的文档的概率，因为中间 LLM 没涉及到回答，所以可以使用参数量较小的模型来实现，性能相对较高<br><img src="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2025/20250506165644.png" alt="image.png"></p>
<h2 id="6-8-检索器的逻辑路由缩减检索范围-–-逻辑路由阶段"><a href="#6-8-检索器的逻辑路由缩减检索范围-–-逻辑路由阶段" class="headerlink" title="6.8 检索器的逻辑路由缩减检索范围 – 逻辑路由阶段"></a>6.8 检索器的逻辑路由缩减检索范围 – 逻辑路由阶段</h2><p>在 RAG 应用开发中，想根据不同的问题检索不同的 检索器&#x2F;向量数据库，其实只需要设定要对应的 Prompt，然后让 LLM 根据传递的问题返回需要选择的 检索器&#x2F;向量数据库 的名称，然后根据得到的名称选择不同的 检索器 即可。</p>
<p>但是对于 LLM 来说，如果使用普通的 prompt 来约束输出内容的格式与规范，因为 LLM 的特性，很难保证输出格式符合特定的需求，所以可以考虑使用 函数回调 来实现，即设定一个 虚假的函数，告诉 LLM，这个函数有对应的参数，让 LLM 强制调用这个函数，这个时候 LLM 就会输出函数的调用参数，从而保证输出的统一性。</p>
<p>使用 函数回调 实现的检索器逻辑路由运行流程图如下<br><img src="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2025/20250506165653.png" alt="image.png"><br>假设目前有 3 个向量数据库&#x2F;集合，分别代表 python_docs、js_docs、golang_docs，需要根据用户传递的问题判断与哪个向量数据库最接近，使用最接近的向量数据库进行检索，代码示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Literal</span>  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">import</span> dotenv  </span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate  </span><br><span class="line"><span class="keyword">from</span> langchain_core.pydantic_v1 <span class="keyword">import</span> BaseModel, Field  </span><br><span class="line"><span class="keyword">from</span> langchain_core.runnables <span class="keyword">import</span> RunnablePassthrough  </span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI  </span><br><span class="line">  </span><br><span class="line">dotenv.load_dotenv()  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RouteQuery</span>(<span class="title class_ inherited__">BaseModel</span>):  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;将用户查询映射到最相关的数据源&quot;&quot;&quot;</span>  </span><br><span class="line">    datasource: <span class="type">Literal</span>[<span class="string">&quot;python_docs&quot;</span>, <span class="string">&quot;js_docs&quot;</span>, <span class="string">&quot;golang_docs&quot;</span>] = Field(  </span><br><span class="line">        description=<span class="string">&quot;根据给定用户问题，选择哪个数据源最相关以回答他们的问题&quot;</span>  </span><br><span class="line">    )  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">choose_route</span>(<span class="params">result: RouteQuery</span>) -&gt; <span class="built_in">str</span>:  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;根据传递的路由结果选择不同的检索器&quot;&quot;&quot;</span>  </span><br><span class="line">    <span class="keyword">if</span> <span class="string">&quot;python_docs&quot;</span> <span class="keyword">in</span> result.datasource:  </span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;chain in python_docs&quot;</span>  </span><br><span class="line">    <span class="keyword">elif</span> <span class="string">&quot;js_docs&quot;</span> <span class="keyword">in</span> result.datasource:  </span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;chain in js_docs&quot;</span>  </span><br><span class="line">    <span class="keyword">else</span>:  </span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;golang_docs&quot;</span>  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 1.构建大语言模型并进行结构化输出  </span></span><br><span class="line">llm = ChatOpenAI(model=<span class="string">&quot;gpt-3.5-turbo-16k&quot;</span>, temperature=<span class="number">0</span>)  </span><br><span class="line">structured_llm = llm.with_structured_output(RouteQuery)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 2.创建路由逻辑链  </span></span><br><span class="line">prompt = ChatPromptTemplate.from_messages([  </span><br><span class="line">    (<span class="string">&quot;system&quot;</span>, <span class="string">&quot;你是一个擅长将用户问题路由到适当的数据源的专家。\n请根据问题涉及的编程语言，将其路由到相关数据源&quot;</span>),  </span><br><span class="line">    (<span class="string">&quot;human&quot;</span>, <span class="string">&quot;&#123;question&#125;&quot;</span>)  </span><br><span class="line">])  </span><br><span class="line">router = &#123;<span class="string">&quot;question&quot;</span>: RunnablePassthrough()&#125; | prompt | structured_llm | choose_route  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 3.执行相应的提问，检查映射的路由  </span></span><br><span class="line">question = <span class="string">&quot;&quot;&quot;为什么下面的代码不工作了，请帮我检查下：  </span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">from langchain_core.prompts import ChatPromptTemplate  </span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">prompt = ChatPromptTemplate.from_messages([&quot;human&quot;, &quot;speak in &#123;language&#125;&quot;])  </span></span><br><span class="line"><span class="string">prompt.invoke(&quot;中文&quot;)&quot;&quot;&quot;</span>  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 4.选择不同的数据库  </span></span><br><span class="line"><span class="built_in">print</span>(router.invoke(question))</span><br></pre></td></tr></table></figure>
<p><strong>输出：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">datasource=<span class="string">&#x27;python_docs&#x27;</span></span><br><span class="line">chain <span class="keyword">for</span> python_docs</span><br></pre></td></tr></table></figure>

<h2 id="6-9-语义路由选择不同的-Prompt-模板-–-逻辑路由阶段"><a href="#6-9-语义路由选择不同的-Prompt-模板-–-逻辑路由阶段" class="headerlink" title="6.9 语义路由选择不同的 Prompt 模板 – 逻辑路由阶段"></a>6.9 语义路由选择不同的 Prompt 模板 – 逻辑路由阶段</h2><p>在 RAG 应用开发中，针对不同场景的问题使用 特定化的prompt模板 效果一般都会比通用模板会好一些，例如在 教培场景，制作一个可以教学 物理+数学 的授课机器人，如果使用通用的 prompt模板，会导致 prompt 编写变得非常复杂；反过来如果 prompt 写的简单，有可能没法起到很好的回复效果。<br><strong>代码示例：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> dotenv  </span><br><span class="line"><span class="keyword">from</span> langchain.utils.math <span class="keyword">import</span> cosine_similarity  </span><br><span class="line"><span class="keyword">from</span> langchain_core.output_parsers <span class="keyword">import</span> StrOutputParser  </span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate  </span><br><span class="line"><span class="keyword">from</span> langchain_core.runnables <span class="keyword">import</span> RunnablePassthrough, RunnableLambda  </span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> OpenAIEmbeddings, ChatOpenAI  </span><br><span class="line">  </span><br><span class="line">dotenv.load_dotenv()  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 1.定义两份不同的prompt模板(物理模板、数学模板)  </span></span><br><span class="line">physics_template = <span class="string">&quot;&quot;&quot;你是一位非常聪明的物理教程。  </span></span><br><span class="line"><span class="string">你擅长以简洁易懂的方式回答物理问题。  </span></span><br><span class="line"><span class="string">当你不知道问题的答案时，你会坦率承认自己不知道。  </span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">这是一个问题：  </span></span><br><span class="line"><span class="string">&#123;query&#125;&quot;&quot;&quot;</span>  </span><br><span class="line">math_template = <span class="string">&quot;&quot;&quot;你是一位非常优秀的数学家。你擅长回答数学问题。  </span></span><br><span class="line"><span class="string">你之所以如此优秀，是因为你能将复杂的问题分解成多个小步骤。  </span></span><br><span class="line"><span class="string">并且回答这些小步骤，然后将它们整合在一起回来更广泛的问题。  </span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">这是一个问题：  </span></span><br><span class="line"><span class="string">&#123;query&#125;&quot;&quot;&quot;</span>  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 2.创建文本嵌入模型，并执行嵌入  </span></span><br><span class="line">embeddings = OpenAIEmbeddings(model=<span class="string">&quot;text-embedding-3-small&quot;</span>)  </span><br><span class="line">prompt_templates = [physics_template, math_template]  </span><br><span class="line">prompt_embeddings = embeddings.embed_documents(prompt_templates)  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">prompt_router</span>(<span class="params"><span class="built_in">input</span></span>) -&gt; ChatPromptTemplate:  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;根据传递的query计算返回不同的提示模板&quot;&quot;&quot;</span>  </span><br><span class="line">    <span class="comment"># 1.计算传入query的嵌入向量  </span></span><br><span class="line">    query_embedding = embeddings.embed_query(<span class="built_in">input</span>[<span class="string">&quot;query&quot;</span>])  </span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 2.计算相似性  </span></span><br><span class="line">    similarity = cosine_similarity([query_embedding], prompt_embeddings)[<span class="number">0</span>]  </span><br><span class="line">    most_similar = prompt_templates[similarity.argmax()]  </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;使用数学模板&quot;</span> <span class="keyword">if</span> most_similar == math_template <span class="keyword">else</span> <span class="string">&quot;使用物理模板&quot;</span>)  </span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 3.构建提示模板  </span></span><br><span class="line">    <span class="keyword">return</span> ChatPromptTemplate.from_template(most_similar)  </span><br><span class="line">  </span><br><span class="line">chain = (  </span><br><span class="line">        &#123;<span class="string">&quot;query&quot;</span>: RunnablePassthrough()&#125;  </span><br><span class="line">        | RunnableLambda(prompt_router)  </span><br><span class="line">        | ChatOpenAI(model=<span class="string">&quot;gpt-3.5-turbo-16k&quot;</span>)  </span><br><span class="line">        | StrOutputParser()  </span><br><span class="line">)  </span><br><span class="line">  </span><br><span class="line"><span class="built_in">print</span>(chain.invoke(<span class="string">&quot;黑洞是什么?&quot;</span>))  </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;======================&quot;</span>)  </span><br><span class="line"><span class="built_in">print</span>(chain.invoke(<span class="string">&quot;能介绍下余弦计算公式么？&quot;</span>))</span><br></pre></td></tr></table></figure>
<h2 id="6-10-自查询检索器实现动态元数据过滤-–-查询构建阶段"><a href="#6-10-自查询检索器实现动态元数据过滤-–-查询构建阶段" class="headerlink" title="6.10 自查询检索器实现动态元数据过滤 – 查询构建阶段"></a>6.10 自查询检索器实现动态元数据过滤 – 查询构建阶段</h2><h3 id="6-10-1-自查询检索器实现"><a href="#6-10-1-自查询检索器实现" class="headerlink" title="6.10.1 自查询检索器实现"></a>6.10.1 自查询检索器实现</h3><p>在 RAG 应用开发中，检索外部数据时，前面的优化案例中，无论是生成的 子查询、问题分解、生成假设性文档，最后在执行检索的时候使用的都是固定的筛选条件（没有附加过滤的相似性搜索）。</p>
<p>但是在某些情况下，用户发起的原始提问其实隐式携带了 筛选条件，例如提问：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">请帮我整理下关于2023年全年关于AI的新闻汇总。</span><br></pre></td></tr></table></figure>
<p>在这段 原始提问 中，如果执行相应的向量数据库相似性搜索，其实是附加了 筛选条件 的，即 year&#x3D;2023，但是在普通的相似性搜索中，是不会考虑 2023 年这个条件的（因为没有添加元数据过滤器，2022年和2023年数据在高维空间其实很接近），存在很大概率会将其他年份的数据也检索出来。<br>并不是所有的数据都支持 查询构建 的，需要看存储的 Document 是否存在元数据，对应的数据库类型是否支持筛选<br>将 查询构建 这个步骤单独拎出来，它的运行流程其实很简单，但是底层的操作非常麻烦，如下：<br><img src="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2025/20250506170342.png" alt="image.png"><br>在 LangChain 中，针对一些高频使用的向量数据库封装了 自查询检索器 的相关支持——SelfQueryRetriever，无需自行构建转换语句与解析，使用该类进行二次包装即可。<br>SelfQueryRetriever 使用起来也非常简单，以 Pinecone 向量数据库为例，首先安装对应的依赖：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install --upgrade --quiet lark</span><br></pre></td></tr></table></figure>
<p>定义好 带元数据的文档、支持过滤的元数据、包装的向量数据库、文档内容的描述 等信息，即可进行快速包装，示例代码如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> dotenv  </span><br><span class="line"><span class="keyword">from</span> langchain.chains.query_constructor.schema <span class="keyword">import</span> AttributeInfo  </span><br><span class="line"><span class="keyword">from</span> langchain.retrievers <span class="keyword">import</span> SelfQueryRetriever  </span><br><span class="line"><span class="keyword">from</span> langchain_core.documents <span class="keyword">import</span> Document  </span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI  </span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> OpenAIEmbeddings  </span><br><span class="line"><span class="keyword">from</span> langchain_pinecone <span class="keyword">import</span> PineconeVectorStore  </span><br><span class="line">  </span><br><span class="line">dotenv.load_dotenv()  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 1.构建文档列表并上传到数据库  </span></span><br><span class="line">documents = [  </span><br><span class="line">    Document(  </span><br><span class="line">        page_content=<span class="string">&quot;肖申克的救赎&quot;</span>,  </span><br><span class="line">        metadata=&#123;<span class="string">&quot;year&quot;</span>: <span class="number">1994</span>, <span class="string">&quot;rating&quot;</span>: <span class="number">9.7</span>, <span class="string">&quot;director&quot;</span>: <span class="string">&quot;弗兰克·德拉邦特&quot;</span>&#125;,  </span><br><span class="line">    ),  </span><br><span class="line">    Document(  </span><br><span class="line">        page_content=<span class="string">&quot;霸王别姬&quot;</span>,  </span><br><span class="line">        metadata=&#123;<span class="string">&quot;year&quot;</span>: <span class="number">1993</span>, <span class="string">&quot;rating&quot;</span>: <span class="number">9.6</span>, <span class="string">&quot;director&quot;</span>: <span class="string">&quot;陈凯歌&quot;</span>&#125;,  </span><br><span class="line">    ),  </span><br><span class="line">    Document(  </span><br><span class="line">        page_content=<span class="string">&quot;阿甘正传&quot;</span>,  </span><br><span class="line">        metadata=&#123;<span class="string">&quot;year&quot;</span>: <span class="number">1994</span>, <span class="string">&quot;rating&quot;</span>: <span class="number">9.5</span>, <span class="string">&quot;director&quot;</span>: <span class="string">&quot;罗伯特·泽米吉斯&quot;</span>&#125;,  </span><br><span class="line">    ),  </span><br><span class="line">    Document(  </span><br><span class="line">        page_content=<span class="string">&quot;泰坦尼克号&quot;</span>,  </span><br><span class="line">        metadat=&#123;<span class="string">&quot;year&quot;</span>: <span class="number">1997</span>, <span class="string">&quot;rating&quot;</span>: <span class="number">9.5</span>, <span class="string">&quot;director&quot;</span>: <span class="string">&quot;詹姆斯·卡梅隆&quot;</span>&#125;,  </span><br><span class="line">    ),  </span><br><span class="line">    Document(  </span><br><span class="line">        page_content=<span class="string">&quot;千与千寻&quot;</span>,  </span><br><span class="line">        metadat=&#123;<span class="string">&quot;year&quot;</span>: <span class="number">2001</span>, <span class="string">&quot;rating&quot;</span>: <span class="number">9.4</span>, <span class="string">&quot;director&quot;</span>: <span class="string">&quot;宫崎骏&quot;</span>&#125;,  </span><br><span class="line">    ),  </span><br><span class="line">    Document(  </span><br><span class="line">        page_content=<span class="string">&quot;星际穿越&quot;</span>,  </span><br><span class="line">        metadat=&#123;<span class="string">&quot;year&quot;</span>: <span class="number">2014</span>, <span class="string">&quot;rating&quot;</span>: <span class="number">9.4</span>, <span class="string">&quot;director&quot;</span>: <span class="string">&quot;克里斯托弗·诺兰&quot;</span>&#125;,  </span><br><span class="line">    ),  </span><br><span class="line">    Document(  </span><br><span class="line">        page_content=<span class="string">&quot;忠犬八公的故事&quot;</span>,  </span><br><span class="line">        metadat=&#123;<span class="string">&quot;year&quot;</span>: <span class="number">2009</span>, <span class="string">&quot;rating&quot;</span>: <span class="number">9.4</span>, <span class="string">&quot;director&quot;</span>: <span class="string">&quot;莱塞·霍尔斯道姆&quot;</span>&#125;,  </span><br><span class="line">    ),  </span><br><span class="line">    Document(  </span><br><span class="line">        page_content=<span class="string">&quot;三傻大闹宝莱坞&quot;</span>,  </span><br><span class="line">        metadat=&#123;<span class="string">&quot;year&quot;</span>: <span class="number">2009</span>, <span class="string">&quot;rating&quot;</span>: <span class="number">9.2</span>, <span class="string">&quot;director&quot;</span>: <span class="string">&quot;拉库马·希拉尼&quot;</span>&#125;,  </span><br><span class="line">    ),  </span><br><span class="line">    Document(  </span><br><span class="line">        page_content=<span class="string">&quot;疯狂动物城&quot;</span>,  </span><br><span class="line">        metadat=&#123;<span class="string">&quot;year&quot;</span>: <span class="number">2016</span>, <span class="string">&quot;rating&quot;</span>: <span class="number">9.2</span>, <span class="string">&quot;director&quot;</span>: <span class="string">&quot;拜伦·霍华德&quot;</span>&#125;,  </span><br><span class="line">    ),  </span><br><span class="line">    Document(  </span><br><span class="line">        page_content=<span class="string">&quot;无间道&quot;</span>,  </span><br><span class="line">        metadat=&#123;<span class="string">&quot;year&quot;</span>: <span class="number">2002</span>, <span class="string">&quot;rating&quot;</span>: <span class="number">9.3</span>, <span class="string">&quot;director&quot;</span>: <span class="string">&quot;刘伟强&quot;</span>&#125;,  </span><br><span class="line">    ),  </span><br><span class="line">]  </span><br><span class="line">db = PineconeVectorStore(  </span><br><span class="line">    index_name=<span class="string">&quot;llmops&quot;</span>,  </span><br><span class="line">    embedding=OpenAIEmbeddings(model=<span class="string">&quot;text-embedding-3-small&quot;</span>),  </span><br><span class="line">    namespace=<span class="string">&quot;dataset&quot;</span>,  </span><br><span class="line">    text_key=<span class="string">&quot;text&quot;</span>  </span><br><span class="line">)  </span><br><span class="line">retriever = db.as_retriever()  </span><br><span class="line"><span class="comment"># db.add_documents(documents)  </span></span><br><span class="line">  </span><br><span class="line"><span class="comment"># 2.创建自查询元数据  </span></span><br><span class="line">metadata_filed_info = [  </span><br><span class="line">    AttributeInfo(name=<span class="string">&quot;year&quot;</span>, description=<span class="string">&quot;电影的年份&quot;</span>, <span class="built_in">type</span>=<span class="string">&quot;integer&quot;</span>),  </span><br><span class="line">    AttributeInfo(name=<span class="string">&quot;rating&quot;</span>, description=<span class="string">&quot;电影的评分&quot;</span>, <span class="built_in">type</span>=<span class="string">&quot;float&quot;</span>),  </span><br><span class="line">    AttributeInfo(name=<span class="string">&quot;director&quot;</span>, description=<span class="string">&quot;电影的导演&quot;</span>, <span class="built_in">type</span>=<span class="string">&quot;string&quot;</span>),  </span><br><span class="line">]  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 3.创建自查询检索  </span></span><br><span class="line">self_query_retriever = SelfQueryRetriever.from_llm(  </span><br><span class="line">    llm=ChatOpenAI(model=<span class="string">&quot;gpt-3.5-turbo-16k&quot;</span>, temperature=<span class="number">0</span>),  </span><br><span class="line">    vectorstore=db,  </span><br><span class="line">    document_contents=<span class="string">&quot;电影的名字&quot;</span>,  </span><br><span class="line">    metadata_field_info=metadata_filed_info,  </span><br><span class="line">    enable_limit=<span class="literal">True</span>,  </span><br><span class="line">)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 4.检索示例  </span></span><br><span class="line">docs = self_query_retriever.invoke(<span class="string">&quot;查找下评分高于9.5分的电影&quot;</span>)  </span><br><span class="line"><span class="built_in">print</span>(docs)  </span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(docs))  </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;===================&quot;</span>)  </span><br><span class="line">base_docs = retriever.invoke(<span class="string">&quot;查找下评分高于9.5分的电影&quot;</span>)  </span><br><span class="line"><span class="built_in">print</span>(base_docs)  </span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(base_docs))</span><br></pre></td></tr></table></figure>
<p><strong>输出：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[Document(metadata=&#123;<span class="string">&#x27;director&#x27;</span>: <span class="string">&#x27;陈凯歌&#x27;</span>, <span class="string">&#x27;rating&#x27;</span>: <span class="number">9.6</span>, <span class="string">&#x27;year&#x27;</span>: <span class="number">1993.0</span>&#125;, page_content=<span class="string">&#x27;霸王别姬&#x27;</span>), Document(metadata=&#123;<span class="string">&#x27;director&#x27;</span>: <span class="string">&#x27;弗兰克·德拉邦特&#x27;</span>, <span class="string">&#x27;rating&#x27;</span>: <span class="number">9.7</span>, <span class="string">&#x27;year&#x27;</span>: <span class="number">1994.0</span>&#125;, page_content=<span class="string">&#x27;肖申克的救赎&#x27;</span>)]</span><br><span class="line"></span><br><span class="line"><span class="number">2</span></span><br></pre></td></tr></table></figure>
<p>自查询检索器 对于<strong>面向特定领域的专用 Agent 效果相对较好</strong>（<strong>对通用 Agent 来说效果较差</strong>），因为这些领域的文档一般相对来说比较规范，例如：<strong>财报、新闻、自媒体文章、教培</strong>等行业，这些行业的数据都能剥离出通用支持过滤与筛选的 元数据&#x2F;字段，使用自查询检索器能抽象出对应的检索字段信息。</p>
<h3 id="6-10-2-自查询检索器执行逻辑"><a href="#6-10-2-自查询检索器执行逻辑" class="headerlink" title="6.10.2 自查询检索器执行逻辑"></a>6.10.2 自查询检索器执行逻辑</h3><p>在 LangChain 中，涉及调用第三方服务或者调用本地自定义工具的，例如课程中学习的 自查询检索器、检索器逻辑路由 等，在底层都是通过一个预设好的 Prompt 生成符合相应规则的内容（字符串、JSON），然后通过 解析器 解析生成的内容，并将解析出来的结构化内容调用特定的接口、服务亦或者本地函数实现。<br>例如在 自查询检索器 底层，首先使用 FewShotPromptTemplate+函数回调&#x2F;结构化输出 生成特定规则的 查询语句。<br><strong>原始问题如下：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">查找下评分高于9.5分的电影</span><br></pre></td></tr></table></figure>
<p><strong>生成的查询语句原文如下：</strong></p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;filter&quot;</span><span class="punctuation">:</span> <span class="string">&quot;gt(\&quot;rating\&quot;, 9.5)&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>接下来使用特定的转换器，将生成的查询语句转换成适配向量数据库的 过滤器，并在检索时传递该参数，从而完成自查询构建的全过程，不同的向量数据库对应的转换器差异也非常大。</p>
<h2 id="6-11-MultiVector-实现多向量检索文档-–-索引阶段"><a href="#6-11-MultiVector-实现多向量检索文档-–-索引阶段" class="headerlink" title="6.11 MultiVector 实现多向量检索文档 – 索引阶段"></a>6.11 MultiVector 实现多向量检索文档 – 索引阶段</h2><h3 id="6-11-1-多表征-向量索引"><a href="#6-11-1-多表征-向量索引" class="headerlink" title="6.11.1 多表征&#x2F;向量索引"></a>6.11.1 多表征&#x2F;向量索引</h3><p>如果能从多个维度记录该文档块的信息，会大大增加该文档块被检索到的概率，多个维度记录信息 等同于为文档块生成 多个向量，支持的方法如下：<br>1. 把文档切割成更小的块：通过检索更小的块，但是查找其父类文档（ParentDocumentRetriever）。<br>2. 摘要：使用 LLM 为每个文档块生成一段摘要，将其和原文档一起嵌入或者代替，返回时返回原文档。<br>3. 假设性问题：使用 LLM 为每个文档块生成适合回答的假设性问题，将其和原文档一起嵌入或者代替，返回时返回原文档。<br>通过这种方式可以为一个文档块生成多条特征&#x2F;向量，<strong>在检索时能提升关联文档被检索到的概率</strong>，多向量检索的运行流程其实也非常简单，以 摘要文档 检索 原文档 为例，运行流程图如下：<br><img src="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2025/20250506172104.png" alt="image.png"><br>通过上面的运行流程，可以很容易知道在 原始文档 和 摘要文档 中都在元数据中设置了 唯一标识，从向量数据库中找到符合规则的数据后，通过查找其 元数据 的唯一标识，即可在 文档数据库 中匹配出原文档，完成整个多表征&#x2F;向量的检索。</p>
<h3 id="6-11-2-多向量索引示例"><a href="#6-11-2-多向量索引示例" class="headerlink" title="6.11.2 多向量索引示例"></a>6.11.2 多向量索引示例</h3><p>在 LangChain 中，为多向量索引的集成封装了 MultiVectorRetriever 类，实例化该类只需要传递 向量数据库、字节存储数据库(文档数据库)、id标识(关联标识) 即可快速完成整个运行流程的集成。<br>以 FAISS向量数据库 和 本地文件存储库 为例，构建一个 存储摘要-&gt;检索原文 的优化策略，代码示例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> uuid  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">import</span> dotenv  </span><br><span class="line"><span class="keyword">from</span> langchain.retrievers <span class="keyword">import</span> MultiVectorRetriever  </span><br><span class="line"><span class="keyword">from</span> langchain.storage <span class="keyword">import</span> LocalFileStore  </span><br><span class="line"><span class="keyword">from</span> langchain_community.document_loaders <span class="keyword">import</span> UnstructuredFileLoader  </span><br><span class="line"><span class="keyword">from</span> langchain_community.vectorstores <span class="keyword">import</span> FAISS  </span><br><span class="line"><span class="keyword">from</span> langchain_core.documents <span class="keyword">import</span> Document  </span><br><span class="line"><span class="keyword">from</span> langchain_core.output_parsers <span class="keyword">import</span> StrOutputParser  </span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate  </span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> OpenAIEmbeddings, ChatOpenAI  </span><br><span class="line"><span class="keyword">from</span> langchain_text_splitters <span class="keyword">import</span> RecursiveCharacterTextSplitter  </span><br><span class="line">  </span><br><span class="line">dotenv.load_dotenv()  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 1.创建加载器、文本分割器并处理文档  </span></span><br><span class="line">loader = UnstructuredFileLoader(<span class="string">&quot;./电商产品数据.txt&quot;</span>)  </span><br><span class="line">text_splitter = RecursiveCharacterTextSplitter(chunk_size=<span class="number">500</span>, chunk_overlap=<span class="number">50</span>)  </span><br><span class="line">docs = loader.load_and_split(text_splitter)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 2.定义摘要生成链  </span></span><br><span class="line">summary_chain = (  </span><br><span class="line">        &#123;<span class="string">&quot;doc&quot;</span>: <span class="keyword">lambda</span> x: x.page_content&#125;  </span><br><span class="line">        | ChatPromptTemplate.from_template(<span class="string">&quot;请总结以下文档的内容：\n\n&#123;doc&#125;&quot;</span>)  </span><br><span class="line">        | ChatOpenAI(model=<span class="string">&quot;gpt-3.5-turbo-16k&quot;</span>, temperature=<span class="number">0</span>)  </span><br><span class="line">        | StrOutputParser()  </span><br><span class="line">)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 3.批量生成摘要与唯一标识  </span></span><br><span class="line">summaries = summary_chain.batch(docs, &#123;<span class="string">&quot;max_concurrency&quot;</span>: <span class="number">5</span>&#125;)  </span><br><span class="line">doc_ids = [<span class="built_in">str</span>(uuid.uuid4()) <span class="keyword">for</span> _ <span class="keyword">in</span> summaries]  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 4.构建摘要文档  </span></span><br><span class="line">summary_docs = [  </span><br><span class="line">    Document(page_content=summary, metadata=&#123;<span class="string">&quot;doc_id&quot;</span>: doc_ids[idx]&#125;)  </span><br><span class="line">    <span class="keyword">for</span> idx, summary <span class="keyword">in</span> <span class="built_in">enumerate</span>(summaries)  </span><br><span class="line">]  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 5.构建文档数据库与向量数据库  </span></span><br><span class="line">byte_store = LocalFileStore(<span class="string">&quot;./multy-vector&quot;</span>)  </span><br><span class="line">db = FAISS.from_documents(  </span><br><span class="line">    summary_docs,  </span><br><span class="line">    embedding=OpenAIEmbeddings(model=<span class="string">&quot;text-embedding-3-small&quot;</span>),  </span><br><span class="line">)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 6.构建多向量检索器  </span></span><br><span class="line">retriever = MultiVectorRetriever(  </span><br><span class="line">    vectorstore=db,  </span><br><span class="line">    byte_store=byte_store,  </span><br><span class="line">    id_key=<span class="string">&quot;doc_id&quot;</span>,  </span><br><span class="line">)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 7.将摘要文档和原文档存储到数据库中  </span></span><br><span class="line">retriever.docstore.mset(<span class="built_in">list</span>(<span class="built_in">zip</span>(doc_ids, docs)))  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 8.执行检索  </span></span><br><span class="line">search_docs = retriever.invoke(<span class="string">&quot;推荐一些潮州特产?&quot;</span>)  </span><br><span class="line"><span class="built_in">print</span>(search_docs)  </span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(search_docs))</span><br></pre></td></tr></table></figure>
<p><strong>输出：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[Document(metadata=&#123;<span class="string">&#x27;source&#x27;</span>: <span class="string">&#x27;./电商产品数据.txt&#x27;</span>&#125;, page_content=<span class="string">&#x27;产品名称: 潮汕鱼丸\n\n电商网址: shop.example.com/fishballs\n\n产品描述: 潮汕鱼丸采用新鲜鱼肉，加入少量淀粉和调味料，手工捶打成丸，Q弹爽滑，鱼香浓郁。\n\n产品特点:\n\n原材料: 新鲜鱼肉、淀粉、盐、胡椒粉\n\n制作工艺: 传统手工捶打\n\n口感: Q弹爽滑，鲜美可口\n\n净重: 500克/袋、1000克/袋\n\n保质期: 6个月（冷冻保存）\n\n发货方式: 顺丰冷链配送，确保新鲜\n\n物流信息: 24小时内发货，预计2\n\n3天到货\n\n推荐菜系:\n\n鱼丸火锅: 搭配各类蔬菜、菌类，煮至鱼丸浮起即可。\n\n鱼丸煮汤: 与蔬菜同煮，味道鲜美。\n\n价格:\n\n500克: 55元/袋\n\n1000克: 100元/袋\n\n6. 潮汕豆腐花\n\n产品名称: 潮汕豆腐花\n\n电商网址: shop.example.com/tofupudding\n\n产品描述: 潮汕豆腐花使用优质黄豆，传统工艺制作，质地细腻，入口即化，豆香浓郁。\n\n产品特点:\n\n原材料: 黄豆、水、石膏\n\n制作工艺: 传统手工点浆\n\n口感: 细腻嫩滑，豆香浓郁\n\n净重: 450克/盒\n\n保质期: 5天（冷藏保存）&#x27;</span>), Document(metadata=&#123;<span class="string">&#x27;source&#x27;</span>: <span class="string">&#x27;./电商产品数据.txt&#x27;</span>&#125;, page_content=<span class="string">&#x27;产品特点:\n\n原材料: 猪后腿肉、香料、盐、糖\n\n制作工艺: 精细切割，手工卷制\n\n口感: 鲜嫩多汁，咸香可口\n\n净重: 400克/袋、800克/袋\n\n保质期: 3个月（冷冻保存）\n\n发货方式: 顺丰冷链配送，确保新鲜\n\n物流信息: 24小时内发货，预计2\n\n3天到货\n\n推荐菜系:\n\n猪肉卷煎烤: 切片后煎至金黄，外脆里嫩。\n\n猪肉卷炖煮: 切块后与蔬菜同炖，风味更佳。\n\n价格:\n\n400克: 58元/袋\n\n800克: 108元/袋\n\n3. 潮汕三宝（酱油、甜醋、虾酱）\n\n产品名称: 潮汕三宝\n\n电商网址: shop.example.com/chaoshanthree\n\n产品描述: 潮汕三宝包含酱油、甜醋和虾酱。酱油由大豆、麦子自然发酵而成，甜醋以糯米酿制，虾酱选用新鲜海虾发酵，是潮汕菜肴必备调味品。\n\n产品特点:\n\n酱油: 大豆、麦子自然发酵，500ml/瓶\n\n甜醋: 糯米酿制，500ml/瓶\n\n虾酱: 新鲜海虾发酵，200克/瓶\n\n保质期: 酱油和甜醋12个月，虾酱6个月\n\n发货方式: 顺丰配送，确保完好\n\n物流信息: 24小时内发货，预计2\n\n3天到货\n\n推荐菜系:&#x27;</span>), Document(metadata=&#123;<span class="string">&#x27;source&#x27;</span>: <span class="string">&#x27;./电商产品数据.txt&#x27;</span>&#125;, page_content=<span class="string">&#x27;口感: 鲜嫩多汁，味道浓郁\n\n净重: 500克/袋、1000克/袋\n\n保质期: 3个月（冷冻保存）\n\n发货方式: 顺丰冷链配送，确保新鲜\n\n物流信息: 24小时内发货，预计2\n\n3天到货\n\n推荐菜系:\n\n红烧狮子头: 加热后直接食用，适合作为主菜。\n\n狮子头炖菜: 与蔬菜同炖，味道更佳。\n\n价格:\n\n500克: 60元/袋\n\n1000克: 110元/袋\n\n10. 潮汕香菇肉酱\n\n产品名称: 潮汕香菇肉酱\n\n电商网址: shop.example.com/mushroomsauce\n\n产品描述: 潮汕香菇肉酱采用香菇和猪肉为主要原料，加入特制酱料炒制而成，香气扑鼻，味道鲜美。\n\n产品特点:\n\n原材料: 香菇、猪肉、酱料\n\n制作工艺: 精细切割，炒制均匀\n\n口感: 鲜香可口，酱香浓郁\n\n净重: 200克/瓶、400克/瓶\n\n保质期: 6个月（常温保存）\n\n发货方式: 顺丰配送，确保完好\n\n物流信息: 24小时内发货，预计2\n\n3天到货\n\n推荐菜系:\n\n拌饭: 加入米饭中，提升口感。\n\n拌面: 加入面条中，风味独特。\n\n价格:\n\n200克: 35元/瓶\n\n400克: 65元/瓶&#x27;</span>), Document(metadata=&#123;<span class="string">&#x27;source&#x27;</span>: <span class="string">&#x27;./电商产品数据.txt&#x27;</span>&#125;, page_content=<span class="string">&#x27;口感: 细腻嫩滑，豆香浓郁\n\n净重: 450克/盒\n\n保质期: 5天（冷藏保存）\n\n发货方式: 顺丰冷链配送，确保新鲜\n\n物流信息: 24小时内发货，预计2\n\n3天到货\n\n推荐菜系:\n\n甜食: 加糖水、红豆、芝麻食用。\n\n咸食: 加入虾米、葱花、酱油食用。\n\n价格: 25元/盒\n\n7. 潮汕鱼露\n\n产品名称: 潮汕鱼露\n\n电商网址: shop.example.com/fishsauce\n\n产品描述: 潮汕鱼露以新鲜小鱼为原料，经过发酵、过滤而成，味道鲜美，是潮汕菜肴必备调味品。\n\n产品特点:\n\n原材料: 小鱼、盐\n\n制作工艺: 自然发酵，传统工艺\n\n口感: 鲜美咸香\n\n净重: 500ml/瓶\n\n保质期: 12个月\n\n发货方式: 顺丰配送，确保完好\n\n物流信息: 24小时内发货，预计2\n\n3天到货\n\n推荐菜系:\n\n凉拌菜: 作为调味料使用，提升菜肴鲜味。\n\n炒菜: 适合炒菜提鲜。\n\n价格: 38元/瓶\n\n8. 潮汕糯米肠\n\n产品名称: 潮汕糯米肠\n\n电商网址: shop.example.com/glutinousrice&#x27;</span>)]</span><br><span class="line"></span><br><span class="line">4</span><br></pre></td></tr></table></figure>

<h3 id="6-11-3-假设性查询检索原文档"><a href="#6-11-3-假设性查询检索原文档" class="headerlink" title="6.11.3 假设性查询检索原文档"></a>6.11.3 假设性查询检索原文档</h3><p>除了使用 摘要 来检索全文，多向量检索一般还适用于 子文档检索父文档 和 假设性查询检索，其中 假设性查询检索 是利用 LLM 对切块后的文档生成多个 假设性标题，在向量数据库中存储 假设性标题 文档块，使用检索到的数据查找 原始文档。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">List</span>  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">import</span> dotenv  </span><br><span class="line"><span class="keyword">from</span> langchain_core.documents <span class="keyword">import</span> Document  </span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate  </span><br><span class="line"><span class="keyword">from</span> langchain_core.pydantic_v1 <span class="keyword">import</span> BaseModel, Field  </span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI  </span><br><span class="line">  </span><br><span class="line">dotenv.load_dotenv()  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">HypotheticalQuestions</span>(<span class="title class_ inherited__">BaseModel</span>):  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;生成假设性问题&quot;&quot;&quot;</span>  </span><br><span class="line">    questions: <span class="type">List</span>[<span class="built_in">str</span>] = Field(  </span><br><span class="line">        description=<span class="string">&quot;假设性问题列表，类型为字符串列表&quot;</span>,  </span><br><span class="line">    )  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 1.构建一个生成假设性问题的prompt  </span></span><br><span class="line">prompt = ChatPromptTemplate.from_template(<span class="string">&quot;生成一个包含3个假设性问题的列表，这些问题可以用于回答下面的文档:\n\n&#123;doc&#125;&quot;</span>)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 2.创建大语言模型，并绑定对应的规范化输出结构  </span></span><br><span class="line">llm = ChatOpenAI(model=<span class="string">&quot;gpt-3.5-turbo-16k&quot;</span>, temperature=<span class="number">0</span>)  </span><br><span class="line">structured_llm = llm.with_structured_output(HypotheticalQuestions)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 3.创建链应用  </span></span><br><span class="line">chain = (  </span><br><span class="line">        &#123;<span class="string">&quot;doc&quot;</span>: <span class="keyword">lambda</span> x: x.page_content&#125;  </span><br><span class="line">        | prompt  </span><br><span class="line">        | structured_llm  </span><br><span class="line">)  </span><br><span class="line">  </span><br><span class="line">hypothetical_questions: HypotheticalQuestions = chain.invoke(  </span><br><span class="line">    Document(page_content=<span class="string">&quot;我叫慕小课，我喜欢打篮球，游泳&quot;</span>)  </span><br><span class="line">)  </span><br><span class="line"><span class="built_in">print</span>(hypothetical_questions)</span><br></pre></td></tr></table></figure>
<p><strong>输出：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">questions=[<span class="string">&#x27;如果你不能打篮球，你会选择什么运动？&#x27;</span>, <span class="string">&#x27;如果你不能游泳，你会选择什么运动？&#x27;</span>, <span class="string">&#x27;如果你不能进行任何体育运动，你会选择什么爱好？&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>接下来针对每个文档生成的 假设性查询 创建 Document列表，并添加 doc_id，添加到向量数据库中，并将 doc_id 与原始文档进行绑定，存储到 文档数据库&#x2F;字节数据库 即可。</p>
<h2 id="6-12-父文档检索器实现拆分和存储平衡"><a href="#6-12-父文档检索器实现拆分和存储平衡" class="headerlink" title="6.12 父文档检索器实现拆分和存储平衡"></a>6.12 父文档检索器实现拆分和存储平衡</h2><h2 id="6-13-递归文档树检索高级RAG优化"><a href="#6-13-递归文档树检索高级RAG优化" class="headerlink" title="6.13 递归文档树检索高级RAG优化"></a>6.13 递归文档树检索高级RAG优化</h2><h2 id="6-14-ReRank-重排序提升RAG系统"><a href="#6-14-ReRank-重排序提升RAG系统" class="headerlink" title="6.14 ReRank 重排序提升RAG系统"></a>6.14 ReRank 重排序提升RAG系统</h2><h3 id="6-14-1-ReRank-重排序"><a href="#6-14-1-ReRank-重排序" class="headerlink" title="6.14.1 ReRank 重排序"></a>6.14.1 ReRank 重排序</h3><p>进一步优化筛选阶段，一般涵盖了 重排序、纠正性RAG 两种策略，其中 重排序 是使用频率最高，性价比最高，通常与 混合检索 一起搭配使用，也是目前主流的优化策略（Dify、Coze、智谱、绝大部分开源 Agent 项目都在使用）。<br>重排序 的核心思想见字知其意，即对检索到的文档 调整顺序，除此之外，重排序 一般还会增加 剔除无关&#x2F;多余数据 的步骤，在前面的课时中，我们学习的 RRF 算法其实就是重排序中最基础一种。<br>运行流程如下：<br><img src="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2025/20250506173152.png" alt="image.png"><br>并且 重排序 的逻辑是输入 文档列表，输出的仍然是 文档列表，和 DocumentTransformer 类似，不过在 LangChain 中 重排序 是一个 DocumentCompressor 组件（压缩组件），如果需要查找 重排序 组件，可以在 文档转换&#x2F;检索器集成 列表中查找，一些高频使用的组件还进行了单独的封装，例如：LongContextReorder 、Cohere Reranker 等。</p>
<p>在 LangChain 中，使用重排工具很简单，可以单独使用，也可以利用 ContextualCompressionRetriever 检索器进行二次包装合并，并传递检索器+重排工具，这样检索输出得到的结果就是经过重排的了。</p>
<h3 id="6-14-2-Cohere-重排序"><a href="#6-14-2-Cohere-重排序" class="headerlink" title="6.14.2 Cohere 重排序"></a>6.14.2 Cohere 重排序</h3><p>目前可用的重新排序模型并不多，一种是选择 Cohere 提供的在线模型，可以通过 API 访问，此外还有一些开源模型，如：bge-rerank-base 和 bge-rerank-large 等，体验与评分最好的是 Cohere 的在线模型，不过它是一项付费服务（注册账号提供免费额度），并且由于 Cohere 服务部署在海外，国内访问速度并没有这么快。</p>
<p>对于学习，我们可以使用 Cohere 的在线模型，如果是部署企业内部的服务，可以使用 bge-rerank-large，链接如下：<br>1. Cohere 在线模型：<a target="_blank" rel="noopener" href="https://cohere.com/">https://cohere.com/</a><br>2. bge-rerank-large 开源模型：<a target="_blank" rel="noopener" href="https://huggingface.co/BAAI/bge-reranker-large">https://huggingface.co/BAAI/bge-reranker-large</a></p>
<p>安装依赖：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install langchain-cohere</span><br></pre></td></tr></table></figure>
<p>在 weaviate 向量数据库中，使用 Cohere 在线模型集成重排序示例代码（使用 weaviate 数据库的 DatasetDemo 集合，存储了 项目API文档.md 文档的内容，示例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> dotenv  </span><br><span class="line"><span class="keyword">import</span> weaviate  </span><br><span class="line"><span class="keyword">from</span> langchain.retrievers <span class="keyword">import</span> ContextualCompressionRetriever  </span><br><span class="line"><span class="keyword">from</span> langchain_cohere <span class="keyword">import</span> CohereRerank  </span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> OpenAIEmbeddings  </span><br><span class="line"><span class="keyword">from</span> langchain_weaviate <span class="keyword">import</span> WeaviateVectorStore  </span><br><span class="line"><span class="keyword">from</span> weaviate.auth <span class="keyword">import</span> AuthApiKey  </span><br><span class="line">  </span><br><span class="line">dotenv.load_dotenv()  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 1.创建向量数据库与重排组件  </span></span><br><span class="line">embedding = OpenAIEmbeddings(model=<span class="string">&quot;text-embedding-3-small&quot;</span>)  </span><br><span class="line">db = WeaviateVectorStore(  </span><br><span class="line">    client=weaviate.connect_to_wcs(  </span><br><span class="line">        cluster_url=<span class="string">&quot;https://mbakeruerziae6psyex7ng.c0.us-west3.gcp.weaviate.cloud&quot;</span>,  </span><br><span class="line">        auth_credentials=AuthApiKey(<span class="string">&quot;ZltPVa9ZSOxUcfafelsggGyyH6tnTYQYJvBx&quot;</span>),  </span><br><span class="line">    ),  </span><br><span class="line">    index_name=<span class="string">&quot;DatasetDemo&quot;</span>,  </span><br><span class="line">    text_key=<span class="string">&quot;text&quot;</span>,  </span><br><span class="line">    embedding=embedding,  </span><br><span class="line">)  </span><br><span class="line">rerank = CohereRerank(model=<span class="string">&quot;rerank-multil ingual-v3.0&quot;</span>)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 2.构建压缩检索器  </span></span><br><span class="line">retriever = ContextualCompressionRetriever(  </span><br><span class="line">    base_retriever=db.as_retriever(search_type=<span class="string">&quot;mmr&quot;</span>),  </span><br><span class="line">    base_compressor=rerank,  </span><br><span class="line">)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 3.执行搜索并排序  </span></span><br><span class="line">search_docs = retriever.invoke(<span class="string">&quot;关于LLMOps应用配置的信息有哪些呢？&quot;</span>)  </span><br><span class="line"><span class="built_in">print</span>(search_docs)  </span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(search_docs))</span><br></pre></td></tr></table></figure>
<p><strong>输出</strong>：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[Document(metadata=&#123;<span class="string">&#x27;source&#x27;</span>: <span class="string">&#x27;./项目API文档.md&#x27;</span>, <span class="string">&#x27;start_index&#x27;</span>: 2324.0, <span class="string">&#x27;relevance_score&#x27;</span>: 0.9298237&#125;, page_content=<span class="string">&#x27;json &#123; &quot;code&quot;: &quot;success&quot;, &quot;data&quot;: &#123; &quot;id&quot;: &quot;5e7834dc-bbca-4ee5-9591-8f297f5acded&quot;, &quot;name&quot;: &quot;慕课LLMOps聊天机器人&quot;, &quot;icon&quot;: &quot;https://imooc-llmops-1257184990.cos.ap-guangzhou.myqcloud.com/2024/04/23/e4422149-4cf7-41b3-ad55-ca8d2caa8f13.png&quot;, &quot;description&quot;: &quot;这是一个慕课LLMOps的Agent应用&quot;, &quot;published_app_config_id&quot;: null, &quot;drafted_app_config_id&quot;: null, &quot;debug_conversation_id&quot;: &quot;1550b71a-1444-47ed-a59d-c2f080fbae94&quot;, &quot;published_app_config&quot;: null, &quot;drafted_app_config&quot;: &#123; &quot;id&quot;: &quot;755dc464-67cd-42ef-9c56-b7528b44e7c8&quot;&#x27;</span>), Document(metadata=&#123;<span class="string">&#x27;source&#x27;</span>: <span class="string">&#x27;./项目API文档.md&#x27;</span>, <span class="string">&#x27;start_index&#x27;</span>: 0.0, <span class="string">&#x27;relevance_score&#x27;</span>: 0.7358315&#125;, page_content=<span class="string">&#x27;LLMOps 项目 API 文档\n\n应用 API 接口统一以 JSON 格式返回，并且包含 3 个字段：code、data 和 message，分别代表业务状态码、业务数据和接口附加信息。\n\n业务状态码共有 6 种，其中只有 success(成功) 代表业务操作成功，其他 5 种状态均代表失败，并且失败时会附加相关的信息：fail(通用失败)、not_found(未找到)、unauthorized(未授权)、forbidden(无权限)和validate_error(数据验证失败)。\n\n接口示例：\n\njson &#123; &quot;code&quot;: &quot;success&quot;, &quot;data&quot;: &#123; &quot;redirect_url&quot;: &quot;https://github.com/login/oauth/authorize?client_id=f69102c6b97d90d69768&amp;redirect_uri=http%3A%2F%2Flocalhost%3A5001%2Foauth%2Fauthorize%2Fgithub&amp;scope=user%3Aemail&quot; &#125;, &quot;message&quot;: &quot;&quot; &#125;&#x27;</span>), Document(metadata=&#123;<span class="string">&#x27;source&#x27;</span>: <span class="string">&#x27;./项目API文档.md&#x27;</span>, <span class="string">&#x27;start_index&#x27;</span>: 675.0, <span class="string">&#x27;relevance_score&#x27;</span>: 0.098772585&#125;, page_content=<span class="string">&#x27;json &#123; &quot;code&quot;: &quot;success&quot;, &quot;data&quot;: &#123; &quot;list&quot;: [ &#123; &quot;app_count&quot;: 0, &quot;created_at&quot;: 1713105994, &quot;description&quot;: &quot;这是专门用来存储慕课LLMOps课程信息的知识库&quot;, &quot;document_count&quot;: 13, &quot;icon&quot;: &quot;https://imooc-llmops-1257184990.cos.ap-guangzhou.myqcloud.com/2024/04/07/96b5e270-c54a-4424-aece-ff8a2b7e4331.png&quot;, &quot;id&quot;: &quot;c0759ca8-2d35-4480-83a8-1f41f29d1401&quot;, &quot;name&quot;: &quot;慕课LLMOps课程知识库&quot;, &quot;updated_at&quot;: 1713106758, &quot;word_count&quot;: 8850 &#125; ], &quot;paginator&quot;: &#123; &quot;current_page&quot;: 1, &quot;page_size&quot;: 20, &quot;total_page&quot;: 1, &quot;total_record&quot;: 2 &#125; &#125;&#x27;</span>)]</span><br><span class="line"></span><br><span class="line">3</span><br></pre></td></tr></table></figure>
<p>通过 LangSmith 观察该检索器的运行流程，可以发现，原始检索器找到了 4 条数据，但是经过重排序后，只返回了相关性高的 3 条（可设置），有一条被剔除了，这也是 压缩器 和 文档转换器 的区别（不过在旧版本的 LangChain 中，两个没有区别，所以在很多文档转换器的文档中都可以看到压缩器的存在）。</p>
<h2 id="6-15-纠正性索引增强生成-CRAG-优化策略"><a href="#6-15-纠正性索引增强生成-CRAG-优化策略" class="headerlink" title="6.15 纠正性索引增强生成 CRAG 优化策略"></a>6.15 纠正性索引增强生成 CRAG 优化策略</h2><h3 id="6-15-1-纠正性检索增强"><a href="#6-15-1-纠正性检索增强" class="headerlink" title="6.15.1 纠正性检索增强"></a>6.15.1 纠正性检索增强</h3><p>纠正性检索增强生成（Corrective Retrieval-Augmented Generation，CRAG）是一种先进的自然语言处理技术，旨在提高检索的生成方法的鲁棒性和准确性。在 CRAG 中引入了一个轻量级的检索评估器来评估检索到的文档的质量，并根据评估结果触发不同的知识检索动作，以确保生成结果的准确性。<br>CRAG 的工作流程包含了以下几个步骤：</p>
<p>1. <strong>检索文档</strong>：首先，基于用户的查询，系统执行检索操作以获取相关的文档或信息。<br>2. <strong>评估检索质量</strong>：CRAG 使用一个轻量级的检索评估器对检索到的每个文档进行质量评估，计算出一个量化的置信度分数。<br>3. <strong>触发知识检索动作</strong>：根据置信度分数，CRAG 将触发以下二个动作之一：<br>· <strong>正确</strong>：如果评估器认为文档与查询高度相关，将采用该文档进行知识精炼。<br>· <strong>错误</strong>：如果文档被评估为不相关或误导性，CRAG将利用网络搜索寻找更多知识来源。<br>4. <strong>知识精炼</strong>：对于评估为正确的文档，CRAG将进行知识精炼，抽取关键信息并过滤掉无关信息。<br>5. <strong>网络搜索</strong>：在需要时，CRAG会执行网络搜索以寻找更多高质量的知识来源，以纠正或补充检索结果。<br>6. <strong>分解-重组</strong>：CRAG采用一种分解-重组算法，将检索到的文档解构为关键信息块，筛选重要信息，并重新组织成结构化知识。<br>7. <strong>生成文本</strong>：最后，利用经过优化和校正的知识，传递给 LLM，生成对应文本。</p>
<p>运行流程如下：<br><img src="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2025/20250506180803.png" alt="image.png"></p>
<blockquote>
<p>需要借助 LangGraph 实现循环处理</p>
</blockquote>
<h2 id="6-16-使用-Self-RAG-纠正低质量的检索生成"><a href="#6-16-使用-Self-RAG-纠正低质量的检索生成" class="headerlink" title="6.16 使用 Self-RAG 纠正低质量的检索生成"></a>6.16 使用 Self-RAG 纠正低质量的检索生成</h2><p>这里我们从一个常见的生活场景入手——参加开卷考试，一般来说我们通常会采用以下两种作答策略：</p>
<p>· 方法一：对于熟悉的题目，直接快速作答；对于不熟悉的题目，快速翻阅参考书，找到相关部分，在脑海中整理分类和总结后，再在试卷上作答。<br>· 方法二：每一个题目都需要参考书本进行解答。先找到相关部分，在脑海中进行整合和总结后，再到试卷上书写答案。</p>
<p>显然，方法一 大家用的更多一些，是首选方法。方法二不仅耗时，还有可能引入无关的或错误的信息，导致出现混淆和错误，甚至在考生原本擅长的领域也不例外。<br>映射到 RAG 应用开发中，很容易可以发现，方法二 是典型的 RAG，即（检索-&gt;整合-&gt;生成）流程，而方法一就是 Self-RAG 的流程。<br>Self-RAG 全称为自我反思 RAG，见名知其意，即对原始查询、检索的内容、生成的内容进行自我反思，根据反思的结果执行不同的操作，例如：<strong>直接输出答案、重新检索、剔除不相关的内容、检测生成内容是否存在幻觉、检测生成内容是否有帮助</strong>等，可以把 Self-RAG 看成是一个拥有自我反思能力的智能体，这个智能体主要用来依据相关知识库回复用户问题，自我迭代，直到输出满意的结果。</p>
<p>一个 Self-RAG 应用主要有三大步骤组成：<br>1. 按需检索（Retrieval as Needed）：使用需要检索时，例如查询“慕小课是谁时？”，模型会输出一个 检索query，表示需要检索与 query 相关的内容；相反，当模型被要求写“写一篇关于Python依赖注入的文章”时，大模型会直接生成答案，无需进行检索。<br>2. 以并行方式生成内容（Parallel Generation）：模型会同时使用 prompt 和检索到的内容来生成模型输出，在整个过程中，会触发多种类型的反思（Reflection），涵盖了：反思文档是否有关联、反思生成内容是否存在幻觉，如果不关联则重新检索，如果存在幻觉&#x2F;支持度不够，则重新生成。<br>3. 内容的评估和选择：对步骤 2 中生成的内容进行评估，并选择最佳文档段落作为输出。</p>
<p>拆分成流程图后，Self-RAG 的运行流程如下：<br><img src="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2025/20250506181148.png" alt="image.png"><br>在 Self-RAG 架构中，用到了 3 处循环结构，对于该架构的 AI 应用，单纯使用 LCEL 表达式构建的链应用也没法或者很难实现整个过程，同样必须使用到 LangGraph 来构建 循环图，所以关于 Self-RAG 整个流程的构建，依旧会等到我们学习完 LangGraph 再来尝试。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="https://baihlup.github.io">梦之痕</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="https://baihlup.github.io/2025/04/29/250%20-%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD&amp;%E5%A4%A7%E6%95%B0%E6%8D%AE/257%20-%20AI%20%E5%A4%A7%E6%A8%A1%E5%9E%8B/09%20-%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%20RAG%20%E7%9A%84%E5%BA%94%E7%94%A8%E5%92%8C%E5%BC%80%E5%8F%91/">https://baihlup.github.io/2025/04/29/250%20-%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD&amp;%E5%A4%A7%E6%95%B0%E6%8D%AE/257%20-%20AI%20%E5%A4%A7%E6%A8%A1%E5%9E%8B/09%20-%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%20RAG%20%E7%9A%84%E5%BA%94%E7%94%A8%E5%92%8C%E5%BC%80%E5%8F%91/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/">大模型</a><a class="post-meta__tags" href="/tags/RAG/">RAG</a></div><div class="post_share"><div class="social-share" data-image="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/41710043961_.pic.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2025/02/12/260%20-%20%E5%90%8E%E7%AB%AF&amp;%E6%9E%B6%E6%9E%84/263%20-%20%E7%B3%BB%E7%BB%9F%E6%9C%8D%E5%8A%A1/06%20-%20WASM%20%E6%8F%92%E4%BB%B6%E5%BC%80%E5%8F%91/" title="06 - WASM 插件开发"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">Next</div><div class="next_info">06 - WASM 插件开发</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/41710043961_.pic.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">梦之痕</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">62</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">46</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">15</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/BaihlUp"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">个人笔记迁移中ing....</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-number">1.</span> <span class="toc-text">1 向量数据库</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1-%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%80%E4%BB%8B"><span class="toc-number">1.1.</span> <span class="toc-text">1.1 向量数据库简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-2-%E4%BC%A0%E7%BB%9F%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%8E%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E5%B7%AE%E5%BC%82"><span class="toc-number">1.2.</span> <span class="toc-text">1.2 传统数据库与向量数据库的差异</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-3-%E4%BC%A0%E7%BB%9F%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%8E%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BC%98%E7%BC%BA%E7%82%B9"><span class="toc-number">1.3.</span> <span class="toc-text">1.3 传统数据库与向量数据库优缺点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-4-%E7%9B%B8%E4%BC%BC%E5%BA%A6%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95"><span class="toc-number">1.4.</span> <span class="toc-text">1.4 相似度搜索算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-1-%E4%BD%99%E5%BC%A6%E7%9B%B8%E4%BC%BC%E5%BA%A6%E4%B8%8E%E6%AC%A7%E6%B0%8F%E8%B7%9D%E7%A6%BB"><span class="toc-number">1.4.1.</span> <span class="toc-text">1.4.1 余弦相似度与欧氏距离</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-2-%E7%9B%B8%E4%BC%BC%E6%80%A7%E6%90%9C%E7%B4%A2%E5%8A%A0%E9%80%9F%E7%AE%97%E6%B3%95"><span class="toc-number">1.4.2.</span> <span class="toc-text">1.4.2 相似性搜索加速算法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-5-%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E9%85%8D%E7%BD%AE%E5%92%8C%E4%BD%BF%E7%94%A8"><span class="toc-number">1.5.</span> <span class="toc-text">1.5 向量数据库的配置和使用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-5-1-Faiss-%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-number">1.5.1.</span> <span class="toc-text">1.5.1 Faiss 向量数据库</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-5-1-1-Faiss-%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8"><span class="toc-number">1.5.1.1.</span> <span class="toc-text">1.5.1.1 Faiss 基本使用</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-5-1-2-%E5%88%A0%E9%99%A4%E6%8C%87%E5%AE%9A%E6%95%B0%E6%8D%AE"><span class="toc-number">1.5.1.2.</span> <span class="toc-text">1.5.1.2 删除指定数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-5-1-3-%E5%B8%A6%E8%BF%87%E6%BB%A4%E7%9A%84%E7%9B%B8%E4%BC%BC%E6%80%A7%E6%90%9C%E7%B4%A2"><span class="toc-number">1.5.1.3.</span> <span class="toc-text">1.5.1.3 带过滤的相似性搜索</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-5-2-Pinecone-%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-number">1.5.2.</span> <span class="toc-text">1.5.2 Pinecone 向量数据库</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-5-2-1-Pinecone-%E9%85%8D%E7%BD%AE"><span class="toc-number">1.5.2.1.</span> <span class="toc-text">1.5.2.1 Pinecone 配置</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-5-2-2-Pinecone-%E4%BD%BF%E7%94%A8"><span class="toc-number">1.5.2.2.</span> <span class="toc-text">1.5.2.2 Pinecone 使用</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-5-3-Weaviate-%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-number">1.5.3.</span> <span class="toc-text">1.5.3 Weaviate 向量数据库</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-5-3-1-Weaviate-%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.5.3.1.</span> <span class="toc-text">1.5.3.1 Weaviate 介绍</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-5-3-2-Weaviate-%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="toc-number">1.5.3.2.</span> <span class="toc-text">1.5.3.2 Weaviate 向量数据库的使用</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-5-4-%E8%87%AA%E5%AE%9A%E4%B9%89%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-number">1.5.4.</span> <span class="toc-text">1.5.4 自定义向量数据库</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D%E5%92%8C%E4%BD%BF%E7%94%A8"><span class="toc-number">2.</span> <span class="toc-text">2 嵌入模型介绍和使用</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D"><span class="toc-number">2.1.</span> <span class="toc-text">2.1 嵌入模型介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-Embedding-%E7%9A%84%E4%BB%B7%E5%80%BC"><span class="toc-number">2.2.</span> <span class="toc-text">2.3 Embedding 的价值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-4-CacheBackEmbedding-%E7%BB%84%E4%BB%B6"><span class="toc-number">2.3.</span> <span class="toc-text">2.4 CacheBackEmbedding 组件</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-5-HuggingFace-Embedding-%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%85%8D%E7%BD%AE%E5%92%8C%E4%BD%BF%E7%94%A8"><span class="toc-number">2.4.</span> <span class="toc-text">2.5 HuggingFace Embedding 模型的配置和使用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-5-1-HuggingFace-%E6%9C%AC%E5%9C%B0%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.4.1.</span> <span class="toc-text">2.5.1 HuggingFace 本地模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-5-2-HuggingFace%E8%BF%9C%E7%A8%8B%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.4.2.</span> <span class="toc-text">2.5.2 HuggingFace远程嵌入模型</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-%E6%96%87%E6%A1%A3%E5%8A%A0%E8%BD%BD%E5%99%A8"><span class="toc-number">3.</span> <span class="toc-text">3 文档加载器</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-Document-%E4%B8%8E%E6%96%87%E6%A1%A3%E5%8A%A0%E8%BD%BD%E5%99%A8"><span class="toc-number">3.1.</span> <span class="toc-text">3.1 Document 与文档加载器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-%E5%86%85%E7%BD%AE%E6%96%87%E6%A1%A3%E5%8A%A0%E8%BD%BD%E5%99%A8%E7%9A%84%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7"><span class="toc-number">3.2.</span> <span class="toc-text">3.2 内置文档加载器的使用技巧</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-1-Markdown-%E6%96%87%E6%A1%A3%E5%8A%A0%E8%BD%BD%E5%99%A8"><span class="toc-number">3.2.1.</span> <span class="toc-text">3.2.1 Markdown 文档加载器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-2-Office-%E6%96%87%E6%A1%A3%E5%8A%A0%E8%BD%BD%E5%99%A8"><span class="toc-number">3.2.2.</span> <span class="toc-text">3.2.2 Office 文档加载器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-3-%E9%80%9A%E7%94%A8%E6%96%87%E6%A1%A3%E5%8A%A0%E8%BD%BD%E5%99%A8"><span class="toc-number">3.2.3.</span> <span class="toc-text">3.2.3 通用文档加载器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-4-%E8%87%AA%E5%AE%9A%E4%B9%89%E6%96%87%E6%A1%A3%E5%8A%A0%E8%BD%BD%E5%99%A8"><span class="toc-number">3.2.4.</span> <span class="toc-text">3.2.4 自定义文档加载器</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-%E6%96%87%E6%9C%AC%E5%88%86%E5%89%B2%E5%99%A8"><span class="toc-number">4.</span> <span class="toc-text">4 文本分割器</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1-DocumentTransformer-%E7%BB%84%E4%BB%B6"><span class="toc-number">4.1.</span> <span class="toc-text">4.1 DocumentTransformer 组件</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2-%E5%AD%97%E7%AC%A6%E5%88%86%E5%89%B2%E5%99%A8"><span class="toc-number">4.2.</span> <span class="toc-text">4.2 字符分割器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-3-%E9%80%92%E5%BD%92%E5%AD%97%E7%AC%A6%E6%96%87%E6%9C%AC%E5%88%86%E5%89%B2%E5%99%A8"><span class="toc-number">4.3.</span> <span class="toc-text">4.3 递归字符文本分割器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-4-%E8%AF%AD%E4%B9%89%E6%96%87%E6%A1%A3%E5%88%86%E5%89%B2%E5%99%A8"><span class="toc-number">4.4.</span> <span class="toc-text">4.4 语义文档分割器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-5-%E8%87%AA%E5%AE%9A%E4%B9%89%E6%96%87%E6%A1%A3%E5%88%86%E5%89%B2%E5%99%A8"><span class="toc-number">4.5.</span> <span class="toc-text">4.5 自定义文档分割器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-6-%E9%9D%9E%E5%88%86%E5%89%B2%E7%B1%BB%E5%9E%8B%E7%9A%84%E6%96%87%E6%A1%A3%E5%88%86%E5%89%B2%E5%99%A8"><span class="toc-number">4.6.</span> <span class="toc-text">4.6 非分割类型的文档分割器</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6-1-%E9%97%AE%E7%AD%94%E8%BD%AC%E6%8D%A2%E5%99%A8"><span class="toc-number">4.6.1.</span> <span class="toc-text">4.6.1 问答转换器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6-2-%E7%BF%BB%E8%AF%91%E8%BD%AC%E6%8D%A2%E5%99%A8"><span class="toc-number">4.6.2.</span> <span class="toc-text">4.6.2 翻译转换器</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5-%E6%96%87%E6%A1%A3%E6%A3%80%E7%B4%A2%E5%99%A8"><span class="toc-number">5.</span> <span class="toc-text">5 文档检索器</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#5-1-%E5%B8%A6%E5%BE%97%E5%88%86%E9%98%88%E5%80%BC%E7%9A%84%E7%9B%B8%E4%BC%BC%E6%80%A7%E6%90%9C%E7%B4%A2"><span class="toc-number">5.1.</span> <span class="toc-text">5.1 带得分阈值的相似性搜索</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-2-as-retriever-%E6%A3%80%E7%B4%A2%E5%99%A8"><span class="toc-number">5.2.</span> <span class="toc-text">5.2 as_retriever() 检索器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-3-MMR-%E6%9C%80%E5%A4%A7%E8%BE%B9%E9%99%85%E7%9B%B8%E5%85%B3%E6%80%A7"><span class="toc-number">5.3.</span> <span class="toc-text">5.3 MMR 最大边际相关性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-4-%E6%A3%80%E7%B4%A2%E5%99%A8%E7%BB%84%E4%BB%B6"><span class="toc-number">5.4.</span> <span class="toc-text">5.4 检索器组件</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-5-%E8%87%AA%E5%AE%9A%E4%B9%89%E6%A3%80%E7%B4%A2%E5%99%A8"><span class="toc-number">5.5.</span> <span class="toc-text">5.5 自定义检索器</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6-RAG-%E4%BC%98%E5%8C%96%E7%AD%96%E7%95%A5"><span class="toc-number">6.</span> <span class="toc-text">6 RAG 优化策略</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-RAG-%E5%BC%80%E5%8F%916%E4%B8%AA%E9%98%B6%E6%AE%B5%E4%BC%98%E5%8C%96%E7%AD%96%E7%95%A5"><span class="toc-number">6.0.1.</span> <span class="toc-text">6.1 RAG 开发6个阶段优化策略</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-2-%E5%A4%9A%E6%9F%A5%E8%AF%A2%E9%87%8D%E5%86%99%E7%AD%96%E7%95%A5"><span class="toc-number">6.1.</span> <span class="toc-text">6.2 多查询重写策略</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-1-Muliti-Query-%E5%A4%9A%E6%9F%A5%E8%AF%A2%E7%AD%96%E7%95%A5"><span class="toc-number">6.1.1.</span> <span class="toc-text">6.2.1 Muliti-Query 多查询策略</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-2-%E6%A0%B8%E5%BF%83%E6%89%A7%E8%A1%8C%E9%80%BB%E8%BE%91"><span class="toc-number">6.1.2.</span> <span class="toc-text">6.2.2 核心执行逻辑</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-3-%E5%A4%9A%E6%9F%A5%E8%AF%A2%E7%BB%93%E6%9E%9C%E8%9E%8D%E5%90%88"><span class="toc-number">6.2.</span> <span class="toc-text">6.3 多查询结果融合</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-3-1-%E5%A4%9A%E6%9F%A5%E8%AF%A2%E7%BB%93%E6%9E%9C%E8%9E%8D%E5%90%88%E7%AD%96%E7%95%A5%E5%8F%8ARRF"><span class="toc-number">6.2.1.</span> <span class="toc-text">6.3.1 多查询结果融合策略及RRF</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-3-2-%E5%A4%9A%E6%9F%A5%E8%AF%A2%E7%BB%93%E6%9E%9C%E8%9E%8D%E5%90%88%E7%AD%96%E7%95%A5%E5%AE%9E%E7%8E%B0"><span class="toc-number">6.2.2.</span> <span class="toc-text">6.3.2 多查询结果融合策略实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-4-%E9%97%AE%E9%A2%98%E5%88%86%E8%A7%A3%E7%AD%96%E7%95%A5%E6%8F%90%E5%8D%87%E5%A4%8D%E6%9D%82%E9%97%AE%E9%A2%98%E6%A3%80%E7%B4%A2%E6%AD%A3%E7%A1%AE%E7%8E%87"><span class="toc-number">6.3.</span> <span class="toc-text">6.4 问题分解策略提升复杂问题检索正确率</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-4-1-%E5%A4%8D%E6%9D%82%E9%97%AE%E9%A2%98%E6%A3%80%E7%B4%A2%E7%9A%84%E9%9A%BE%E7%82%B9%E4%B8%8E%E5%88%86%E8%A7%A3"><span class="toc-number">6.3.1.</span> <span class="toc-text">6.4.1 复杂问题检索的难点与分解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-4-2-%E8%BF%AD%E4%BB%A3%E5%BC%8F%E5%9B%9E%E7%AD%94%E5%AE%9E%E7%8E%B0"><span class="toc-number">6.3.2.</span> <span class="toc-text">6.4.2 迭代式回答实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-5-Step-Back-%E5%9B%9E%E7%AD%94%E5%9B%9E%E9%80%80%E7%AD%96%E7%95%A5"><span class="toc-number">6.4.</span> <span class="toc-text">6.5 Step-Back 回答回退策略</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-5-1-%E5%B0%91%E9%87%8F%E7%A4%BA%E4%BE%8B%E6%A8%A1%E6%9D%BF"><span class="toc-number">6.4.1.</span> <span class="toc-text">6.5.1 少量示例模板</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-5-2-Step-Back-%E5%9B%9E%E7%AD%94%E5%9B%9E%E9%80%80%E7%AD%96%E7%95%A5"><span class="toc-number">6.4.2.</span> <span class="toc-text">6.5.2 Step-Back 回答回退策略</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-6-%E6%B7%B7%E5%90%88%E7%AD%96%E7%95%A5%E5%AE%9E%E7%8E%B0-doc-doc-%E5%AF%B9%E7%A7%B0%E6%A3%80%E7%B4%A2"><span class="toc-number">6.5.</span> <span class="toc-text">6.6 混合策略实现 doc-doc 对称检索</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-6-1-HyDE-%E6%B7%B7%E5%90%88%E7%AD%96%E7%95%A5"><span class="toc-number">6.5.1.</span> <span class="toc-text">6.6.1 HyDE 混合策略</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-6-2-%E5%B1%80%E9%99%90%E6%80%A7"><span class="toc-number">6.5.2.</span> <span class="toc-text">6.6.2 局限性</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-7-%E9%9B%86%E6%88%90%E5%A4%9A%E7%A7%8D%E6%A3%80%E7%B4%A2%E5%99%A8%E7%AE%97%E6%B3%95%E7%9A%84%E6%B7%B7%E5%90%88%E6%A3%80%E7%B4%A2"><span class="toc-number">6.6.</span> <span class="toc-text">6.7 集成多种检索器算法的混合检索</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-7-1-%E9%9B%86%E6%88%90%E6%A3%80%E7%B4%A2%E5%99%A8%E7%9A%84%E4%BC%98%E5%8A%BF%E4%B8%8E%E4%BD%BF%E7%94%A8"><span class="toc-number">6.6.1.</span> <span class="toc-text">6.7.1 集成检索器的优势与使用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-7-2-%E6%9F%A5%E8%AF%A2%E8%BD%AC%E6%8D%A2%E9%98%B6%E6%AE%B5%E4%BC%98%E5%8C%96%E7%AD%96%E7%95%A5%E6%80%BB%E7%BB%93"><span class="toc-number">6.6.2.</span> <span class="toc-text">6.7.2 查询转换阶段优化策略总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-8-%E6%A3%80%E7%B4%A2%E5%99%A8%E7%9A%84%E9%80%BB%E8%BE%91%E8%B7%AF%E7%94%B1%E7%BC%A9%E5%87%8F%E6%A3%80%E7%B4%A2%E8%8C%83%E5%9B%B4-%E2%80%93-%E9%80%BB%E8%BE%91%E8%B7%AF%E7%94%B1%E9%98%B6%E6%AE%B5"><span class="toc-number">6.7.</span> <span class="toc-text">6.8 检索器的逻辑路由缩减检索范围 – 逻辑路由阶段</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-9-%E8%AF%AD%E4%B9%89%E8%B7%AF%E7%94%B1%E9%80%89%E6%8B%A9%E4%B8%8D%E5%90%8C%E7%9A%84-Prompt-%E6%A8%A1%E6%9D%BF-%E2%80%93-%E9%80%BB%E8%BE%91%E8%B7%AF%E7%94%B1%E9%98%B6%E6%AE%B5"><span class="toc-number">6.8.</span> <span class="toc-text">6.9 语义路由选择不同的 Prompt 模板 – 逻辑路由阶段</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-10-%E8%87%AA%E6%9F%A5%E8%AF%A2%E6%A3%80%E7%B4%A2%E5%99%A8%E5%AE%9E%E7%8E%B0%E5%8A%A8%E6%80%81%E5%85%83%E6%95%B0%E6%8D%AE%E8%BF%87%E6%BB%A4-%E2%80%93-%E6%9F%A5%E8%AF%A2%E6%9E%84%E5%BB%BA%E9%98%B6%E6%AE%B5"><span class="toc-number">6.9.</span> <span class="toc-text">6.10 自查询检索器实现动态元数据过滤 – 查询构建阶段</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-10-1-%E8%87%AA%E6%9F%A5%E8%AF%A2%E6%A3%80%E7%B4%A2%E5%99%A8%E5%AE%9E%E7%8E%B0"><span class="toc-number">6.9.1.</span> <span class="toc-text">6.10.1 自查询检索器实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-10-2-%E8%87%AA%E6%9F%A5%E8%AF%A2%E6%A3%80%E7%B4%A2%E5%99%A8%E6%89%A7%E8%A1%8C%E9%80%BB%E8%BE%91"><span class="toc-number">6.9.2.</span> <span class="toc-text">6.10.2 自查询检索器执行逻辑</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-11-MultiVector-%E5%AE%9E%E7%8E%B0%E5%A4%9A%E5%90%91%E9%87%8F%E6%A3%80%E7%B4%A2%E6%96%87%E6%A1%A3-%E2%80%93-%E7%B4%A2%E5%BC%95%E9%98%B6%E6%AE%B5"><span class="toc-number">6.10.</span> <span class="toc-text">6.11 MultiVector 实现多向量检索文档 – 索引阶段</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-11-1-%E5%A4%9A%E8%A1%A8%E5%BE%81-%E5%90%91%E9%87%8F%E7%B4%A2%E5%BC%95"><span class="toc-number">6.10.1.</span> <span class="toc-text">6.11.1 多表征&#x2F;向量索引</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-11-2-%E5%A4%9A%E5%90%91%E9%87%8F%E7%B4%A2%E5%BC%95%E7%A4%BA%E4%BE%8B"><span class="toc-number">6.10.2.</span> <span class="toc-text">6.11.2 多向量索引示例</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-11-3-%E5%81%87%E8%AE%BE%E6%80%A7%E6%9F%A5%E8%AF%A2%E6%A3%80%E7%B4%A2%E5%8E%9F%E6%96%87%E6%A1%A3"><span class="toc-number">6.10.3.</span> <span class="toc-text">6.11.3 假设性查询检索原文档</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-12-%E7%88%B6%E6%96%87%E6%A1%A3%E6%A3%80%E7%B4%A2%E5%99%A8%E5%AE%9E%E7%8E%B0%E6%8B%86%E5%88%86%E5%92%8C%E5%AD%98%E5%82%A8%E5%B9%B3%E8%A1%A1"><span class="toc-number">6.11.</span> <span class="toc-text">6.12 父文档检索器实现拆分和存储平衡</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-13-%E9%80%92%E5%BD%92%E6%96%87%E6%A1%A3%E6%A0%91%E6%A3%80%E7%B4%A2%E9%AB%98%E7%BA%A7RAG%E4%BC%98%E5%8C%96"><span class="toc-number">6.12.</span> <span class="toc-text">6.13 递归文档树检索高级RAG优化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-14-ReRank-%E9%87%8D%E6%8E%92%E5%BA%8F%E6%8F%90%E5%8D%87RAG%E7%B3%BB%E7%BB%9F"><span class="toc-number">6.13.</span> <span class="toc-text">6.14 ReRank 重排序提升RAG系统</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-14-1-ReRank-%E9%87%8D%E6%8E%92%E5%BA%8F"><span class="toc-number">6.13.1.</span> <span class="toc-text">6.14.1 ReRank 重排序</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-14-2-Cohere-%E9%87%8D%E6%8E%92%E5%BA%8F"><span class="toc-number">6.13.2.</span> <span class="toc-text">6.14.2 Cohere 重排序</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-15-%E7%BA%A0%E6%AD%A3%E6%80%A7%E7%B4%A2%E5%BC%95%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90-CRAG-%E4%BC%98%E5%8C%96%E7%AD%96%E7%95%A5"><span class="toc-number">6.14.</span> <span class="toc-text">6.15 纠正性索引增强生成 CRAG 优化策略</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-15-1-%E7%BA%A0%E6%AD%A3%E6%80%A7%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA"><span class="toc-number">6.14.1.</span> <span class="toc-text">6.15.1 纠正性检索增强</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-16-%E4%BD%BF%E7%94%A8-Self-RAG-%E7%BA%A0%E6%AD%A3%E4%BD%8E%E8%B4%A8%E9%87%8F%E7%9A%84%E6%A3%80%E7%B4%A2%E7%94%9F%E6%88%90"><span class="toc-number">6.15.</span> <span class="toc-text">6.16 使用 Self-RAG 纠正低质量的检索生成</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/04/29/250%20-%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD&amp;%E5%A4%A7%E6%95%B0%E6%8D%AE/257%20-%20AI%20%E5%A4%A7%E6%A8%A1%E5%9E%8B/09%20-%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%20RAG%20%E7%9A%84%E5%BA%94%E7%94%A8%E5%92%8C%E5%BC%80%E5%8F%91/" title="大模型 RAG 的应用和开发">大模型 RAG 的应用和开发</a><time datetime="2025-04-29T00:00:00.000Z" title="Created 2025-04-29 00:00:00">2025-04-29</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/02/12/260%20-%20%E5%90%8E%E7%AB%AF&amp;%E6%9E%B6%E6%9E%84/263%20-%20%E7%B3%BB%E7%BB%9F%E6%9C%8D%E5%8A%A1/06%20-%20WASM%20%E6%8F%92%E4%BB%B6%E5%BC%80%E5%8F%91/" title="06 - WASM 插件开发">06 - WASM 插件开发</a><time datetime="2025-02-12T00:00:00.000Z" title="Created 2025-02-12 00:00:00">2025-02-12</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/09/28/270%20-%20%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/279%20-%20%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/02%20-%20%E8%AE%B0%E5%BD%95%E8%AE%BF%E9%97%AE%20HTTPS%20%E7%BD%91%E7%AB%99%E6%8A%A5%E9%94%99%E9%97%AE%E9%A2%98/" title="记录访问 HTTPS 网站报错问题">记录访问 HTTPS 网站报错问题</a><time datetime="2024-09-28T00:00:00.000Z" title="Created 2024-09-28 00:00:00">2024-09-28</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/08/29/270%20-%20%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/279%20-%20%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/06%20-%20%E7%BD%91%E7%BB%9C%E7%9F%A5%E8%AF%86%E4%B8%B2%E8%AE%B2%EF%BC%9A%E7%94%A8%E5%8F%8C%E5%8D%81%E4%B8%80%E7%9A%84%E6%95%85%E4%BA%8B%E4%B8%B2%E8%B5%B7%E7%A2%8E%E7%89%87%E7%9A%84%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE%EF%BC%88%E4%B8%AD%EF%BC%89/" title="Untitled">Untitled</a><time datetime="2024-08-29T08:17:15.548Z" title="Created 2024-08-29 08:17:15">2024-08-29</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/08/29/270%20-%20%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/279%20-%20%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/05%20-%20%E7%BD%91%E7%BB%9C%E7%9F%A5%E8%AF%86%E4%B8%B2%E8%AE%B2%EF%BC%9A%E7%94%A8%E5%8F%8C%E5%8D%81%E4%B8%80%E7%9A%84%E6%95%85%E4%BA%8B%E4%B8%B2%E8%B5%B7%E7%A2%8E%E7%89%87%E7%9A%84%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE%EF%BC%88%E4%B8%8A%EF%BC%89/" title="Untitled">Untitled</a><time datetime="2024-08-29T08:17:15.544Z" title="Created 2024-08-29 08:17:15">2024-08-29</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By 梦之痕</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>