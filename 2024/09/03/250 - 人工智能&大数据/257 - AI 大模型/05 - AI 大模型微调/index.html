<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>AI 大模型微调 | 梦之痕</title><meta name="author" content="梦之痕"><meta name="copyright" content="梦之痕"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="0 参考资料GeekAGI 知识库： https:&#x2F;&#x2F;geek-agi.feishu.cn&#x2F;wiki&#x2F;B9rYwwg6xidZYJkbrlscxTQFnOc项目地址：https:&#x2F;&#x2F;github.com&#x2F;DjangoPeng&#x2F;LLM-quickstart 0.1 搭建开发环境本项目对于硬件有一定要求：GPU 显存不小于16GB，支持最低配置显卡型号为 NVIDIA Tesla T4。建议使用 GP">
<meta property="og:type" content="article">
<meta property="og:title" content="AI 大模型微调">
<meta property="og:url" content="https://baihlup.github.io/2024/09/03/250%20-%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD&%E5%A4%A7%E6%95%B0%E6%8D%AE/257%20-%20AI%20%E5%A4%A7%E6%A8%A1%E5%9E%8B/05%20-%20AI%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83/index.html">
<meta property="og:site_name" content="梦之痕">
<meta property="og:description" content="0 参考资料GeekAGI 知识库： https:&#x2F;&#x2F;geek-agi.feishu.cn&#x2F;wiki&#x2F;B9rYwwg6xidZYJkbrlscxTQFnOc项目地址：https:&#x2F;&#x2F;github.com&#x2F;DjangoPeng&#x2F;LLM-quickstart 0.1 搭建开发环境本项目对于硬件有一定要求：GPU 显存不小于16GB，支持最低配置显卡型号为 NVIDIA Tesla T4。建议使用 GP">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/41710043961_.pic.jpg">
<meta property="article:published_time" content="2024-09-03T00:00:00.000Z">
<meta property="article:modified_time" content="2025-12-04T08:14:07.098Z">
<meta property="article:author" content="梦之痕">
<meta property="article:tag" content="大模型微调">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/41710043961_.pic.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://baihlup.github.io/2024/09/03/250%20-%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD&amp;%E5%A4%A7%E6%95%B0%E6%8D%AE/257%20-%20AI%20%E5%A4%A7%E6%A8%A1%E5%9E%8B/05%20-%20AI%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Error',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'AI 大模型微调',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-12-04 08:14:07'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/41710043961_.pic.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">65</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">48</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">17</div></a></div><hr class="custom-hr"/></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="梦之痕"><span class="site-name">梦之痕</span></a></span><div id="menus"><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">AI 大模型微调</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2024-09-03T00:00:00.000Z" title="Created 2024-09-03 00:00:00">2024-09-03</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-12-04T08:14:07.098Z" title="Updated 2025-12-04 08:14:07">2025-12-04</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI%E5%A4%A7%E6%A8%A1%E5%9E%8B/">AI大模型</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="AI 大模型微调"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="0-参考资料"><a href="#0-参考资料" class="headerlink" title="0 参考资料"></a>0 参考资料</h1><p><strong>GeekAGI 知识库：</strong> <a target="_blank" rel="noopener" href="https://geek-agi.feishu.cn/wiki/B9rYwwg6xidZYJkbrlscxTQFnOc">https://geek-agi.feishu.cn/wiki/B9rYwwg6xidZYJkbrlscxTQFnOc</a><br>项目地址：<a target="_blank" rel="noopener" href="https://github.com/DjangoPeng/LLM-quickstart">https://github.com/DjangoPeng/LLM-quickstart</a></p>
<h2 id="0-1-搭建开发环境"><a href="#0-1-搭建开发环境" class="headerlink" title="0.1 搭建开发环境"></a>0.1 搭建开发环境</h2><p>本项目对于硬件有一定要求：GPU 显存不小于16GB，支持最低配置显卡型号为 NVIDIA Tesla T4。<br>建议使用 GPU 云服务器来进行模型训练和微调。<br>项目使用 Python 版本为 3.10，环境关键依赖的官方文档如下：</p>
<ul>
<li>Python 环境管理 <a target="_blank" rel="noopener" href="https://docs.conda.io/projects/miniconda/en/latest/">Miniconda</a></li>
<li>Python 交互式开发环境 <a target="_blank" rel="noopener" href="https://jupyterlab.readthedocs.io/en/stable/getting_started/installation.html">Jupyter Lab</a></li>
</ul>
<p><strong>以下是详细的安装指导（以 Ubuntu 22.04 操作系统为例）</strong>：</p>
<ol>
<li><strong>安装操作系统级软件依赖</strong><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo apt update &amp;&amp; sudo apt upgrade</span><br><span class="line">sudo apt install ffmpeg</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 检查是否安装成功</span></span></span><br><span class="line">ffmpeg -version</span><br></pre></td></tr></table></figure></li>
</ol>
<p>参考：<a target="_blank" rel="noopener" href="https://phoenixnap.com/kb/install-ffmpeg-ubuntu">音频工具包 ffmpeg 官方安装文档</a></p>
<ol start="2">
<li><strong>安装 Python 环境管理工具 Miniconda</strong></li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p ~/miniconda3</span><br><span class="line">wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh</span><br><span class="line">bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3</span><br><span class="line">rm -rf ~/miniconda3/miniconda.sh</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">根据自己的终端执行</span></span><br><span class="line">~/miniconda3/bin/conda init bash</span><br><span class="line">~/miniconda3/bin/conda init zsh</span><br></pre></td></tr></table></figure>

<p>安装完成后，建议新建一个 Python 虚拟环境，命名为 <code>peft</code>。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">conda create -n peft python=3.10</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">激活环境</span></span><br><span class="line">conda activate peft</span><br></pre></td></tr></table></figure>

<p>之后每次使用需要激活此环境。</p>
<ol start="3">
<li><strong>安装 Python 依赖软件包</strong></li>
</ol>
<p>完整 Python 依赖软件包见<a href="requirements.txt">requirements.txt</a>。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure>

<ol start="3">
<li><strong>安装和配置 Jupyter Lab</strong></li>
</ol>
<p>上述开发环境安装完成后，使用 Miniconda 安装 Jupyter Lab：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install -c conda-forge jupyterlab</span><br></pre></td></tr></table></figure>

<p>使用 Jupyter Lab 开发的最佳实践是后台常驻，下面是相关配置（以 root 用户为例）：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">生成 Jupyter Lab 配置文件，</span></span><br><span class="line">jupyter lab --generate-config</span><br></pre></td></tr></table></figure>

<p>打开上面执行输出的<code>~/.jupyter/jupyter_lab_config.py</code>配置文件后，修改以下配置项：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">c.ServerApp.allow_root = <span class="literal">True</span> <span class="comment"># 非 root 用户启动，无需修改</span></span><br><span class="line">c.ServerApp.ip = <span class="string">&#x27;*&#x27;</span></span><br></pre></td></tr></table></figure>

<p>使用 nohup 后台启动 Jupyter Lab</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">nohup</span> jupyter lab --port=8000 --NotebookApp.token=<span class="string">&#x27;替换为你的密码&#x27;</span> --notebook-dir=./ &amp;</span></span><br></pre></td></tr></table></figure>
<p>Jupyter Lab 输出的日志将会保存在 <code>nohup.out</code> 文件（已在 <code>.gitignore</code>中过滤）。</p>
<ol start="4">
<li>安装 CUDA Toolkit 和 GPU 驱动</li>
</ol>
<p>根据你的实际情况，找到对应的 <a target="_blank" rel="noopener" href="https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=22.04&target_type=runfile_local">CUDA 12.04</a>：下载并安装 CUDA 12.04 Toolkit（包含GPU驱动）：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wget https://developer.download.nvidia.com/compute/cuda/12.4.0/local_installers/cuda_12.4.0_550.54.14_linux.run</span><br><span class="line">sudo sh cuda_12.4.0_550.54.14_linux.run</span><br></pre></td></tr></table></figure>

<p>**注意使用<code>runfile</code>方式，可以连同版本匹配的 GPU 驱动一起安装好。</p>
<p>安装完成后，使用 <code>nvidia-smi</code> 指令查看版本：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">nvidia-smi</span><br><span class="line">Mon Dec 18 12:10:47 2023</span><br><span class="line">+---------------------------------------------------------------------------------------+</span><br><span class="line">| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |</span><br><span class="line">|-----------------------------------------+----------------------+----------------------+</span><br><span class="line">| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |</span><br><span class="line">| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |</span><br><span class="line">|                                         |                      |               MIG M. |</span><br><span class="line">|=========================================+======================+======================|</span><br><span class="line">|   0  Tesla T4                       Off | 00000000:00:0D.0 Off |                    0 |</span><br><span class="line">| N/A   44C    P0              26W /  70W |      2MiB / 15360MiB |      6%      Default |</span><br><span class="line">|                                         |                      |                  N/A |</span><br><span class="line">+-----------------------------------------+----------------------+----------------------+</span><br><span class="line"></span><br><span class="line">+---------------------------------------------------------------------------------------+</span><br><span class="line">| Processes:                                                                            |</span><br><span class="line">|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |</span><br><span class="line">|        ID   ID                                                             Usage      |</span><br><span class="line">|=======================================================================================|</span><br><span class="line">|  No running processes found                                                           |</span><br><span class="line">+---------------------------------------------------------------------------------------+</span><br></pre></td></tr></table></figure>

<h1 id="1-大语言模型（LLM）的文本生成"><a href="#1-大语言模型（LLM）的文本生成" class="headerlink" title="1 大语言模型（LLM）的文本生成"></a>1 大语言模型（LLM）的文本生成</h1><h2 id="1-1-LLM-的推理方式"><a href="#1-1-LLM-的推理方式" class="headerlink" title="1.1 LLM 的推理方式"></a>1.1 LLM 的推理方式</h2><p>大型语言模型（LLM）基于 Transformer 架构，通过<strong>两个主要步骤生成文本：</strong></p>
<ol>
<li>预填充（Prefill）：将输入提示中的所有Token转换为模型可以处理的格式。</li>
<li>生成（Completion）：模型采用自回归方式，一次生成一个新Token。</li>
</ol>
<p>例如，在一个聊天机器人与 LLM 的对话中：</p>
<ul>
<li>第一个问题输入了 8 个Token</li>
<li>第二个问题输入了 10 个Token</li>
</ul>
<p>那么第二个问题的「输入Token数」是多少？<br>一般用户可能会认为是 10 个，但实际上消耗的Token数是：8 + 第一次生成的输出Token数 + 10。</p>
<h2 id="1-2-LLM-的文本生成模式"><a href="#1-2-LLM-的文本生成模式" class="headerlink" title="1.2 LLM 的文本生成模式"></a>1.2 LLM 的文本生成模式</h2><h3 id="1-2-1-Completion模式"><a href="#1-2-1-Completion模式" class="headerlink" title="1.2.1 Completion模式"></a>1.2.1 Completion模式</h3><p>Completion模式是LLM模型最基础的文本生成模式，适用于从给定提示（prompt）开始自动补全文本段落或回答问题。</p>
<p><strong>工作流程：</strong></p>
<ol>
<li>输出提示：用户提供一个完整的输入提示：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">prompt = <span class="string">&quot;The history of artificial intelligence is&quot;</span></span><br></pre></td></tr></table></figure></li>
<li>生成文本：基于输入提示，LLM 模型使用逐个Token生成的方式生成后续文本。</li>
</ol>
<h3 id="1-2-2-Chat-模式"><a href="#1-2-2-Chat-模式" class="headerlink" title="1.2.2 Chat 模式"></a>1.2.2 Chat 模式</h3><p>Chat模式旨在模拟多轮对话，使得LLM能够与用户进行连贯的互动。</p>
<p><strong>工作流程：</strong></p>
<ol>
<li>多轮对话输入：用户提供多轮对话的上下文，如：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">messages = [</span><br><span class="line">	&#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;You are a helpful assistant.&quot;</span>&#125;,</span><br><span class="line">	&#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;Can you tell me about the history of AI?&quot;</span>&#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure></li>
<li>生成回复：基于对话上下文，LLM模型逐个Token生成助手的回复。</li>
</ol>
<blockquote>
<p>在生成策略方面, 两种模式都采用了逐个Token生成的策略, 即根据前面已生成的Token预测下一个Token。</p>
</blockquote>
<h2 id="1-3-LLM-的文本生成策略"><a href="#1-3-LLM-的文本生成策略" class="headerlink" title="1.3 LLM 的文本生成策略"></a>1.3 LLM 的文本生成策略</h2><p>大规模语言模型（LLM）的文本生成策略指的是从模型中生成文本的具体方法或技术。这些策略用于控制模型生成的文本内容、质量和连贯性。LLM 模型会为所有Token输出 logits（得分），通过 softmax 函数可以将这些得分转换成每个Token在生成时被选中的概率。</p>
<h3 id="1-3-1-Greedy-sampling（贪婪抽样）"><a href="#1-3-1-Greedy-sampling（贪婪抽样）" class="headerlink" title="1.3.1 Greedy sampling（贪婪抽样）"></a>1.3.1 Greedy sampling（贪婪抽样）</h3><p>在贪婪策略中，模型总是选择它认为在每一步最有可能的Token，不考虑其他可能性或探索不同选项。模型选择概率最高的Token，并根据所选的Token继续生成文本。<br><img src="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2025/20250512105133.png" alt="image.png"><br>使用贪婪策略计算上高效且直接，但有时会导致生成重复或过于确定的输出。<br>由于模型在每一步只考虑最有可能的Token，它可能无法充分捕捉上下文和语言的多样性，或生成最具创意的回应。</p>
<h3 id="1-3-2-Beam-search（束搜索）"><a href="#1-3-2-Beam-search（束搜索）" class="headerlink" title="1.3.2 Beam search（束搜索）"></a>1.3.2 Beam search（束搜索）</h3><p>在束搜索中，模型假设了一组“K”个最有可能的Token，而不是在每一步只考虑最有可能的Token。这组 K个Token被称为“束”（beam）。<br><img src="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2025/20250512105335.png" alt="image.png"><br>模型为每个Token生成可能的序列，并在文本生成的每一步通过扩展每个束的可能线路来跟踪他们的概率。这个过程持续进行，直到生成的文本达到所需的长度，或者每个束都遇到一个“结束”Token。模型从所有束中选择整体概率最高的序列作为最终输出。</p>
<h3 id="1-3-3-Normal-random-sampling（正常随机抽样）"><a href="#1-3-3-Normal-random-sampling（正常随机抽样）" class="headerlink" title="1.3.3 Normal random sampling（正常随机抽样）"></a>1.3.3 Normal random sampling（正常随机抽样）</h3><p>通过选择一个随机值将其映射到选定的Token上来选择下一个词，可以想象成转动一个轮盘，每个Token的区域由它的概率决定。概率越高，Token被选中的机会就越大。<br><img src="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2025/20250512105909.png" alt="image.png"></p>
<h3 id="1-3-4-Random-sampling-with-Temperature（随机温度抽样）"><a href="#1-3-4-Random-sampling-with-Temperature（随机温度抽样）" class="headerlink" title="1.3.4 Random sampling with Temperature（随机温度抽样）"></a>1.3.4 Random sampling with Temperature（随机温度抽样）</h3><p>LLM 使用softmax 函数将 logits 转换为概率。温度这个超参数，会影响文本生成的随机性。<br><img src="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2025/20250512110149.png" alt="image.png"><br>在标准的softmax函数中，根据模型输出的 logits 计算每个Token的概率，其中，$x_i$ 表示第$i$个Token的logit值，$N$为词表大小。<br>其中，在温度softmax 函数中，引入了一个 温度参数 T，温度T作为分母出现在指数项中，这个参数可以控制概率分布的形状：</p>
<ol>
<li>当$T &#x3D; 1$时，温度Softmax函数退化为标准的Softmax函数。此时，概率分布与原始logits保持一致。</li>
<li>当$T &gt; 1$时，温度较高，概率分布变得更加平缓。这意味着原本概率较低的Token现在有更大的机会被选中，生成的文本将更加多样化和富有创造性。但同时，生成的文本也可能偏离原始主题或失去连贯性。</li>
<li>当$0 &lt; T &lt; 1$时，温度较低，概率分布变得更加尖锐。原本概率较高的Token将占据主导地位，生成的文本将更加保守和确定性强。这有助于生成与提示高度相关的文本，但可能缺乏多样性</li>
</ol>
<p>因此，温度参数$T$提供了一种调节生成文本特性的方法。可以根据具体的应用场景和需求，灵活选择合适的温度值。较高的温度适用于创意写作、故事生成等任务，而较低的问题则适用于事实回答、摘要生成等任务。</p>
<h3 id="1-3-5-Top-K-sampling（Tok-K抽样）"><a href="#1-3-5-Top-K-sampling（Tok-K抽样）" class="headerlink" title="1.3.5 Top-K sampling（Tok-K抽样）"></a>1.3.5 Top-K sampling（Tok-K抽样）</h3><p>结合温度参数，对仅Top-K 的token 进行随机采样。<br><img src="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2025/20250512111116.png" alt="image.png"></p>
<h3 id="1-3-6-Nucelus-sampling-or-top-p-sampling（Top-p-抽样）"><a href="#1-3-6-Nucelus-sampling-or-top-p-sampling（Top-p-抽样）" class="headerlink" title="1.3.6 Nucelus sampling or top-p sampling（Top-p 抽样）"></a>1.3.6 Nucelus sampling or top-p sampling（Top-p 抽样）</h3><p>Token概率的分布可能会有很大差异，这在文本生成过程中可能会带来意想不到的结果。核采样（Nucleus Sampling）旨在解决不同采样技术的某些局限性。它不是指定要考虑的固定数量的“K”个Token，而是使用一个概率阈值“p”。这个阈值代表您希望在采样中包含的累积概率。<br>模型在每一步计算所有可能Token的概率，然后按降序排列。<br><img src="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2025/20250512111236.png" alt="image.png"><br>模型继续向生成的文本中添加Token，直到它们的总概率超过指定的阈值。<br>核采样的优势在于允许基于上下文进行更动态和自适应的Token选择。在每一步选择的Token数量可以根据该上下文中 Token 的概率而变化，这可以导致更多样化和更高质量的输出。</p>
<h3 id="1-3-7-LLM-生成策略比较"><a href="#1-3-7-LLM-生成策略比较" class="headerlink" title="1.3.7  LLM 生成策略比较"></a>1.3.7  LLM 生成策略比较</h3><p><strong>主要生成策略的比较：</strong></p>
<ul>
<li>贪婪搜索： 确定性强，但可能生成重复或缺乏多样性的内容。</li>
<li>束搜索： 生成质量高，但计算复杂度较高。</li>
<li>温度采样： 可调节生成内容的多样性和随机性。</li>
<li>Top-k 和 Top-p 采样：提高生成质量，同时保持灵活性。</li>
</ul>
<p><strong>生成策略选择建议</strong>：</p>
<ul>
<li>贪婪搜索： 用于生成确定性较高的内容。</li>
<li>束搜索： 用于生成质量要求较高的内容。</li>
<li>温度采样： 用于调整生成的随机性。</li>
<li>Top-k和Top-p采样： 用于确保生成内容的多样性。</li>
</ul>
<h1 id="2-LLM-的-Token-与分词器"><a href="#2-LLM-的-Token-与分词器" class="headerlink" title="2 LLM 的 Token 与分词器"></a>2 LLM 的 Token 与分词器</h1><p>LLM（大型语言模型）中的 Token 是模型在处理文本时使用的基本单位。它可以是单词、子词甚至字符，具体取决于模型的分词策略。不同模型和库对 Token 的定义和分词方式可能有所不同。</p>
<h2 id="2-1-分词器（Tokenizer）"><a href="#2-1-分词器（Tokenizer）" class="headerlink" title="2.1 分词器（Tokenizer）"></a>2.1 分词器（Tokenizer）</h2><p><img src="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2025/20250512111826.png" alt="image.png"><br>分词器用于将文本转换为模型可以理解的 Token 序列。常见的分词方式包括：</p>
<ul>
<li>字典分词：使用预定义的词汇表将输入文本与词汇表中的词匹配。</li>
<li>BPE（Byte-Pair Encoding）：一种子词分词方法，将常见的字符或字符组合编码成 Token。</li>
<li>SentencePiece：类似 BPE 的子词分词方法，通常用于处理多种语言。</li>
<li>WordPiece：BERT 模型使用的分词方法，通过子词组合生成更大的词汇。</li>
</ul>
<p><strong>特殊Token</strong>：<br>在 LLM 中，通常还会使用一些特殊的 Token 来标识不同的功能：</p>
<ul>
<li>[CLS]：用于表示句子或文本的开始。</li>
<li>[SEP]：分隔不同句子。</li>
<li>[PAD]：用于填充对齐不同长度的输入。</li>
<li>[UNK]：表示未知的 Token，通常是词汇表中未包含的单词或符号。</li>
<li>[MASK]：用于掩码语言模型中的填补位置（如 BERT 的预训练任务）。</li>
</ul>
<p><strong>Token</strong> 计数：<br>在 LLM 的推理过程中，输入和输出的 Token 数量对计算资源和费用具有重要影响。例如，GPT-3 使用的分词策略通常是基于字节对编码（BPE），平均每个单词可能会被分成 1.3 个Token。</p>
<p><strong>使用示例：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tikToken</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 GPT-3 模型的编码器</span></span><br><span class="line">encoder = tikToken.get_encoding(<span class="string">&#x27;gpt2&#x27;</span>)</span><br><span class="line">text = <span class="string">&quot;Hello, I&#x27;m an AI assistant.&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对文本进行分词并获取 Token ID</span></span><br><span class="line">Token_ids = encoder.encode(text)</span><br><span class="line">Tokens = [encoder.decode([tid]) <span class="keyword">for</span> tid <span class="keyword">in</span> Token_ids]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Tokens: <span class="subst">&#123;Tokens&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Token IDs: <span class="subst">&#123;Token_ids&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>输出：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Tokens: [<span class="string">&#x27;Hello&#x27;</span>, <span class="string">&#x27;,&#x27;</span>, <span class="string">&#x27; I&#x27;</span>, <span class="string">&quot;&#x27;m&quot;</span>, <span class="string">&#x27; an&#x27;</span>, <span class="string">&#x27; AI&#x27;</span>, <span class="string">&#x27; assistant&#x27;</span>, <span class="string">&#x27;.&#x27;</span>]</span><br><span class="line">Token IDs: [15496, 11, 314, 39, 346, 616, 3265, 13]</span><br></pre></td></tr></table></figure>

<h1 id="3-LLM-的文本生成过程"><a href="#3-LLM-的文本生成过程" class="headerlink" title="3 LLM 的文本生成过程"></a>3 LLM 的文本生成过程</h1><h2 id="3-1-LLM-文本生成步骤"><a href="#3-1-LLM-文本生成步骤" class="headerlink" title="3.1 LLM 文本生成步骤"></a>3.1 LLM 文本生成步骤</h2><ol>
<li>输入阶段（Input）：</li>
</ol>
<ul>
<li>用户提供一个初始输入，这可以是一个提示、问题或部分文本，模型将基于此生成文本。</li>
</ul>
<ol>
<li>分词（Tokenization）：</li>
</ol>
<p>输入文本被转换成一系列的Token（Tokens）。分词器会将输入文本分割成更小的单元，这些单元可以是单词、子词或字符，这取决于模型的分词策略。</p>
<ol start="3">
<li>嵌入（Embedding）：</li>
</ol>
<p>每个Token被转换成一个固定长度的向量，这个过程称为嵌入。这些向量通常通过预训练的词嵌入模型生成，它们能够捕捉词汇的语义信息。</p>
<ol start="4">
<li>位置编码（Positional Encoding）：</li>
</ol>
<p>为了保持词序信息，因为Transformer模型本身不具备捕捉顺序的能力，通常会给每个嵌入向量添加一个位置编码。</p>
<ol start="5">
<li>Transformer处理：</li>
</ol>
<ul>
<li>经过嵌入和位置编码的输入被送入Transformer模型。Transformer模型由多个相同的层组成，每层都包括自注意力（Self-Attention）机制和前馈神经网络（Feed-Forward NeuralNetwork）。</li>
<li>自注意力机制允许模型在处理当前词时考虑到句子中的其他词，这有助于捕捉文本中的长距离依赖关系。</li>
</ul>
<ol start="6">
<li>输出转换（Output Transformation）：</li>
</ol>
<p>Transformer模型的最后一层输出被送入一个线性层，将输出向量映射到模型词汇表大小的维度空间。</p>
<ol start="7">
<li>Softmax函数：</li>
</ol>
<p>应用Softmax函数将线性层的输出转换为概率分布，每个Token对应的概率表示模型预测该Token是下一个词的可能性。</p>
<ol start="8">
<li>采样（Sampling）：</li>
</ol>
<p>基于Softmax函数得到的概率分布，模型进行采样以选择下一个生成的Token。采样策略可以是贪心采样、随机采样、核采样（Nucleus Sampling）或温度采样（TemperatureSampling）等。</p>
<ol start="9">
<li>生成文本：</li>
</ol>
<p>重复采样过程，直到生成了足够长度的文本或直到遇到句子结束的Token。</p>
<ol start="10">
<li>后处理（Post-processing）：</li>
</ol>
<p>生成的文本可能需要一些后处理，如去除多余的特殊Token，或者进行语法和语义的修正。</p>
<h2 id="3-2-LLM-文本生成的Q、K、V"><a href="#3-2-LLM-文本生成的Q、K、V" class="headerlink" title="3.2 LLM 文本生成的Q、K、V"></a>3.2 LLM 文本生成的Q、K、V</h2><p>在大语言模型(LLM)的文本生成任务中,Q、K、V分别代表Query(查询)、Key(键)和Value(值)。<br><img src="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2025/20250512122848.png" alt="image.png"></p>
<p>这三个概念源于注意力机制(Attention Mechanism), 特别是自注意力(Self-Attention)机制, 它们在Transformer架构中扮演着关键角色。</p>
<ol>
<li>Query(Q): 表示当前正在处理的词或词组。在自注意力机制中, 每个词都被视为一个Query。对于生成任务, Query通常是前面已生成的词或词组。我们希望了解当前Query与其他词(Key-Value对)之间的相关性, 以决定如何生成下一个词。</li>
<li>Key(K): 表示一组候选词或词组, 用于与Query进行相关性计算。在自注意力机制中, 每个词也同时扮演Key的角色。通过计算Query与不同Key之间的相似度(通常使用点积或其他相似度函数), 我们可以得到一个注意力分布, 表示当前Query与每个Key的相关程度。</li>
<li>Value(V): 与Key相对应, 表示每个候选词或词组的实际表示或含义。一旦我们根据Query和Key的相关性计算出注意力分布, 就可以使用这个分布对Value进行加权求和，得到当前Query的上下文表示。这个上下文表示融合了与当前Query相关的所有词的信息, 用于指导下一个词的生成。</li>
</ol>
<p>在Transformer的自注意力层中, 输入序列首先被映射到三个不同的矩阵: Q矩阵、K矩阵和V矩阵。这三个矩阵的维度通常为(序列长度, 隐藏层大小)。然后, 通过将Q矩阵与K矩阵转置相乘, 并除以一个缩放因子(通常为隐藏层大小的平方根), 得到注意力分布。最后, 将注意力分布与V矩阵相乘, 得到输出表示。<br>对于生成任务, Transformer的解码器部分会根据前面生成的词(作为Query)与编码器输出或解码器自身的隐藏状态(作为Key-Value对)进行交互, 逐步生成后续的词。这个过程反复进行, 直到生成结束Token或达到最大长度限制。<br>总之, Q、K、V机制使得大语言模型能够动态地关注与当前生成相关的上下文信息, 从而生成更加连贯、自然的文本。这种机制也使得模型能够处理长距离依赖关系, 捕捉词与词之间的复杂交互。</p>
<p><strong>Q、K、V 示例：</strong><br>以”Paris is the city”作为提示(Prompt), 看看大语言模型如何利用Q、K、V机制生成后续文本。假设我们要生成的下一个词是”of”。<br>在示例中”of”可以被视为一种”预填充”(prefill)或”预测”(prediction)阶段得到的Token。<br>在实际的文本生成过程中, 大语言模型通常会使用一种称为”自回归”(autoregressive)的方法, 即根据前面已生成的词来预测下一个词。<br><strong>这个过程分为两个阶段：</strong></p>
<ol>
<li>预填充(Prefill)阶段: 在这个阶段, 模型根据前面的上下文(如”Paris is the city”)和其内部的语言知识,预测出最可能的下一个词。在我们的例子中, 模型预测”of”是一个高概率的候选词。这个预测过程本质上就是通过Q、K、V 机制, 利用注意力权重和上下文表示来估计每个词的概率分布。</li>
<li>采样(Sampling)阶段: 一旦模型得到了下一个词的概率分布 ,它就可以从这个分布中采样出一个实际的词。常见的采样策略包括贪婪采样(选择概率最高的词)、随机采样(根据概率随机选择)、束搜索(保留多个高概率候选)等。采样得到的词会被添加到已生成的序列中,然后模型再次进入预填充阶段,预测下一个词。</li>
</ol>
<p>假设我们要生成的下一个词是”of”。此时, “of”对应的Query向量(Q)会与前面已生成词(“Paris”, “is”, “the”, “city”)的Key向量(K)进行交互。</p>
<ol>
<li>首先, 模型将”of”的嵌入向量作为Query(Q), 将”Paris”,”is”,”the”,”city”的嵌入向量分别作为Key(K)。</li>
<li>然后, 模型计算Query与每个Key的相似度得分。在这个例子中,”of”可能与”city”的相似度最高, 因为”city of”是一个常见的短语。相似度得分可以表示为:<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Similarity(<span class="string">&quot;of&quot;</span>, <span class="string">&quot;Paris&quot;</span>) = 0.1</span><br><span class="line">Similarity(<span class="string">&quot;of&quot;</span>, <span class="string">&quot;is&quot;</span>) = 0.2</span><br><span class="line">Similarity(<span class="string">&quot;of&quot;</span>, <span class="string">&quot;the&quot;</span>) = 0.3</span><br><span class="line">Similarity(<span class="string">&quot;of&quot;</span>, <span class="string">&quot;city&quot;</span>) = 0.8</span><br></pre></td></tr></table></figure></li>
<li>接下来, 模型将这些相似度得分转化为注意力权重。这通常通过Softmax函数实现, 以确保所有权重的和为1：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Attention_weights = Softmax([0.1, 0.2, 0.3, 0.8])</span><br><span class="line">			= [0.05, 0.07, 0.10, 0.78]</span><br></pre></td></tr></table></figure></li>
<li>最后, 模型使用注意力权重对每个词的Value向量(V)进行加权求和。Value向量表示每个词的实际语义表示。假设我们有:<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Value(<span class="string">&quot;Paris&quot;</span>) = [0.2, 0.1, ..., 0.5]</span><br><span class="line">Value(<span class="string">&quot;is&quot;</span>) = [0.3, 0.2, ..., 0.4]</span><br><span class="line">Value(<span class="string">&quot;the&quot;</span>) = [0.1, 0.3, ..., 0.2]</span><br><span class="line">Value(<span class="string">&quot;city&quot;</span>) = [0.5, 0.2, ..., 0.9]</span><br></pre></td></tr></table></figure>
那么,”of”的上下文表示(Context)就是：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Context(<span class="string">&quot;of&quot;</span>) = 0.05 * Value(<span class="string">&quot;Paris&quot;</span>) + 0.07 * Value(<span class="string">&quot;is&quot;</span>)</span><br><span class="line">		+ 0.10 * Value(<span class="string">&quot;the&quot;</span>) + 0.78 * Value(<span class="string">&quot;city&quot;</span>)</span><br><span class="line">			= [0.44, 0.20, ..., 0.79]</span><br></pre></td></tr></table></figure>
这个上下文表示融合了与”of”最相关的词(“city”)的语义信息, 同时也考虑了其他词的影响。<br>模型将使用这个上下文表示来预测下一个词, 比如”light”, “love”, “fashion”等。<br>通过这种方式, 大语言模型可以动态地关注与当前生成最相关的上下文, 并根据这些信息生成连贯、自然的后续文本。在实际的模型中, 这个过程会重复多次(多头注意力机制), 并通过多个Transformer层进行处理, 以捕捉更复杂、更抽象的语言模式和依赖关系。</li>
</ol>
<h2 id="3-3-预填充（prefill）和解码（decode）"><a href="#3-3-预填充（prefill）和解码（decode）" class="headerlink" title="3.3 预填充（prefill）和解码（decode）"></a>3.3 预填充（prefill）和解码（decode）</h2><p>预填充和解码对于模型生成连贯、准确和高效的文本非常重要。</p>
<h3 id="3-3-1-预填充（prefill）阶段"><a href="#3-3-1-预填充（prefill）阶段" class="headerlink" title="3.3.1 预填充（prefill）阶段"></a>3.3.1 预填充（prefill）阶段</h3><p>预填充阶段主要用于准备初始上下文，为模型生成后续文本提供基础。在这个阶段，模型会处理输入的初始文本（通常是用户提供的提示或上下文），并生成相应的内部状态（如KV缓存）。<br><strong>具体过程：</strong></p>
<ol>
<li>初始上下文输入：<ol>
<li>用户提供一个初始文本作为提示，例如问题、句子或段落</li>
<li>该文本通常被编码为一系列Token</li>
</ol>
</li>
<li>初始上下文的注意力计算：<ol>
<li>模型对输入的所有Token进行处理，生成键和值，并存储在KV缓存中</li>
</ol>
</li>
<li>初始概率分布<ol>
<li>计算最后一个Token的概率分布，作为生成下一个Token的起始点</li>
</ol>
</li>
</ol>
<h3 id="3-3-2-解码（decode）阶段"><a href="#3-3-2-解码（decode）阶段" class="headerlink" title="3.3.2 解码（decode）阶段"></a>3.3.2 解码（decode）阶段</h3><p>解码阶段是模型根据预填充阶段准备好的上下文，生成后续文本的过程。在这个阶段，模型会逐步生成每个新Token，并在每次生成后更新KV缓存。<br><strong>具体过程：</strong></p>
<ol>
<li>Token生成：<ol>
<li>从预填充阶段的初始概率分布中采样一个新Token，作为下一个输入。</li>
<li>根据用户选择的策略（如贪心、采样、温度）进行采样。</li>
</ol>
</li>
<li>KV缓存更新<ol>
<li>使用新Token进行前向传递，更新KV缓存，生成新的键和值。</li>
<li>生成新的概率分布，用于采样下一个Token。</li>
</ol>
</li>
<li>循环迭代<ol>
<li>重复步骤1和步骤2，直到达到生成的最大长度或满足停止条件。</li>
</ol>
</li>
</ol>
<p><strong>预填充和解码阶段的意义：</strong></p>
<ol>
<li>预填充阶段的意义：<ol>
<li>为模型提供初始的上下文，使得生成的文本更连贯和准确</li>
<li>预先计算KV缓存，减少解码阶段的计算量</li>
</ol>
</li>
<li>解码阶段的意义<ol>
<li>根据预填充阶段的初始上下文逐步生成连贯的文本</li>
<li>通过选择不同的采样策略（如温度、Top-k、Top-p）控制生成的文本质量</li>
</ol>
</li>
</ol>
<h1 id="4-大模型微调原理"><a href="#4-大模型微调原理" class="headerlink" title="4 大模型微调原理"></a>4 大模型微调原理</h1><h2 id="4-1-基座模型到对话模型的转变"><a href="#4-1-基座模型到对话模型的转变" class="headerlink" title="4.1 基座模型到对话模型的转变"></a>4.1 基座模型到对话模型的转变</h2><ol>
<li><p>预训练LLM基础模型<br>在大规模语料库上预训练一个LLM基础模型。这个模型通常是一个大型的Transformer模型, 如GPT-3、T5、LLaMA等。<br>预训练的目标是让模型学习到语言的基本结构、语法、语义等一般性知识。<br>预训练通常采用自监督学习的方式, 即让模型在大量无标签的文本数据上学习预测下一个词或恢复被掩盖的词。通过这种方式, 模型可以学习到词汇、短语、句法、语义等多层次的语言表征。<br>预训练得到的LLM基础模型具有强大的语言理解和生成能力, 但它们通常是通用的, 没有针对特定任务或领域进行优化。</p>
</li>
<li><p>使用SFT进行任务适应</p>
</li>
</ol>
<p>为了让LLM基础模型适应特定的任务，如对话、问答、摘要等, 我们需要在特定任务的标注数据集上对其进行有监督微调(SFT)。<br>在对话任务中, 我们通常准备一个由人类对话组成的数据集, 其中每个样本包括一个对话历史(上下文)和一个人类回复(目标)。<br>然后, 我们将这些样本输入到LLM基础模型中, 让其学习根据给定的对话历史生成恰当的回复。SFT的过程类似于监督学习, 即最小化模型生成的回复与真实人类回复之间的差异(通常使用交叉熵损失)。<br>通过在大量对话样本上训练, 模型可以学习到对话的一般模式、礼貌用语、常见话题等。SFT得到的模型已经可以进行基本的对话交互, 但其生成的回复可能仍然存在一些问题, 如不够流畅、不够贴切、缺乏逻辑等。</p>
<ol start="3">
<li>使用RLHF 提高对话质量</li>
</ol>
<p>RLHF的核心思想是让模型根据人类对其生成回复的反馈(奖励)来学习和改进其行为策略。<br>具体来说, RLHF的过程如下：</p>
<ol>
<li>收集一批SFT模型生成的回复样本, 并让人类对每个回复打分(如按相关性、流畅度、逻辑性等维度)。</li>
<li>训练一个奖励模型, 用于根据对话历史和回复内容预测人类的评分。这个奖励模型本质上是一个回归模型, 可以用人类打分的样本进行监督学习。</li>
<li>使用奖励模型对SFT模型的生成过程进行引导。具体来说, 在生成每个词时, 我们使用奖励模型评估不同词选择对应的未来奖励, 并鼓励模型选择能获得更高奖励的词(通常使用策略梯度算法)。</li>
<li>重复步骤1-3,不断收集新的人类反馈数据, 优化奖励模型和对话模型, 直到对话质量满足要求。</li>
</ol>
<p>通过RLHF, 对话模型可以学习到更加符合人类偏好的对话策略, 生成更加自然、贴切、有逻辑的回复。同时, RLHF也可以帮助模型学习避免生成不恰当、有害或偏见的内容。</p>
<ol start="4">
<li>模型部署和持续优化</li>
</ol>
<p>经过SFT和RLHF优化的LLM聊天模型已经可以投入实际应用, 为用户提供智能对话服务。但我们的工作并没有就此结束, 还需要持续监控模型的性能, 收集用户反馈, 发现和解决存在的问题。<br>同时, 随着时间的推移, 用户的需求和偏好可能会发生变化, 新的话题和任务也会不断出现。因此, 我们需要持续收集新的对话数据, 定期对模型进行重新训练和优化, 以适应不断变化的应用环境。<br>此外, 我们还需要重视聊天模型的伦理性、安全性和可解释性。这需要在数据收集、模型训练、应用部署的各个环节进行全面考虑, 并与用户、社会各界保持积极沟通和互动。</p>
<h2 id="4-2-特定领域SFT微调的流程"><a href="#4-2-特定领域SFT微调的流程" class="headerlink" title="4.2 特定领域SFT微调的流程"></a>4.2 特定领域SFT微调的流程</h2><p>进行特定领域SFT微调通常包括以下几个关键步骤：</p>
<ol>
<li>领域数据准备:</li>
</ol>
<ul>
<li>收集高质量的领域文本数据, 包括各种形式的结构化、半结构化和非结构化数据。</li>
<li>对原始数据进行清洗、脱敏,去除噪音、错误和敏感信息。</li>
<li>根据任务需求, 将数据标注为特定的输入输出格式, 如问答对、摘要对、翻译对等。</li>
</ul>
<ol start="2">
<li>微调样本构建:</li>
</ol>
<ul>
<li>根据LLM的特点和领域任务的要求, 设计输入输出的样本格式和提示模板。</li>
<li>利用领域知识图谱、行业规范文档等, 丰富输入样本的背景信息和约束条件。</li>
<li>将格式统一的输入输出样本整理为可直接用于微调的数据集。</li>
</ul>
<ol start="3">
<li>微调策略设计:</li>
</ol>
<ul>
<li>选择一个与领域任务相近的通用LLM作为初始化模型, 以提高微调的效率和效果。</li>
<li>设计微调的目标函数, 除了基本的语言建模损失, 还可加入特定任务的评估指标。</li>
<li>确定微调的超参数, 如学习率、BatchSize、训练轮数等, 以平衡性能和成本。</li>
</ul>
<ol start="4">
<li>模型微调训练:</li>
</ol>
<ul>
<li>利用构建好的微调样本集, 在初始化的LLM上进行梯度下降训练。</li>
<li>动态调整超参数, 监控模型在验证集上的收敛情况。</li>
<li>对训练过程中的checkpoints进行测试, 选择性能最优的模型进行后续优化。</li>
</ul>
<ol start="5">
<li>微调模型评估:</li>
</ol>
<ul>
<li>在独立的测试集上全面评估微调后的模型, 既要考察通用的语言质量, 也要重点评测领域任务的关键指标。</li>
<li>邀请领域专家对模型输出进行人工评审, 提供改进反馈。</li>
<li>针对评估结果, 有针对性地扩充数据、调整模型, 进行多轮迭代优化。</li>
</ul>
<ol start="6">
<li>模型部署应用:</li>
</ol>
<ul>
<li>将微调优化后的模型部署到生产环境, 为特定领域任务提供智能辅助。</li>
<li>构建人机交互界面, 让领域用户可以方便地使用LLM的能力。持续收集用户反馈数据, 定期审核模型性能, 并根据领域知识的更新迭代模型。</li>
</ul>
<h2 id="4-3-LLM-微调方法"><a href="#4-3-LLM-微调方法" class="headerlink" title="4.3 LLM 微调方法"></a>4.3 LLM 微调方法</h2><p>为了在不同的应用场景中高效地利用和优化LLM, 研究者们提出了多种训练方法, 包括Full-tuning、Freeze-tuning、LoRA(Low-Rank Adaptation)和QLoRA(Quantized Low-Rank Adaptation)。</p>
<h3 id="4-3-1-Full-tuning（全量微调）"><a href="#4-3-1-Full-tuning（全量微调）" class="headerlink" title="4.3.1 Full-tuning（全量微调）"></a>4.3.1 Full-tuning（全量微调）</h3><p>Full-tuning 是对预训练模型的所有参数进行全面调整，以适应特定的下游任务，流程如下：</p>
<ol>
<li>加载预训练的LLM模型及其所有参数</li>
<li>在下游任务的训练数据上,使用任务特定的损失函数(如交叉熵、平方损失等)对模型进行端到端的微调。</li>
<li>所有模型参数都得到更新,以最小化任务损失。</li>
<li>微调后的模型可以直接应用于目标任务的推理。</li>
</ol>
<p>Full-tuning的优势在于其灵活性和表现力。通过调整所有参数, 模型可以充分适应目标任务的特点,学习任务特定的知识和模式。这种全面的微调通常能够取得较好的性能表现。<br> Full-tuning也存在一些局限性：<br> 1. 它需要大量的计算资源和时间成本。对于大型LLM,全参数微调需要训练数亿甚至上千亿个参数,对计算设备提出了很高的要求。<br> 2. Full-tuning可能面临过拟合的风险。当下游任务的训练数据有限时, 全参数微调可能过度适应训练集的特点, 导致在测试集上泛化能力下降。</p>
<h3 id="4-3-2-Freeze-tuning（冻结部分参数微调）"><a href="#4-3-2-Freeze-tuning（冻结部分参数微调）" class="headerlink" title="4.3.2 Freeze-tuning（冻结部分参数微调）"></a>4.3.2 Freeze-tuning（冻结部分参数微调）</h3><p>Freeze-tuning 与 Full-tuning不同, Freeze-tuning只调整模型的一部分参数, 而将其余参数”冻结”为预训练的初始值。<br>通常, Freeze-tuning只微调LLM的最后一层或几层, 而保持底层的表示学习层不变。流程如下：</p>
<ol>
<li>加载预训练的LLM模型及其参数。</li>
<li>选择要微调的顶层(如最后一个Transformer块), 并解冻其参数。</li>
<li>冻结其余底层的参数, 使其保持预训练的初始值不变。</li>
<li>在下游任务的训练数据上, 只对解冻的顶层参数进行微调, 以最小化任务损失。</li>
</ol>
<p>Freeze-tuning的优势在于 计算效率 和 泛化能力。：</p>
<ol>
<li>由于只微调部分参数, Freeze-tuning大大减少了训练时间和资源消耗。</li>
<li>冻结底层参数有助于保留预训练模型学到的通用语言知识, 降低过拟合风险, 提高模型在下游任务上的泛化表现。</li>
</ol>
<p>Freeze-tuning的表现力相对有限：</p>
<ol>
<li>由于大部分参数被冻结, 模型的适应能力受到限制, 可能难以充分捕捉目标任务的特定模式。</li>
<li>在数据充足、任务复杂度高的场景下, Freeze-tuning可能难以达到Full-tuning的性能水平。</li>
</ol>
<h3 id="4-3-3-LoRA-低秩适应的参数高效微调"><a href="#4-3-3-LoRA-低秩适应的参数高效微调" class="headerlink" title="4.3.3 LoRA 低秩适应的参数高效微调"></a>4.3.3 LoRA 低秩适应的参数高效微调</h3><p>LoRA(Low-Rank Adaptation) 与Freeze-tuning固定底层参数不同, LoRA在每个Transformer层中引入了一组低秩分解矩阵, 并只训练这些新增的低秩矩阵, 而保持原有的预训练权重不变。<br><strong>LoRA的流程如下:</strong></p>
<ol>
<li>加载预训练的LLM模型及其参数。</li>
<li>在每个Transformer层的自注意力机制和前馈神经网络模块中, 并行地插入一组低秩分解矩阵（秩 r&lt;&lt;d,d为隐藏层维度）。</li>
<li>冻结预训练模型的所有原有权重, 只微调新增的低秩矩阵。</li>
<li>在前向传播时,将低秩矩阵与原有权重相乘, 得到适应后的隐藏层表示。</li>
</ol>
<p>LoRA的核心思想是, 通过少量的额外参数(低秩矩阵)来捕捉下游任务的特定模式, 而不改变预训练权重。<br>这种方法大大减少了可训练参数的数量(比Full-tuning少2-3个数量级), 显著提高了训练效率。同时, 通过冻结预训练权重, LoRA在很大程度上保留了原有模型的泛化能力, 降低了过拟合风险。</p>
<h3 id="4-3-4-QLoRA：低秩适应的量化优化"><a href="#4-3-4-QLoRA：低秩适应的量化优化" class="headerlink" title="4.3.4 QLoRA：低秩适应的量化优化"></a>4.3.4 QLoRA：低秩适应的量化优化</h3><p>QLoRA(Quantized Low-Rank Adaptation)是LoRA的一种量化优化变体。QLoRA 的核心思想是在应用 LoRA 进行模型调优之前, 先对预训练的基础模型进行量化。<br>量化是一种将模型参数从高精度浮点数转换为低精度整数的技术, 可以在保持模型性能的同时, 大幅降低模型的存储和计算开销。</p>
<p><strong>QLoRA 的训练过程可以分为以下几个步骤：</strong></p>
<ol>
<li>对基础模型进行量化：使用量化技术, 如整数量化或二值化, 将预训练模型的权重从浮点数转换为低精度整数。这一步可以显著减小模型的体积和内存占用。</li>
<li>在量化模型上应用 LoRA：在量化后的基础模型上, 通过添加低秩矩阵对模型进行微调。这一步与标准的 LoRA 类似, 通过训练额外的低秩矩阵来适应特定任务, 而无需调整原始模型的权重。</li>
<li>训练和部署：使用任务特定的数据对 LoRA 矩阵进行训练, 得到适应后的模型。在推理阶段, 量化后的基础模型和训练好的 LoRA 矩阵可以高效地结合, 以生成所需的输出。</li>
</ol>
<p><strong>QLoRA 结合了量化和参数高效微调的优点,具有以下特点：</strong></p>
<ol>
<li>显著减小模型体积：通过量化技术,QLoRA 可以将基础模型的体积减小到原来的几分之一, 甚至更小。这使得 QLoRA 模型更容易存储和部署在资源受限的设备上。</li>
<li>加速推理：量化后的模型可以使用整数运算, 而整数运算通常比浮点运算更快。因此, QLoRA 模型在推理阶段可以获得显著的加速。</li>
<li>参数高效：与标准的微调方法相比,QLoRA 只需训练额外的低秩矩阵, 而不需要调整整个模型的参数。这使得 QLoRA 的参数开销非常小, 非常适合在参数量受限的情况下进行模型调优。</li>
</ol>
<h3 id="4-3-5-总结"><a href="#4-3-5-总结" class="headerlink" title="4.3.5 总结"></a>4.3.5 总结</h3><p>Full-tuning、Freeze-tuning、LoRA和QLoRA代表了LLM训练的不同思路和方法。它们在参数效率、计算成本、性能表现等方面各有优劣,适用于不同的任务场景和资源限制。</p>
<ol>
<li>Full-tuning是最传统、最灵活的微调方法, 通过调整所有参数来充分适应下游任务, 但计算成本高,过拟合风险大。</li>
<li>Freeze-tuning通过冻结底层参数, 在减少计算开销的同时保留了预训练知识, 提高了泛化能力, 但表现力有限</li>
<li>LoRA通过引入少量低秩分解矩阵, 在保留预训练权重的同时高效地适应下游任务, 取得了参数效率和性能的良好平衡</li>
<li>QLoRA在LoRA的基础上进行量化优化, 进一步压缩模型尺寸, 加速计算过程,特别适用于资源受限场景</li>
</ol>
<h1 id="6-大模型开发工具库-Transformers"><a href="#6-大模型开发工具库-Transformers" class="headerlink" title="6 大模型开发工具库-Transformers"></a>6 大模型开发工具库-Transformers</h1><p><strong>Transformers Notebooks：</strong><a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/notebooks">https://huggingface.co/docs/transformers/notebooks</a></p>
<h2 id="6-1-Transformers-核心模块"><a href="#6-1-Transformers-核心模块" class="headerlink" title="6.1 Transformers 核心模块"></a>6.1 Transformers 核心模块</h2><h3 id="6-1-1-Pipelines"><a href="#6-1-1-Pipelines" class="headerlink" title="6.1.1 Pipelines"></a>6.1.1 Pipelines</h3><p><strong>Pipelines</strong>（管道）是使用模型进行推理的一种简单易上手的方式。<br>这些管道是抽象了 Transformers 库中大部分复杂代码的对象，提供了一个专门用于多种任务的简单API，包括<strong>命名实体识别、掩码语言建模、情感分析、特征提取和问答</strong>等。</p>
<table>
<thead>
<tr>
<th>Modality</th>
<th>Task</th>
<th>Description</th>
<th>Pipeline API</th>
</tr>
</thead>
<tbody><tr>
<td>Audio</td>
<td>Audio classification</td>
<td>为音频文件分配一个标签</td>
<td>pipeline(task&#x3D;“audio-classification”)</td>
</tr>
<tr>
<td></td>
<td>Automatic speech recognition</td>
<td>将音频文件中的语音提取为文本</td>
<td>pipeline(task&#x3D;“automatic-speech-recognition”)</td>
</tr>
<tr>
<td>Computer vision</td>
<td>Image classification</td>
<td>为图像分配一个标签</td>
<td>pipeline(task&#x3D;“image-classification”)</td>
</tr>
<tr>
<td></td>
<td>Object detection</td>
<td>预测图像中目标对象的边界框和类别</td>
<td>pipeline(task&#x3D;“object-detection”)</td>
</tr>
<tr>
<td></td>
<td>Image segmentation</td>
<td>为图像中每个独立的像素分配标签（支持语义、全景和实例分割）</td>
<td>pipeline(task&#x3D;“image-segmentation”)</td>
</tr>
<tr>
<td>Natural language processing</td>
<td>Text classification</td>
<td>为给定的文本序列分配一个标签</td>
<td>pipeline(task&#x3D;“sentiment-analysis”)</td>
</tr>
<tr>
<td></td>
<td>Token classification</td>
<td>为序列里的每个 token 分配一个标签（人, 组织, 地址等等）</td>
<td>pipeline(task&#x3D;“ner”)</td>
</tr>
<tr>
<td></td>
<td>Question answering</td>
<td>通过给定的上下文和问题, 在文本中提取答案</td>
<td>pipeline(task&#x3D;“question-answering”)</td>
</tr>
<tr>
<td></td>
<td>Summarization</td>
<td>为文本序列或文档生成总结</td>
<td>pipeline(task&#x3D;“summarization”)</td>
</tr>
<tr>
<td></td>
<td>Translation</td>
<td>将文本从一种语言翻译为另一种语言</td>
<td>pipeline(task&#x3D;“translation”)</td>
</tr>
<tr>
<td>Multimodal</td>
<td>Document question answering</td>
<td>根据给定的文档和问题回答一个关于该文档的问题。</td>
<td>pipeline(task&#x3D;“document-question-answering”)</td>
</tr>
<tr>
<td></td>
<td>Visual Question Answering</td>
<td>给定一个图像和一个问题，正确地回答有关图像的问题</td>
<td>pipeline(task&#x3D;“vqa”)</td>
</tr>
</tbody></table>
<p>Pipelines 已支持的完整任务列表：<a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/task_summary">https://huggingface.co/docs/transformers/task_summary</a></p>
<p><strong>Pipeline API</strong> 是对所有其他可用管道的包装。它可以像任何其他管道一样实例化，并且降低AI推理的学习和使用成本。<br><img src="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20240904090258.png" alt="image.png"></p>
<ol>
<li><strong>使用Pipeline API 实现 Text Classification 任务</strong></li>
</ol>
<p><strong>Text classification</strong>(文本分类)与任何模态中的分类任务一样，文本分类将一个文本序列（可以是句子级别、段落或者整篇文章）标记为预定义的类别集合之一。文本分类有许多实际应用，其中包括：</p>
<ul>
<li>情感分析：根据某种极性（如积极或消极）对文本进行标记，以在政治、金融和市场等领域支持决策制定。</li>
<li>内容分类：根据某个主题对文本进行标记，以帮助组织和过滤新闻和社交媒体信息流中的信息（天气、体育、金融等）。</li>
</ul>
<p>下面以 <code>Text classification</code> 中的情感分析任务为例，展示如何使用 Pipeline API。</p>
<p>模型主页：<a target="_blank" rel="noopener" href="https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english">https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english</a></p>
<p><strong>在transformers自定义模型下载的路径：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">os.environ[<span class="string">&#x27;HF_HOME&#x27;</span>] = <span class="string">&#x27;/home/baihl/LLM/model&#x27;</span></span><br><span class="line">os.environ[<span class="string">&#x27;HF_HUB_CACHE&#x27;</span>] = <span class="string">&#x27;/home/baihl/LLM/model&#x27;</span></span><br></pre></td></tr></table></figure>

<p><strong>加载对应的模型：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> pipeline</span><br><span class="line"></span><br><span class="line"><span class="comment"># 仅指定任务时，使用默认模型（不推荐）</span></span><br><span class="line">pipe = pipeline(<span class="string">&quot;sentiment-analysis&quot;</span>)</span><br><span class="line">pipe(<span class="string">&quot;今儿上海可真冷啊&quot;</span>)   <span class="comment"># [&#123;&#x27;label&#x27;: &#x27;NEGATIVE&#x27;, &#x27;score&#x27;: 0.8957214951515198&#125;]</span></span><br></pre></td></tr></table></figure>

<p><strong>使用模型进行推理：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">pipe(<span class="string">&quot;You learn things really quickly. You understand the theory class as soon as it is taught.&quot;</span>)</span><br><span class="line"><span class="comment"># [&#123;&#x27;label&#x27;: &#x27;POSITIVE&#x27;, &#x27;score&#x27;: 0.9961802959442139&#125;]</span></span><br><span class="line"></span><br><span class="line">text_list = [</span><br><span class="line">    <span class="string">&quot;Today Shanghai is really cold.&quot;</span>,</span><br><span class="line">    <span class="string">&quot;I think the taste of the garlic mashed pork in this store is average.&quot;</span>,</span><br><span class="line">    <span class="string">&quot;You learn things really quickly. You understand the theory class as soon as it is taught.&quot;</span></span><br><span class="line">]</span><br><span class="line">pipe(text_list)</span><br><span class="line"></span><br><span class="line"><span class="comment"># [&#123;&#x27;label&#x27;: &#x27;NEGATIVE&#x27;, &#x27;score&#x27;: 0.9995032548904419&#125;,</span></span><br><span class="line"><span class="comment"># &#123;&#x27;label&#x27;: &#x27;NEGATIVE&#x27;, &#x27;score&#x27;: 0.9984821677207947&#125;,</span></span><br><span class="line"><span class="comment"># &#123;&#x27;label&#x27;: &#x27;POSITIVE&#x27;, &#x27;score&#x27;: 0.9961802959442139&#125;]</span></span><br></pre></td></tr></table></figure>

<ol start="2">
<li><strong>使用 Pipelines 实现智能问答</strong></li>
</ol>
<p><strong>Question Answering</strong>(问答)是另一个token-level的任务，返回一个问题的答案，有时带有上下文（开放领域），有时不带上下文（封闭领域）。每当我们向虚拟助手提出问题时，例如询问一家餐厅是否营业，就会发生这种情况。它还可以提供客户或技术支持，并帮助搜索引擎检索您要求的相关信息。</p>
<p>有两种常见的问答类型：</p>
<ul>
<li>提取式：给定一个问题和一些上下文，模型必须从上下文中提取出一段文字作为答案</li>
<li>生成式：给定一个问题和一些上下文，答案是根据上下文生成的；这种方法由<code>Text2TextGenerationPipeline</code>处理，而不是下面展示的<code>QuestionAnsweringPipeline</code></li>
</ul>
<p>模型主页：<a target="_blank" rel="noopener" href="https://huggingface.co/distilbert-base-cased-distilled-squad">https://huggingface.co/distilbert-base-cased-distilled-squad</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> pipeline</span><br><span class="line">question_answerer = pipeline(<span class="string">&quot;question-answering&quot;</span>, model=<span class="string">&#x27;distilbert-base-cased-distilled-squad&#x27;</span>)</span><br><span class="line"></span><br><span class="line">preds = question_answerer(</span><br><span class="line">    question=<span class="string">&quot;What is the name of the repository?&quot;</span>,</span><br><span class="line">    context=<span class="string">&quot;The name of the repository is huggingface/transformers&quot;</span>,</span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(</span><br><span class="line">    <span class="string">f&quot;score: <span class="subst">&#123;<span class="built_in">round</span>(preds[<span class="string">&#x27;score&#x27;</span>], <span class="number">4</span>)&#125;</span>, start: <span class="subst">&#123;preds[<span class="string">&#x27;start&#x27;</span>]&#125;</span>, end: <span class="subst">&#123;preds[<span class="string">&#x27;end&#x27;</span>]&#125;</span>, answer: <span class="subst">&#123;preds[<span class="string">&#x27;answer&#x27;</span>]&#125;</span>&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># score: 0.9327, start: 30, end: 54, answer: huggingface/transformers</span></span><br></pre></td></tr></table></figure>

<ol start="3">
<li><strong>使用 Pipelines 实现语音识别</strong></li>
</ol>
<p><strong>Audio classification</strong>(音频分类)是一项将音频数据从预定义的类别集合中进行标记的任务。这是一个广泛的类别，具有许多具体的应用，其中一些包括：</p>
<ul>
<li>声学场景分类：使用场景标签（“办公室”、“海滩”、“体育场”）对音频进行标记。</li>
<li>声学事件检测：使用声音事件标签（“汽车喇叭声”、“鲸鱼叫声”、“玻璃破碎声”）对音频进行标记。</li>
<li>标记：对包含多种声音的音频进行标记（鸟鸣、会议中的说话人识别）。</li>
<li>音乐分类：使用流派标签（“金属”、“嘻哈”、“乡村”）对音乐进行标记。</li>
</ul>
<p>模型主页：<a target="_blank" rel="noopener" href="https://huggingface.co/superb/hubert-base-superb-er">https://huggingface.co/superb/hubert-base-superb-er</a><br>数据集主页：<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/superb#er">https://huggingface.co/datasets/superb#er</a></p>
<blockquote>
<p>情感识别（ER）为每个话语预测一个情感类别。我们采用了最广泛使用的ER数据集IEMOCAP，并遵循传统的评估协议：我们删除不平衡的情感类别，只保留最后四个具有相似数量数据点的类别，并在标准分割的五折交叉验证上进行评估。评估指标是准确率（ACC）。</p>
</blockquote>
<p>依赖必要的音频处理数据包：ffmpeg</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">apt update &amp; apt upgrade</span><br><span class="line">apt install -y ffmpeg</span><br><span class="line">pip install ffmpeg ffmpeg-python</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> pipeline</span><br><span class="line">classifier = pipeline(task=<span class="string">&quot;audio-classification&quot;</span>, model=<span class="string">&quot;superb/hubert-base-superb-er&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 Hugging Face Datasets 上的测试文件</span></span><br><span class="line">preds = classifier(<span class="string">&quot;https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac&quot;</span>)</span><br><span class="line">preds = [&#123;<span class="string">&quot;score&quot;</span>: <span class="built_in">round</span>(pred[<span class="string">&quot;score&quot;</span>], <span class="number">4</span>), <span class="string">&quot;label&quot;</span>: pred[<span class="string">&quot;label&quot;</span>]&#125; <span class="keyword">for</span> pred <span class="keyword">in</span> preds]</span><br><span class="line">preds</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用本地的音频文件做测试</span></span><br><span class="line">preds = classifier(<span class="string">&quot;data/audio/mlk.flac&quot;</span>)</span><br><span class="line">preds = [&#123;<span class="string">&quot;score&quot;</span>: <span class="built_in">round</span>(pred[<span class="string">&quot;score&quot;</span>], <span class="number">4</span>), <span class="string">&quot;label&quot;</span>: pred[<span class="string">&quot;label&quot;</span>]&#125; <span class="keyword">for</span> pred <span class="keyword">in</span> preds]</span><br><span class="line">preds</span><br></pre></td></tr></table></figure>

<p><strong>Automatic speech recognition</strong>（自动语音识别）将语音转录为文本。这是最常见的音频任务之一，部分原因是因为语音是人类交流的自然形式。如今，ASR系统嵌入在智能技术产品中，如扬声器、电话和汽车。我们可以要求虚拟助手播放音乐、设置提醒和告诉我们天气。<br>但是，Transformer架构帮助解决的一个关键挑战是低资源语言。通过在大量语音数据上进行预训练，仅在一个低资源语言的一小时标记语音数据上进行微调，仍然可以产生与以前在100倍更多标记数据上训练的ASR系统相比高质量的结果。</p>
<p>模型主页：<a target="_blank" rel="noopener" href="https://huggingface.co/openai/whisper-small">https://huggingface.co/openai/whisper-small</a></p>
<p>下面展示使用 <code>OpenAI Whisper Small</code> 模型实现 ASR 的 Pipeline API 示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> pipeline</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 `model` 参数指定模型</span></span><br><span class="line">transcriber = pipeline(task=<span class="string">&quot;automatic-speech-recognition&quot;</span>, model=<span class="string">&quot;openai/whisper-small&quot;</span>)</span><br><span class="line"></span><br><span class="line">text = transcriber(<span class="string">&quot;data/audio/mlk.flac&quot;</span>)</span><br><span class="line">text</span><br></pre></td></tr></table></figure>

<h3 id="6-1-2-使用-AutoClass-管理-Tokenizer-和-Model"><a href="#6-1-2-使用-AutoClass-管理-Tokenizer-和-Model" class="headerlink" title="6.1.2 使用 AutoClass 管理 Tokenizer 和 Model"></a>6.1.2 使用 AutoClass 管理 Tokenizer 和 Model</h3><p>实际上，在 Transformers 库内部实现中，Pipeline 作为管理：<code>原始文本-输入Token IDs-模型推理-输出概率-生成结果</code> 的流水线抽象，背后串联了 Transformers 库的核心模块 <code>Tokenizer</code>和 <code>Models</code>。<br><strong>Piplines 运行原理：</strong><br><img src="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20240904093322.png" alt="image.png"></p>
<p>通常，想要使用的模型（网络架构）可以从提供给 <code>from_pretrained()</code> 方法的预训练模型的名称或路径中推测出来。<br>AutoClasses就是为了帮助用户完成这个工作，以便根据<code>预训练权重/配置文件/词汇表的名称/路径自动检索相关模型</code>。<br>比如手动加载<code>bert-base-chinese</code>模型以及对应的 <code>tokenizer</code> 方法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModel</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;bert-base-chinese&quot;</span>)</span><br><span class="line">model = AutoModel.from_pretrained(<span class="string">&quot;bert-base-chinese&quot;</span>)</span><br></pre></td></tr></table></figure>

<ol>
<li><strong>使用 BERT Tokenizer 编码文本</strong></li>
</ol>
<p>编码 (Encoding) 过程包含两个步骤：</p>
<ul>
<li>分词：使用分词器按某种策略将文本切分为 tokens；</li>
<li>映射：将 tokens 转化为对应的 token IDs。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一步：分词</span></span><br><span class="line">sequence = <span class="string">&quot;美国的首都是华盛顿特区&quot;</span></span><br><span class="line">tokens = tokenizer.tokenize(sequence)</span><br><span class="line"><span class="built_in">print</span>(tokens) <span class="comment"># [&#x27;美&#x27;, &#x27;国&#x27;, &#x27;的&#x27;, &#x27;首&#x27;, &#x27;都&#x27;, &#x27;是&#x27;, &#x27;华&#x27;, &#x27;盛&#x27;, &#x27;顿&#x27;, &#x27;特&#x27;, &#x27;区&#x27;]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二步：映射</span></span><br><span class="line">token_ids = tokenizer.convert_tokens_to_ids(tokens)</span><br><span class="line"><span class="built_in">print</span>(token_ids) <span class="comment"># [5401, 1744, 4638, 7674, 6963, 3221, 1290, 4670, 7561, 4294, 1277]</span></span><br></pre></td></tr></table></figure>

<ol start="2">
<li><strong>使用 Tokenizer.encode 方法端到端处理</strong></li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">token_ids_e2e = tokenizer.encode(sequence)</span><br><span class="line"><span class="built_in">print</span>(token_ids_e2e)</span><br><span class="line"><span class="comment"># [101, 5401, 1744, 4638, 7674, 6963, 3221, 1290, 4670, 7561, 4294, 1277, 102]</span></span><br></pre></td></tr></table></figure>

<ol start="3">
<li><strong>编解码多段文本</strong></li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sequence_batch = [<span class="string">&quot;美国的首都是华盛顿特区&quot;</span>, <span class="string">&quot;中国的首都是北京&quot;</span>]</span><br><span class="line">token_ids_batch = tokenizer.encode(sequence_batch)</span><br><span class="line">tokenizer.decode(token_ids_batch)</span><br><span class="line"><span class="comment"># &#x27;[CLS] 美 国 的 首 都 是 华 盛 顿 特 区 [SEP] 中 国 的 首 都 是 北 京 [SEP]&#x27;</span></span><br></pre></td></tr></table></figure>

<p><img src="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20240904101002.png" alt="image.png"></p>
<ol start="4">
<li><strong>直接使用 <code>tokenizer.__call__</code> 方法完成文本编码 + 特殊编码补全</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">embedding_batch = tokenizer(<span class="string">&quot;美国的首都是华盛顿特区&quot;</span>, <span class="string">&quot;中国的首都是北京&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(embedding_batch)</span><br><span class="line"><span class="comment"># &#123;&#x27;input_ids&#x27;: [101, 5401, 1744, 4638, 7674, 6963, 3221, 1290, 4670, 7561, 4294, 1277, 102, 704, 1744, 4638, 7674, 6963, 3221, 1266, 776, 102], &#x27;token_type_ids&#x27;: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1], &#x27;attention_mask&#x27;: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]&#125;</span></span><br></pre></td></tr></table></figure></li>
</ol>
<ul>
<li>input_ids: token_ids</li>
<li>token_type_ids: token_id 归属的句子编号</li>
<li>attention_mask: 指示哪些token需要被关注（注意力机制）</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 优化下输出结构</span></span><br><span class="line"><span class="keyword">for</span> key, value <span class="keyword">in</span> embedding_batch.items():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;key&#125;</span>: <span class="subst">&#123;value&#125;</span>\n&quot;</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">input_ids: [101, 5401, 1744, 4638, 7674, 6963, 3221, 1290, 4670, 7561, 4294, 1277, 102, 704, 1744, 4638, 7674, 6963, 3221, 1266, 776, 102]</span><br><span class="line">token_type_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]</span><br><span class="line">attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</span><br></pre></td></tr></table></figure>


<ol start="5">
<li><strong>添加新 Token</strong></li>
</ol>
<p>当出现了词表或嵌入空间中不存在的新Token，需要使用 Tokenizer 将其添加到词表中。 Transformers 库提供了两种不同方法：</p>
<ul>
<li>add_tokens: 添加常规的正文文本 Token，以追加（append）的方式添加到词表末尾。</li>
<li>add_special_tokens: 添加特殊用途的 Token，优先在已有特殊词表中选择（<code>bos_token, eos_token, unk_token, sep_token, pad_token, cls_token, mask_token</code>）。如果预定义均不满足，则都添加到<code>additional_special_tokens</code>。</li>
</ul>
<p>先查看已有词表，确保新添加的 Token 不在词表中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">len</span>(tokenizer.vocab.keys) <span class="comment"># 21128</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> islice</span><br><span class="line"><span class="comment"># 使用 islice 查看词表部分内容</span></span><br><span class="line"><span class="keyword">for</span> key, value <span class="keyword">in</span> islice(tokenizer.vocab.items(), <span class="number">10</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;key&#125;</span>: <span class="subst">&#123;value&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">new_tokens = [<span class="string">&quot;天干&quot;</span>, <span class="string">&quot;地支&quot;</span>]</span><br><span class="line"><span class="comment"># 将集合作差结果添加到词表中</span></span><br><span class="line">new_tokens = <span class="built_in">set</span>(new_tokens) - <span class="built_in">set</span>(tokenizer.vocab.keys())</span><br><span class="line">tokenizer.add_tokens(<span class="built_in">list</span>(new_tokens)) <span class="comment"># 2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 新增加了2个Token，词表总数由 21128 增加到 21130 </span></span><br><span class="line"><span class="built_in">len</span>(tokenizer.vocab.keys())  <span class="comment"># 21130</span></span><br></pre></td></tr></table></figure>

<ol start="6">
<li><strong>添加特殊 Token</strong></li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">new_special_token = &#123;<span class="string">&quot;sep_token&quot;</span>: <span class="string">&quot;NEW_SPECIAL_TOKEN&quot;</span>&#125;</span><br><span class="line">tokenizer.add_special_tokens(new_special_token) <span class="comment"># 1</span></span><br><span class="line"><span class="comment"># 新增加了1个特殊Token，词表总数由 21128 增加到 21131</span></span><br><span class="line"><span class="built_in">len</span>(tokenizer.vocab.keys())  <span class="comment"># 21131</span></span><br></pre></td></tr></table></figure>

<ol start="7">
<li><strong>使用 <code>save_pretrained</code> 方法保存指定 Model 和 Tokenizer</strong></li>
</ol>
<p>借助 <code>AutoClass</code> 的设计理念，保存 Model 和 Tokenizer 的方法也相当高效便捷。<br>假设我们对<code>bert-base-chinese</code>模型以及对应的 <code>tokenizer</code> 做了修改，并更名为<code>new-bert-base-chinese</code>，方法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.save_pretrained(<span class="string">&quot;./models/new-bert-base-chinese&quot;</span>)</span><br><span class="line">model.save_pretrained(<span class="string">&quot;./models/new-bert-base-chinese&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>保存 Tokenizer 会在指定路径下创建以下文件：</p>
<ul>
<li>tokenizer.json: Tokenizer 元数据文件；</li>
<li>special_tokens_map.json: 特殊字符映射关系配置文件；</li>
<li>tokenizer_config.json: Tokenizer 基础配置文件，存储构建 Tokenizer 需要的参数；</li>
<li>vocab.txt: 词表文件；</li>
<li>added_tokens.json: 单独存放新增 Tokens 的配置文件。</li>
</ul>
<p>保存 Model 会在指定路径下创建以下文件：</p>
<ul>
<li>config.json：模型配置文件，存储模型结构参数，例如 Transformer 层数、特征空间维度等；</li>
<li>pytorch_model.bin：又称为 state dictionary，存储模型的权重。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.save_pretrained(<span class="string">&quot;./models/new-bert-base-chinese&quot;</span>)</span><br><span class="line">model.save_pretrained(<span class="string">&quot;./models/new-bert-base-chinese&quot;</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(&#x27;./models/new-bert-base-chinese/tokenizer_config.json&#x27;,</span><br><span class="line"> &#x27;./models/new-bert-base-chinese/special_tokens_map.json&#x27;,</span><br><span class="line"> &#x27;./models/new-bert-base-chinese/vocab.txt&#x27;,</span><br><span class="line"> &#x27;./models/new-bert-base-chinese/added_tokens.json&#x27;,</span><br><span class="line"> &#x27;./models/new-bert-base-chinese/tokenizer.json&#x27;)</span><br></pre></td></tr></table></figure>

<h2 id="6-2-Transformers-微调训练"><a href="#6-2-Transformers-微调训练" class="headerlink" title="6.2 Transformers 微调训练"></a>6.2 Transformers 微调训练</h2><p>基于 Transformers 实现模型微调训练的主要流程，包括：</p>
<ul>
<li>数据集下载</li>
<li>数据预处理</li>
<li>训练超参数配置</li>
<li>训练评估指标设置</li>
<li>训练器基本介绍</li>
<li>实战训练</li>
<li>模型保存</li>
</ul>
<p><strong>微调 BERT 情感分类：</strong><a target="_blank" rel="noopener" href="https://github.com/DjangoPeng/LLM-quickstart/blob/main/transformers/fine-tune-quickstart.ipynb">https://github.com/DjangoPeng/LLM-quickstart/blob/main/transformers/fine-tune-quickstart.ipynb</a><br><strong>微调语言模型-问答任务：</strong><a target="_blank" rel="noopener" href="https://github.com/DjangoPeng/LLM-quickstart/blob/main/transformers/fine-tune-QA.ipynb">https://github.com/DjangoPeng/LLM-quickstart/blob/main/transformers/fine-tune-QA.ipynb</a></p>
<h3 id="6-2-1-数据集处理库-Datasets"><a href="#6-2-1-数据集处理库-Datasets" class="headerlink" title="6.2.1 数据集处理库 Datasets"></a>6.2.1 数据集处理库 Datasets</h3><p>Datasets 解决数据来源问题，可以快速准备好数据集以进行深度学习模型的训练。</p>
<p><strong><code>datasets.load_dataset</code> 实现原理：</strong><br>如果您想向数据集添加额外的属性，例如类别标签。有两种方法来填充BuilderConfig类或其子类的属性</p>
<ul>
<li>在datasets DatasetBuilder.BUILDER_CONFIGS()属性中提供预定义的BuilderConfig类(或子类)实例列表。</li>
<li>当调用load_dataset()时，各参数默认值会直接读取 BuilderConfig 类的预定义值，否则会被覆盖。<br><img src="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20240904104926.png" alt="image.png"></li>
</ul>
<p>实际构造数据集的类 DatasetBuilder：<br><img src="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20240904105121.png" alt="image.png"></p>
<p><strong>下载数据集：</strong><br>Yelp评论数据集包括来自Yelp的评论。它是从Yelp Dataset Challenge 2015数据中提取的。<br>支持的任务和排行榜：文本分类、情感分类：该数据集主要用于文本分类：给定文本，预测情感。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line">dataset = load_dataset(<span class="string">&quot;yelp_review_full&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(dataset)</span><br></pre></td></tr></table></figure>
<p><strong>输出：</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">DatasetDict(&#123;</span><br><span class="line">    train: Dataset(&#123;</span><br><span class="line">        features: [&#x27;label&#x27;, &#x27;text&#x27;],</span><br><span class="line">        num_rows: 650000</span><br><span class="line">    &#125;)</span><br><span class="line">    test: Dataset(&#123;</span><br><span class="line">        features: [&#x27;label&#x27;, &#x27;text&#x27;],</span><br><span class="line">        num_rows: 50000</span><br><span class="line">    &#125;)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataset[<span class="string">&quot;train&quot;</span>][<span class="number">111</span>]</span><br></pre></td></tr></table></figure>

<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#x27;label&#x27;: 2,</span><br><span class="line"> &#x27;text&#x27;: &quot;As far as Starbucks go, this is a pretty nice one.  The baristas are friendly and while I was here, a lot of regulars must have come in, because they bantered away with almost everyone.  The bathroom was clean and well maintained and the trash wasn&#x27;t overflowing in the canisters around the store.  The pastries looked fresh, but I didn&#x27;t partake.  The noise level was also at a nice working level - not too loud, music just barely audible.\\n\\nI do wish there was more seating.  It is nice that this location has a counter at the end of the bar for sole workers, but it doesn&#x27;t replace more tables.  I&#x27;m sure this isn&#x27;t as much of a problem in the summer when there&#x27;s the space outside.\\n\\nThere was a treat receipt promo going on, but the barista didn&#x27;t tell me about it, which I found odd.  Usually when they have promos like that going on, they ask everyone if they want their receipt to come back later in the day to claim whatever the offer is.  Today it was one of their new pastries for $1, I know in the summer they do $2 grande iced drinks with that morning&#x27;s receipt.\\n\\nOverall, nice working or socializing environment.  Very friendly and inviting.  It&#x27;s what I&#x27;ve come to expect from Starbucks, so points for consistency.&quot;&#125;</span><br></pre></td></tr></table></figure>

<p><strong>数据展示工具：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> display, HTML</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_random_elements</span>(<span class="params">dataset, num_examples=<span class="number">10</span></span>):</span><br><span class="line">    <span class="keyword">assert</span> num_examples &lt;= <span class="built_in">len</span>(dataset), <span class="string">&quot;Can&#x27;t pick more elements than there are in the dataset.&quot;</span></span><br><span class="line">    picks = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_examples):</span><br><span class="line">        pick = random.randint(<span class="number">0</span>, <span class="built_in">len</span>(dataset)-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">while</span> pick <span class="keyword">in</span> picks:</span><br><span class="line">            pick = random.randint(<span class="number">0</span>, <span class="built_in">len</span>(dataset)-<span class="number">1</span>)</span><br><span class="line">        picks.append(pick)</span><br><span class="line">    </span><br><span class="line">    df = pd.DataFrame(dataset[picks])</span><br><span class="line">    <span class="keyword">for</span> column, typ <span class="keyword">in</span> dataset.features.items():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(typ, datasets.ClassLabel):</span><br><span class="line">            df[column] = df[column].transform(<span class="keyword">lambda</span> i: typ.names[i])</span><br><span class="line">    display(HTML(df.to_html()))</span><br><span class="line"></span><br><span class="line">show_random_elements(dataset[<span class="string">&quot;train&quot;</span>]) <span class="comment"># 默认随机展示10条</span></span><br></pre></td></tr></table></figure>

<p><img src="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20240904104704.png" alt="image.png"></p>
<h3 id="6-2-2-预处理数据"><a href="#6-2-2-预处理数据" class="headerlink" title="6.2.2 预处理数据"></a>6.2.2 预处理数据</h3><p>下载数据集到本地后，使用 Tokenizer 来处理文本，对于长度不等的输入数据，可以使用填充（padding）和截断（truncation）策略来处理。<br>Datasets 的 <code>map</code> 方法，支持一次性在整个数据集上应用预处理函数。<br>下面使用填充到最大长度的策略，处理整个数据集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;bert-base-cased&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tokenize_function</span>(<span class="params">examples</span>):</span><br><span class="line">    <span class="keyword">return</span> tokenizer(examples[<span class="string">&quot;text&quot;</span>], padding=<span class="string">&quot;max_length&quot;</span>, truncation=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">tokenized_datasets = dataset.<span class="built_in">map</span>(tokenize_function, batched=<span class="literal">True</span>)</span><br><span class="line">show_random_elements(tokenized_datasets[<span class="string">&quot;train&quot;</span>], num_examples=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20240904104723.png" alt="image.png"><br><img src="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20240904112747.png" alt="image.png"></p>
<p><strong>数据抽样：</strong></p>
<p>使用 1000 个数据样本，在 BERT 上演示小规模训练（基于 Pytorch Trainer）<br><code>shuffle()</code>函数会随机重新排列列的值。如果您希望对用于洗牌数据集的算法有更多控制，可以在此函数中指定generator参数来使用不同的numpy.random.Generator。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">small_train_dataset = tokenized_datasets[<span class="string">&quot;train&quot;</span>].shuffle(seed=<span class="number">42</span>).select(<span class="built_in">range</span>(<span class="number">1000</span>))</span><br><span class="line">small_eval_dataset = tokenized_datasets[<span class="string">&quot;test&quot;</span>].shuffle(seed=<span class="number">42</span>).select(<span class="built_in">range</span>(<span class="number">1000</span>))</span><br></pre></td></tr></table></figure>

<h3 id="6-2-3-微调训练配置"><a href="#6-2-3-微调训练配置" class="headerlink" title="6.2.3 微调训练配置"></a>6.2.3 微调训练配置</h3><ol>
<li><strong>加载 BERT 模型</strong></li>
</ol>
<p>警告通知我们正在丢弃一些权重（<code>vocab_transform</code> 和 <code>vocab_layer_norm</code> 层），并随机初始化其他一些权重（<code>pre_classifier</code> 和 <code>classifier</code> 层）。在微调模型情况下是绝对正常的，因为我们正在删除用于预训练模型的掩码语言建模任务的头部，并用一个新的头部替换它，对于这个新头部，我们没有预训练的权重，所以库会警告我们在用它进行推理之前应该对这个模型进行微调，而这正是我们要做的事情。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForSequenceClassification</span><br><span class="line"></span><br><span class="line">model = AutoModelForSequenceClassification.from_pretrained(<span class="string">&quot;bert-base-cased&quot;</span>, num_labels=<span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<p>上边输出如下告警：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: [&#x27;classifier.weight&#x27;, &#x27;classifier.bias&#x27;]</span><br><span class="line">You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.</span><br></pre></td></tr></table></figure>

<ol start="2">
<li><strong>训练超参数（TrainingArguments）</strong></li>
</ol>
<p>完整配置参数与默认值：<a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/v4.36.1/en/main_classes/trainer#transformers.TrainingArguments">https://huggingface.co/docs/transformers/v4.36.1/en/main_classes&#x2F;trainer#transformers.TrainingArguments</a></p>
<p>源代码定义：<a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/blob/v4.36.1/src/transformers/training_args.py#L161">https://github.com/huggingface/transformers/blob/v4.36.1/src/transformers/training_args.py#L161</a></p>
<blockquote>
<p>最重要配置：模型权重保存路径(output_dir)</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> TrainingArguments</span><br><span class="line"></span><br><span class="line">model_dir = <span class="string">&quot;models/bert-base-cased-finetune-yelp&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># logging_steps 默认值为500，根据我们的训练数据和步长，将其设置为100</span></span><br><span class="line">training_args = TrainingArguments(output_dir=model_dir,</span><br><span class="line">                                  per_device_train_batch_size=<span class="number">16</span>,</span><br><span class="line">                                  num_train_epochs=<span class="number">5</span>,</span><br><span class="line">                                  logging_steps=<span class="number">100</span>)</span><br><span class="line"><span class="comment"># 完整的超参数配置</span></span><br><span class="line"><span class="built_in">print</span>(training_args)</span><br></pre></td></tr></table></figure>

<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">TrainingArguments(</span><br><span class="line">_n_gpu=1, # 有一个gpu</span><br><span class="line">adafactor=False,</span><br><span class="line">adam_beta1=0.9,</span><br><span class="line">adam_beta2=0.999,</span><br><span class="line">adam_epsilon=1e-08,</span><br><span class="line">auto_find_batch_size=False,</span><br><span class="line">bf16=False,</span><br><span class="line">bf16_full_eval=False,</span><br><span class="line">data_seed=None,</span><br><span class="line">dataloader_drop_last=False,</span><br><span class="line">dataloader_num_workers=0,</span><br><span class="line">dataloader_persistent_workers=False,</span><br><span class="line">dataloader_pin_memory=True,</span><br><span class="line">ddp_backend=None,</span><br><span class="line">ddp_broadcast_buffers=None,</span><br><span class="line">ddp_bucket_cap_mb=None,</span><br><span class="line">.....</span><br></pre></td></tr></table></figure>

<ol start="3">
<li><strong>训练过程中的指标评估（Evaluate）</strong></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/docs/evaluate/index">Hugging Face Evaluate 库</a></strong> 支持使用一行代码，获得数十种不同领域（自然语言处理、计算机视觉、强化学习等）的评估方法。 当前支持 <strong>完整评估指标：<a target="_blank" rel="noopener" href="https://huggingface.co/evaluate-metric">https://huggingface.co/evaluate-metric</a></strong><br>训练器（Trainer）在训练过程中不会自动评估模型性能。因此，我们需要向训练器传递一个函数来计算和报告指标。 </p>
<p>Evaluate库提供了一个简单的准确率函数，您可以使用<code>evaluate.load</code>函数加载</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> evaluate</span><br><span class="line"></span><br><span class="line">metric = evaluate.load(<span class="string">&quot;accuracy&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>接着，调用 <code>compute</code> 函数来计算预测的准确率。<br>在将预测传递给 compute 函数之前，我们需要将 logits 转换为预测值（<strong>所有Transformers 模型都返回 logits</strong>）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_metrics</span>(<span class="params">eval_pred</span>):</span><br><span class="line">    logits, labels = eval_pred</span><br><span class="line">    predictions = np.argmax(logits, axis=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> metric.compute(predictions=predictions, references=labels)</span><br></pre></td></tr></table></figure>

<ol start="4">
<li><strong>训练过程中指标监控</strong></li>
</ol>
<p>通常，为了监控训练过程中的评估指标变化，我们可以在<code>TrainingArguments</code>指定<code>evaluation_strategy</code>参数，以便在 epoch 结束时报告评估指标。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> TrainingArguments, Trainer</span><br><span class="line"></span><br><span class="line">training_args = TrainingArguments(output_dir=model_dir,</span><br><span class="line">                                  evaluation_strategy=<span class="string">&quot;epoch&quot;</span>, </span><br><span class="line">                                  per_device_train_batch_size=<span class="number">16</span>,</span><br><span class="line">                                  num_train_epochs=<span class="number">3</span>,</span><br><span class="line">                                  logging_steps=<span class="number">30</span>)</span><br></pre></td></tr></table></figure>

<h3 id="6-2-4-微调训练模块-Trainer"><a href="#6-2-4-微调训练模块-Trainer" class="headerlink" title="6.2.4 微调训练模块 Trainer"></a>6.2.4 微调训练模块 Trainer</h3><p>完成了以上的配置后，可以执行 Trainer 进行训练：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">trainer = Trainer(</span><br><span class="line">    model=model,</span><br><span class="line">    args=training_args,</span><br><span class="line">    train_dataset=small_train_dataset,</span><br><span class="line">    eval_dataset=small_eval_dataset,</span><br><span class="line">    compute_metrics=compute_metrics,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer.train()  <span class="comment"># 开始训练</span></span><br></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/20240904110238.png" alt="image.png"></p>
<p>因为数据集只有 1000，约6分钟训练完成。</p>
<p><strong>训练过程中可以使用 nvidia-smi 查看 GPU 使用：</strong></p>
<p>为了实时查看GPU使用情况，可以使用 <code>watch</code> 指令实现轮询：<code>watch -n 1 nvidia-smi</code>:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Every 1.0s: nvidia-smi                                                   Wed Dec 20 14:37:41 2023</span><br><span class="line"></span><br><span class="line">Wed Dec 20 14:37:41 2023</span><br><span class="line">+---------------------------------------------------------------------------------------+</span><br><span class="line">| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |</span><br><span class="line">|-----------------------------------------+----------------------+----------------------+</span><br><span class="line">| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |</span><br><span class="line">| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |</span><br><span class="line">|                                         |                      |               MIG M. |</span><br><span class="line">|=========================================+======================+======================|</span><br><span class="line">|   0  Tesla T4                       Off | 00000000:00:0D.0 Off |                    0 |</span><br><span class="line">| N/A   64C    P0              69W /  70W |   6665MiB / 15360MiB |     98%      Default |</span><br><span class="line">|                                         |                      |                  N/A |</span><br><span class="line">+-----------------------------------------+----------------------+----------------------+</span><br><span class="line"></span><br><span class="line">+---------------------------------------------------------------------------------------+</span><br><span class="line">| Processes:                                                                            |</span><br><span class="line">|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |</span><br><span class="line">|        ID   ID                                                             Usage      |</span><br><span class="line">|=======================================================================================|</span><br><span class="line">|    0   N/A  N/A     18395      C   /root/miniconda3/bin/python                6660MiB |</span><br><span class="line">+---------------------------------------------------------------------------------------+</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">small_test_dataset = tokenized_datasets[<span class="string">&quot;test&quot;</span>].shuffle(seed=<span class="number">64</span>).select(<span class="built_in">range</span>(<span class="number">100</span>))</span><br><span class="line">trainer.evaluate(small_test_dataset)  <span class="comment"># 使用测试集进行评估</span></span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#x27;eval_loss&#x27;: 1.0753791332244873,</span><br><span class="line"> &#x27;eval_accuracy&#x27;: 0.52,  # 准确率</span><br><span class="line"> &#x27;eval_runtime&#x27;: 2.9889,</span><br><span class="line"> &#x27;eval_samples_per_second&#x27;: 33.457,</span><br><span class="line"> &#x27;eval_steps_per_second&#x27;: 4.349,</span><br><span class="line"> &#x27;epoch&#x27;: 3.0&#125;</span><br></pre></td></tr></table></figure>

<h3 id="6-2-5-保存模型和训练状态"><a href="#6-2-5-保存模型和训练状态" class="headerlink" title="6.2.5 保存模型和训练状态"></a>6.2.5 保存模型和训练状态</h3><ul>
<li>使用 <code>trainer.save_model</code> 方法保存模型，后续可以通过 from_pretrained() 方法重新加载</li>
<li>使用 <code>trainer.save_state</code> 方法保存训练状态</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">trainer.save_model(model_dir)</span><br><span class="line">trainer.save_state()</span><br><span class="line"></span><br><span class="line"><span class="comment"># trainer.model.save_pretrained(&quot;./&quot;)</span></span><br></pre></td></tr></table></figure>

<h2 id="6-3-Transformers-模型量化"><a href="#6-3-Transformers-模型量化" class="headerlink" title="6.3 Transformers 模型量化***"></a>6.3 Transformers 模型量化***</h2><h1 id="7-大模型微调工具-HF-PEFT"><a href="#7-大模型微调工具-HF-PEFT" class="headerlink" title="7 大模型微调工具 HF PEFT"></a>7 大模型微调工具 HF PEFT</h1><h1 id="11-快速入门-LangChain"><a href="#11-快速入门-LangChain" class="headerlink" title="11 快速入门 LangChain"></a>11 快速入门 LangChain</h1><h1 id="12-基于LangChain-和-ChatGLM-私有化部署聊天机器人"><a href="#12-基于LangChain-和-ChatGLM-私有化部署聊天机器人" class="headerlink" title="12 基于LangChain 和 ChatGLM 私有化部署聊天机器人"></a>12 基于LangChain 和 ChatGLM 私有化部署聊天机器人</h1></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="https://baihlup.github.io">梦之痕</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="https://baihlup.github.io/2024/09/03/250%20-%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD&amp;%E5%A4%A7%E6%95%B0%E6%8D%AE/257%20-%20AI%20%E5%A4%A7%E6%A8%A1%E5%9E%8B/05%20-%20AI%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83/">https://baihlup.github.io/2024/09/03/250%20-%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD&amp;%E5%A4%A7%E6%95%B0%E6%8D%AE/257%20-%20AI%20%E5%A4%A7%E6%A8%A1%E5%9E%8B/05%20-%20AI%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83/">大模型微调</a></div><div class="post_share"><div class="social-share" data-image="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/41710043961_.pic.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/09/28/270%20-%20%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/279%20-%20%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/02%20-%20%E8%AE%B0%E5%BD%95%E8%AE%BF%E9%97%AE%20HTTPS%20%E7%BD%91%E7%AB%99%E6%8A%A5%E9%94%99%E9%97%AE%E9%A2%98/" title="记录访问 HTTPS 网站报错问题"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">Previous</div><div class="prev_info">记录访问 HTTPS 网站报错问题</div></div></a></div><div class="next-post pull-right"><a href="/2024/08/29/270%20-%20%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/279%20-%20%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/06%20-%20%E7%BD%91%E7%BB%9C%E7%9F%A5%E8%AF%86%E4%B8%B2%E8%AE%B2%EF%BC%9A%E7%94%A8%E5%8F%8C%E5%8D%81%E4%B8%80%E7%9A%84%E6%95%85%E4%BA%8B%E4%B8%B2%E8%B5%B7%E7%A2%8E%E7%89%87%E7%9A%84%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE%EF%BC%88%E4%B8%AD%EF%BC%89/" title=""><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">Next</div><div class="next_info"></div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://raw.githubusercontent.com/BaihlUp/Figurebed/master/2024/41710043961_.pic.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">梦之痕</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">65</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">48</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">17</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/BaihlUp"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">个人笔记迁移中ing....</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#0-%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="toc-number">1.</span> <span class="toc-text">0 参考资料</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#0-1-%E6%90%AD%E5%BB%BA%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83"><span class="toc-number">1.1.</span> <span class="toc-text">0.1 搭建开发环境</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#1-%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%88LLM%EF%BC%89%E7%9A%84%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90"><span class="toc-number">2.</span> <span class="toc-text">1 大语言模型（LLM）的文本生成</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1-LLM-%E7%9A%84%E6%8E%A8%E7%90%86%E6%96%B9%E5%BC%8F"><span class="toc-number">2.1.</span> <span class="toc-text">1.1 LLM 的推理方式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-2-LLM-%E7%9A%84%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90%E6%A8%A1%E5%BC%8F"><span class="toc-number">2.2.</span> <span class="toc-text">1.2 LLM 的文本生成模式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-1-Completion%E6%A8%A1%E5%BC%8F"><span class="toc-number">2.2.1.</span> <span class="toc-text">1.2.1 Completion模式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-2-Chat-%E6%A8%A1%E5%BC%8F"><span class="toc-number">2.2.2.</span> <span class="toc-text">1.2.2 Chat 模式</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-3-LLM-%E7%9A%84%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90%E7%AD%96%E7%95%A5"><span class="toc-number">2.3.</span> <span class="toc-text">1.3 LLM 的文本生成策略</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-1-Greedy-sampling%EF%BC%88%E8%B4%AA%E5%A9%AA%E6%8A%BD%E6%A0%B7%EF%BC%89"><span class="toc-number">2.3.1.</span> <span class="toc-text">1.3.1 Greedy sampling（贪婪抽样）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-2-Beam-search%EF%BC%88%E6%9D%9F%E6%90%9C%E7%B4%A2%EF%BC%89"><span class="toc-number">2.3.2.</span> <span class="toc-text">1.3.2 Beam search（束搜索）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-3-Normal-random-sampling%EF%BC%88%E6%AD%A3%E5%B8%B8%E9%9A%8F%E6%9C%BA%E6%8A%BD%E6%A0%B7%EF%BC%89"><span class="toc-number">2.3.3.</span> <span class="toc-text">1.3.3 Normal random sampling（正常随机抽样）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-4-Random-sampling-with-Temperature%EF%BC%88%E9%9A%8F%E6%9C%BA%E6%B8%A9%E5%BA%A6%E6%8A%BD%E6%A0%B7%EF%BC%89"><span class="toc-number">2.3.4.</span> <span class="toc-text">1.3.4 Random sampling with Temperature（随机温度抽样）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-5-Top-K-sampling%EF%BC%88Tok-K%E6%8A%BD%E6%A0%B7%EF%BC%89"><span class="toc-number">2.3.5.</span> <span class="toc-text">1.3.5 Top-K sampling（Tok-K抽样）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-6-Nucelus-sampling-or-top-p-sampling%EF%BC%88Top-p-%E6%8A%BD%E6%A0%B7%EF%BC%89"><span class="toc-number">2.3.6.</span> <span class="toc-text">1.3.6 Nucelus sampling or top-p sampling（Top-p 抽样）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-7-LLM-%E7%94%9F%E6%88%90%E7%AD%96%E7%95%A5%E6%AF%94%E8%BE%83"><span class="toc-number">2.3.7.</span> <span class="toc-text">1.3.7  LLM 生成策略比较</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-LLM-%E7%9A%84-Token-%E4%B8%8E%E5%88%86%E8%AF%8D%E5%99%A8"><span class="toc-number">3.</span> <span class="toc-text">2 LLM 的 Token 与分词器</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-%E5%88%86%E8%AF%8D%E5%99%A8%EF%BC%88Tokenizer%EF%BC%89"><span class="toc-number">3.1.</span> <span class="toc-text">2.1 分词器（Tokenizer）</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-LLM-%E7%9A%84%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90%E8%BF%87%E7%A8%8B"><span class="toc-number">4.</span> <span class="toc-text">3 LLM 的文本生成过程</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-LLM-%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90%E6%AD%A5%E9%AA%A4"><span class="toc-number">4.1.</span> <span class="toc-text">3.1 LLM 文本生成步骤</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-LLM-%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90%E7%9A%84Q%E3%80%81K%E3%80%81V"><span class="toc-number">4.2.</span> <span class="toc-text">3.2 LLM 文本生成的Q、K、V</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3-%E9%A2%84%E5%A1%AB%E5%85%85%EF%BC%88prefill%EF%BC%89%E5%92%8C%E8%A7%A3%E7%A0%81%EF%BC%88decode%EF%BC%89"><span class="toc-number">4.3.</span> <span class="toc-text">3.3 预填充（prefill）和解码（decode）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-1-%E9%A2%84%E5%A1%AB%E5%85%85%EF%BC%88prefill%EF%BC%89%E9%98%B6%E6%AE%B5"><span class="toc-number">4.3.1.</span> <span class="toc-text">3.3.1 预填充（prefill）阶段</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-2-%E8%A7%A3%E7%A0%81%EF%BC%88decode%EF%BC%89%E9%98%B6%E6%AE%B5"><span class="toc-number">4.3.2.</span> <span class="toc-text">3.3.2 解码（decode）阶段</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E5%8E%9F%E7%90%86"><span class="toc-number">5.</span> <span class="toc-text">4 大模型微调原理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1-%E5%9F%BA%E5%BA%A7%E6%A8%A1%E5%9E%8B%E5%88%B0%E5%AF%B9%E8%AF%9D%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%BD%AC%E5%8F%98"><span class="toc-number">5.1.</span> <span class="toc-text">4.1 基座模型到对话模型的转变</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2-%E7%89%B9%E5%AE%9A%E9%A2%86%E5%9F%9FSFT%E5%BE%AE%E8%B0%83%E7%9A%84%E6%B5%81%E7%A8%8B"><span class="toc-number">5.2.</span> <span class="toc-text">4.2 特定领域SFT微调的流程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-3-LLM-%E5%BE%AE%E8%B0%83%E6%96%B9%E6%B3%95"><span class="toc-number">5.3.</span> <span class="toc-text">4.3 LLM 微调方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-1-Full-tuning%EF%BC%88%E5%85%A8%E9%87%8F%E5%BE%AE%E8%B0%83%EF%BC%89"><span class="toc-number">5.3.1.</span> <span class="toc-text">4.3.1 Full-tuning（全量微调）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-2-Freeze-tuning%EF%BC%88%E5%86%BB%E7%BB%93%E9%83%A8%E5%88%86%E5%8F%82%E6%95%B0%E5%BE%AE%E8%B0%83%EF%BC%89"><span class="toc-number">5.3.2.</span> <span class="toc-text">4.3.2 Freeze-tuning（冻结部分参数微调）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-3-LoRA-%E4%BD%8E%E7%A7%A9%E9%80%82%E5%BA%94%E7%9A%84%E5%8F%82%E6%95%B0%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83"><span class="toc-number">5.3.3.</span> <span class="toc-text">4.3.3 LoRA 低秩适应的参数高效微调</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-4-QLoRA%EF%BC%9A%E4%BD%8E%E7%A7%A9%E9%80%82%E5%BA%94%E7%9A%84%E9%87%8F%E5%8C%96%E4%BC%98%E5%8C%96"><span class="toc-number">5.3.4.</span> <span class="toc-text">4.3.4 QLoRA：低秩适应的量化优化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-5-%E6%80%BB%E7%BB%93"><span class="toc-number">5.3.5.</span> <span class="toc-text">4.3.5 总结</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7%E5%BA%93-Transformers"><span class="toc-number">6.</span> <span class="toc-text">6 大模型开发工具库-Transformers</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#6-1-Transformers-%E6%A0%B8%E5%BF%83%E6%A8%A1%E5%9D%97"><span class="toc-number">6.1.</span> <span class="toc-text">6.1 Transformers 核心模块</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-1-Pipelines"><span class="toc-number">6.1.1.</span> <span class="toc-text">6.1.1 Pipelines</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-2-%E4%BD%BF%E7%94%A8-AutoClass-%E7%AE%A1%E7%90%86-Tokenizer-%E5%92%8C-Model"><span class="toc-number">6.1.2.</span> <span class="toc-text">6.1.2 使用 AutoClass 管理 Tokenizer 和 Model</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-2-Transformers-%E5%BE%AE%E8%B0%83%E8%AE%AD%E7%BB%83"><span class="toc-number">6.2.</span> <span class="toc-text">6.2 Transformers 微调训练</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-1-%E6%95%B0%E6%8D%AE%E9%9B%86%E5%A4%84%E7%90%86%E5%BA%93-Datasets"><span class="toc-number">6.2.1.</span> <span class="toc-text">6.2.1 数据集处理库 Datasets</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-2-%E9%A2%84%E5%A4%84%E7%90%86%E6%95%B0%E6%8D%AE"><span class="toc-number">6.2.2.</span> <span class="toc-text">6.2.2 预处理数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-3-%E5%BE%AE%E8%B0%83%E8%AE%AD%E7%BB%83%E9%85%8D%E7%BD%AE"><span class="toc-number">6.2.3.</span> <span class="toc-text">6.2.3 微调训练配置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-4-%E5%BE%AE%E8%B0%83%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9D%97-Trainer"><span class="toc-number">6.2.4.</span> <span class="toc-text">6.2.4 微调训练模块 Trainer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-5-%E4%BF%9D%E5%AD%98%E6%A8%A1%E5%9E%8B%E5%92%8C%E8%AE%AD%E7%BB%83%E7%8A%B6%E6%80%81"><span class="toc-number">6.2.5.</span> <span class="toc-text">6.2.5 保存模型和训练状态</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-3-Transformers-%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96"><span class="toc-number">6.3.</span> <span class="toc-text">6.3 Transformers 模型量化***</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E5%B7%A5%E5%85%B7-HF-PEFT"><span class="toc-number">7.</span> <span class="toc-text">7 大模型微调工具 HF PEFT</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#11-%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8-LangChain"><span class="toc-number">8.</span> <span class="toc-text">11 快速入门 LangChain</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#12-%E5%9F%BA%E4%BA%8ELangChain-%E5%92%8C-ChatGLM-%E7%A7%81%E6%9C%89%E5%8C%96%E9%83%A8%E7%BD%B2%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA"><span class="toc-number">9.</span> <span class="toc-text">12 基于LangChain 和 ChatGLM 私有化部署聊天机器人</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/25/280%20-%20%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/281%20-%20%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/02%20-%20eBPF%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%AE%9E%E6%88%98/" title="eBPF核心技术与实战">eBPF核心技术与实战</a><time datetime="2025-07-25T00:00:00.000Z" title="Created 2025-07-25 00:00:00">2025-07-25</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/05/28/250%20-%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD&amp;%E5%A4%A7%E6%95%B0%E6%8D%AE/257%20-%20AI%20%E5%A4%A7%E6%A8%A1%E5%9E%8B/10%20-%20%E5%A4%A7%E6%A8%A1%E5%9E%8BRAG%E8%BF%9B%E9%98%B6%E5%AE%9E%E6%88%98%E8%90%A5/" title="10 - 大模型RAG进阶实战营">10 - 大模型RAG进阶实战营</a><time datetime="2025-05-28T00:00:00.000Z" title="Created 2025-05-28 00:00:00">2025-05-28</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/04/29/250%20-%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD&amp;%E5%A4%A7%E6%95%B0%E6%8D%AE/257%20-%20AI%20%E5%A4%A7%E6%A8%A1%E5%9E%8B/09%20-%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%20RAG%20%E7%9A%84%E5%BA%94%E7%94%A8%E5%92%8C%E5%BC%80%E5%8F%91/" title="大模型 RAG 的应用和开发">大模型 RAG 的应用和开发</a><time datetime="2025-04-29T00:00:00.000Z" title="Created 2025-04-29 00:00:00">2025-04-29</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/02/12/260%20-%20%E5%90%8E%E7%AB%AF&amp;%E6%9E%B6%E6%9E%84/263%20-%20%E7%B3%BB%E7%BB%9F%E6%9C%8D%E5%8A%A1/06%20-%20WASM%20%E6%8F%92%E4%BB%B6%E5%BC%80%E5%8F%91/" title="06 - WASM 插件开发">06 - WASM 插件开发</a><time datetime="2025-02-12T00:00:00.000Z" title="Created 2025-02-12 00:00:00">2025-02-12</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/09/28/270%20-%20%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/279%20-%20%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/02%20-%20%E8%AE%B0%E5%BD%95%E8%AE%BF%E9%97%AE%20HTTPS%20%E7%BD%91%E7%AB%99%E6%8A%A5%E9%94%99%E9%97%AE%E9%A2%98/" title="记录访问 HTTPS 网站报错问题">记录访问 HTTPS 网站报错问题</a><time datetime="2024-09-28T00:00:00.000Z" title="Created 2024-09-28 00:00:00">2024-09-28</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By 梦之痕</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>